Data Preparation
================

Training and evaluating RQA systems require a dataset of (question, answer, passage) pairs, denoted as *<q, a, p>*. However, full *<q, a, p>* pairs may not always be available in practice.
This section of documentation provides a set of scripts to generate questions, answers and prepare the passages for retrieval. This allows researchers to easily **create** new datasets for specific domains, but also **augment** existing (e.g., information retrieval) datasets.

Prepare Document
----------------

LocalRQA supports integration with frameworks such as LangChain and LlamaIndex to easily ingest text data in various formats, such as JSON data, HTML data, data from Google Drive, etc.
In the ``./local_rqa/scripts/data/process_docs.py``, you could specify the ``loader_func`` and the corresponding ``loader_parameters`` to the document loader that LangChain support based on your case. There are some usecases in the ``langchain_text_loader.py`` and ``llamaindex_text_loader.py`` files.

- Prepare Document from website url

Replace the ``main`` function in ``./local_rqa/scripts/data/process_docs.py`` with the following code to prepare document for website url::
    
    loader_func, split_func = SeleniumURLLoader, CharacterTextSplitter
    loader_parameters = {'urls': ["<website_url>"]}
    splitter_parameters = {'chunk_size': 400, 'chunk_overlap': 50, 'separator': "\n\n"}
    kwargs = {"loader_params": loader_parameters, "splitter_params": splitter_parameters}
    docs = LangChainTextLoader(save_folder="<path/to/save/dir>", save_filename="<filename>", loader_func, split_func).load_data(**kwargs)


- Prepare Document from jsonl file

Here is an example for data in jsonl format::

    {
        "source": "<passage_source>", 
        "title": "<passage_title>",
        "text": "<passage_content>"
    }


Then run the following command::

    python scripts/data/process_docs.py \
    --document_path path/to/data.jsonl \
    --model_name_or_path path/to/embedding_model \
    --chunk_size 400 \
    --chunk_overlap_size 50 \
    --save_dir path/to/save/dir \
    --save_name filename


It will load and split the documents into list of Document object. E.g.
::

    [
        Document(
            page_content='<passage_content>', 
            fmt_content='Source: <passage_source>\nTitle: <passage_title>\nContent:\n<passage_content>', 
            metadata={'source': '<passage_source>', 'seq_num': 1, 'title': '<passage_title>'}
        )
    ]


Data Generation
---------------
LocalRQA use a sampling algorithm to obtain gold and hard negative documents, and prompt an LLM to generate questions and answers from each gold document. This can be useful for researchers to create new RQA datasets for specific domains.

- Generate Questions

Generate questions by using gpt-3.5-turbo based on the each document::

    python scripts/data/doc_to_q.py \
    -mode all \
    -document_path path/to/document.pkl \
    --prompt_model gpt-3.5-turbo \
    --num_hard_negs_per_doc 2 \
    --num_train_data 600 \
    --num_eval_test_data 150 \
    --save_dir path/to/save/dir


- Generate Answers

Generate answers by using gpt-4-turbo based on the <q,p> pairs generated by the previous step::

    python scripts/data/doc_q_to_a.py \
    --prompt_model gpt-4-1106-preview \
    --dataset_w_q path/to/train_w_q.jsonl \  # generated by the previous step
    --save_name train_w_qa.jsonl \
    --save_dir path/to/save/dir \
    --end_data_idx 4  # a small number to test if it works
