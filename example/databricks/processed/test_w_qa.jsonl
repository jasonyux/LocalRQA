{"hard_neg_docs": [{"page_content": "![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How do I configure access to a table using Data Explorer?", "gold_answer": "To configure access to a table using Data Explorer, follow these steps:\n\n1. Click the **Data** icon in the sidebar.\n2. In the drop-down list at the top right, select a SQL warehouse, for example, **Starter Warehouse**. The `default` schema should be selected, and the schema comment and owner will display.\n3. Type the beginning of the table name (e.g., `peo`) in the text box following the `default` schema. Data Explorer will display the `people10m` table.\n4. Click the **Permissions** tab.\n5. Click the **Grant** button.\n6. Enter the email address of the user you want to grant access to (e.g., `user@example.com`).\n7. Select the **USAGE** checkbox.\n8. Click **OK**.\n9. Click on the `people10m` table to select it.\n10. Again, click the **Permissions** tab.\n11. Click the **Grant** button to finalize the permission settings."}
{"hard_neg_docs": [{"page_content": "![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I enable a user to access a specific table in Databricks?", "gold_answer": "To enable a user to access a specific table in Databricks, you can follow these steps:\n\n1. Click the **Data** icon in the sidebar.\n2. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n3. Type the first few letters of the table name (in this example, type `peo`) in the text box following the `default` schema. Data Explorer will display the `people10m` table.\n4. Click the `people10m` table.\n5. Click the **Permissions** tab.\n6. Click the **Grant** button.\n7. Enter the user's email address (for example, `user@example.com`).\n8. Select the **USAGE** checkbox.\n9. Click **OK**.\n\nBy following these steps, you can configure access for the user to the `default.people10m` table using the Data Explorer in Databricks."}
{"hard_neg_docs": [{"page_content": "```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the recommended migration path for Azure Data Lake Storage Gen1 in Databricks?", "gold_answer": "The recommended migration path for Azure Data Lake Storage Gen1 in Databricks is to migrate all ADLS (Azure Data Lake Storage) to ADLS2 (Azure Data Lake Storage Gen2). If you have not yet migrated, Databricks suggests referring to their documentation on how to proceed with the migration."}
{"hard_neg_docs": [{"page_content": "```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:", "fmt_content": "Source: https://docs.databricks.com/storage/azure-storage.html\nContent:\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:", "metadata": {"source": "https://docs.databricks.com/storage/azure-storage.html", "seq_num": 1793, "description": "", "text": "---\nlint: false\ndescription: Learn how to configure <Databricks> to use the ABFS driver to read and write data stored on <ADLS2> and Blob Storage.\n---\n\n# Access <ADLS2> and Blob Storage\n\nUse the Azure Blob Filesystem driver (ABFS) to connect to Azure Blob Storage and <ADLS2> from <Databricks>. Databricks recommends securing access to Azure storage containers by using Azure service principals set in cluster configurations.\n\n.. azure-aws::\n  .. note:: Databricks no longer recommends mounting external data locations to <DBFS>. See [_](/dbfs/mounts.md).\n\nThis article details how to access Azure storage containers using:\n\n.. azure-aws::\n\n  - <UC> managed external locations\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\n.. gcp::\n\n  - Azure service principals\n  - SAS tokens\n  - Account keys\n\nYou will set Spark properties to configure these credentials for a compute environment, either:\n\n- Scoped to <a Databricks> cluster\n- Scoped to <a Databricks> notebook\n\nAzure service principals can also be used to access Azure storage from <DBSQL>; see [_](/sql/admin/data-access-configuration.md).\n\nDatabricks recommends using [secret scopes](/security/secrets/secret-scopes.md) for storing all credentials.\n\n## Deprecated patterns for storing and accessing data from <Databricks>\n\nThe following are deprecated storage patterns:\n\n.. aws::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. azure::\n\n  - Databricks no longer recommends [_](/data-governance/credential-passthrough/adls-passthrough.md).\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n.. gcp::\n\n  - <ListItemBlob>\n\n  - <ListItemGen1>\n\n\n<!--  replacements used for the above lists...    -->\n\n.. <ListItemBlob> replace:: The legacy Windows Azure Storage Blob driver (WASB) has been deprecated. ABFS has numerous benefits over WASB. See [Azure documentation on ABFS](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-abfs-driver). For documentation for working with the legacy WASB driver, see [_](/storage/wasb-blob.md).\n\n.. <ListItemGen1> replace:: Azure has announced the pending retirement of [Azure Data Lake Storage Gen1](https://learn.microsoft.com/azure/data-lake-store/data-lake-store-overview). <Databricks> recommends migrating all <ADLS> to <ADLS2>. If you have not yet migrated, see [_](/storage/azure-datalake.md).\n\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake\n  wasb-blob\n\n.. azure::\n\n  <a id=\"adls2-uc\"></a>\n\n  ## Access <ADLS2> with <UC> external locations\n\n  .. note:: <ADLS2> is the only Azure storage type supported by <UC>.\n\n  <UC> manages access to data in <ADLS2> using [external locations](/data-governance/unity-catalog/manage-external-locations-and-credentials.md). Administrators primarily use external locations to configure <UC> [external tables](/data-governance/unity-catalog/index.md#external-table), but can also delegate  access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\n  Use the fully qualified ABFS URI to access data secured with <UC>. Because permissions are managed by <UC>, you do not need to pass any additional options or configurations for authentication.\n\n  .. warning:: <UC> ignores Spark configuration settings when accessing data managed by external locations.\n\n  Examples of reading:\n\n  ```python\n  dbutils.fs.ls(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.read.format(\"parquet\").load(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\")\n\n  spark.sql(\"SELECT * FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`\")\n  ```\n\n  Examples of writing:\n\n  ```python\n  dbutils.fs.mv(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n\n  df.write.format(\"parquet\").save(\"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/new-location\")\n  ```\n\n  Examples of creating external tables:\n\n  ```python\n  df.write.option(\"path\", \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\").saveAsTable(\"my_table\")\n\n  spark.sql(\"\"\"\n    CREATE TABLE my_table\n    LOCATION \"abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/table\"\n    AS (SELECT *\n      FROM parquet.`abfss://container@storageAccount.dfs.core.windows.net/external-location/path/to/data`)\n  \"\"\")\n  ```\n\n<a id=\"abfs-uri\"></a>\n\n## Direct access using ABFS URI for Blob Storage or <ADLS2>\n\nIf you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the `abfss` driver for greater security.\n\n\n```python\nspark.read.load(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n\ndbutils.fs.ls(\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-data>\")\n```\n\n```sql\nCREATE TABLE <database-name>.<table-name>;\n\nCOPY INTO <database-name>.<table-name>\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/path/to/folder'\nFILEFORMAT = CSV\nCOPY_OPTIONS ('mergeSchema' = 'true');\n```\n\n<a id=\"abfs-sp\"></a>\n\n## Access <ADLS2> or Blob Storage using OAuth 2.0 with an Azure service principal\n\nYou can securely access data in an Azure storage account using OAuth 2.0 with an <AAD> (Azure AD) application service principal for authentication; see [_](/storage/aad-storage-service-principal.md).\n\n```python\nservice_credential = dbutils.secrets.get(scope=\"<scope>\",key=\"<service-credential-key>\")\n\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"OAuth\")\nspark.conf.set(\"fs.azure.account.oauth.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\nspark.conf.set(\"fs.azure.account.oauth2.client.id.<storage-account>.dfs.core.windows.net\", \"<application-id>\")\nspark.conf.set(\"fs.azure.account.oauth2.client.secret.<storage-account>.dfs.core.windows.net\", service_credential)\nspark.conf.set(\"fs.azure.account.oauth2.client.endpoint.<storage-account>.dfs.core.windows.net\", \"https://login.microsoftonline.com/<directory-id>/oauth2/token\")\n```\n\nReplace\n\n- `<scope>` with the Databricks secret scope name.\n- `<service-credential-key>` with the name of the key containing the client secret.\n- `<storage-account>` with the name of the Azure storage account.\n- `<application-id>` with the **Application (client) ID** for the Azure Active Directory application.\n- `<directory-id>` with the **Directory (tenant) ID** for the Azure Active Directory application.\n\n## Access <ADLS2> or Blob Storage using a SAS token\n\nYou can use storage [shared access signatures (SAS)](https://learn.microsoft.com/azure/storage/common/storage-sas-overview) to access an <ADLS2> storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.\n\nYou can configure SAS tokens for multiple storage accounts in the same Spark session.\n\n.. note:: SAS support is available in <DBR> 7.5 and above.\n\n```python\nspark.conf.set(\"fs.azure.account.auth.type.<storage-account>.dfs.core.windows.net\", \"SAS\")\nspark.conf.set(\"fs.azure.sas.token.provider.type.<storage-account>.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\nspark.conf.set(\"fs.azure.sas.fixed.token.<storage-account>.dfs.core.windows.net\", \"<token>\")\n```\n\n<a id=\"auth-with-access-key\"></a>\n\n## Access <ADLS2> or Blob Storage using the account key\n\nYou can use [storage account access keys](https://learn.microsoft.com/azure/storage/common/storage-account-keys-manage?tabs=azure-portal) to manage access to Azure Storage.\n\n```python\nspark.conf.set(\n    \"fs.azure.account.key.<storage-account>.dfs.core.windows.net\",\n    dbutils.secrets.get(scope=\"<scope>\", key=\"<storage-account-access-key>\"))\n```\n\nReplace\n\n- `<storage-account>` with the Azure Storage account name.\n- `<scope>` with the <Databricks> secret scope name.\n- `<storage-account-access-key>` with the name of the key containing the Azure storage account access key.\n\n## Example notebook\n\n.. notebook:: adls-gen2-service-principal.html\n  :title: ADLS Gen2 OAuth 2.0 with Azure service principals notebook\n\n## <ADLS2> FAQs and known issues\n\nSee [_](/storage/azure-datalake-gen2-faqs-issues.md).\n\n.. toctree::\n  :maxdepth: 1\n  :hidden:\n\n  azure-datalake-gen2-faqs-issues\n\n.. <ADLS> replace:: Azure Data Lake Storage Gen1\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I access ADLS2 with UC external locations in Databricks?", "gold_answer": "You can access Azure Data Lake Storage Gen2 (ADLS2) with Unity Catalog (UC) external locations in Databricks by managing access to data in ADLS2 using external locations. Administrators primarily use external locations to configure UC external tables, but they can also delegate access to users or groups using the available privileges (`READ FILES`, `WRITE FILES`, and `CREATE TABLE`).\n\nWhen accessing data secured with UC, you should use the fully qualified Azure Blob File System (ABFS) URI. Since the permissions are managed by UC, there is no need to pass any additional options or configurations for authentication.\n\nIt's important to note that Unity Catalog ignores Spark configuration settings when accessing data managed by external locations."}
{"hard_neg_docs": [{"page_content": ".. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "question": "What are the updated required permissions for creating new workspaces in Databricks on GCP?", "gold_answer": "The updated required permissions for creating new workspaces in Databricks on GCP are:\n\n- `resourcemanager.projects.testIamPermissions`\n- For customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`\n- Additionally, the Databricks-created workspace service account is granted a role that includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`.\n\nThese changes were implemented on February 28, 2023, and do not affect existing workspaces."}
{"hard_neg_docs": [{"page_content": ".. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/february.html\nContent:\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/february.html", "seq_num": 1924, "description": "", "text": "---\ndescription: February 2023 release notes for new <Databricks> features and improvements.\n---\n\n# February 2023\n\nThese features and <Databricks> platform improvements were released in February 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. aws::\n\n  ## SAML single sign-on (SSO) in the account console is generally available\n\n  **February 28, 2023**\n\n  SAML single sign-on (SSO) in the account console is now generally available. Account administrators can now enable account single sign-on authentication using using either SAML 2.0 or OpenID Connect (OIDC). Once you have enabled SSO, all account admins and users are required to use SSO to log in to the account console. Only the account owner can log in using their username (email address) and password. See [_](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n.. gcp::\n\n  ## Updated required permissions for new workspaces\n\n  **February 28, 2023**\n\n  The [set of required permissions](/administration-guide/cloud-configurations/gcp/permissions.md) for new workspaces changed. To create a workspace, there is a new required permission `resourcemanager.projects.testIamPermissions`. There are also several new permissions that are required only for customer-managed VPCs: `compute.networks.get`, `compute.projects.get`, `compute.subnetworks.get`. The Databricks-created workspace service account is granted a role that now includes `compute.instances.setLabels`, `compute.disks.get`, and `compute.disks.setLabels`. This does not affect existing workspaces.\n\n  ## Secure a workspace with private connectivity using Google Private Service Connect (Public Preview)\n\n  **February 28, 2023**\n\n  You can now secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google [Private Service Connect](https://cloud.google.com/vpc/docs/private-service-connect) (PSC) on the workspace. See [_](/administration-guide/cloud-configurations/gcp/private-service-connect.md).\n\n.. azure::\n\n  ## Support for ND A100 v4 series instances (Public Preview)\n\n  **February 28, 2023**\n\n  Databricks now supports [ND A100 v4-series instances](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series) for GPU clusters. ND A100 instances are optimized for large deep learning workloads.\n\n## Ray on <Databricks> (Public Preview)\n\n**February 24, 2023**\n\nWith <DBR> 12.0 and above, you can create a Ray cluster and run Ray applications in <Databricks> with the Ray on Spark API. See [_](/machine-learning/ray-integration.md).\n\n## Notebook cell output results limit increased\n\n**February 23, 2023**\n\nWhen you execute a cell in a notebook, the lesser of 10,000 rows or 2 MB of output is displayed.\n\n## <Databricks> Jobs now supports running continuous jobs\n\n**February 22, 2023**\n\nYou can ensure there is always an active run of your <Databricks> job with the new `continuous` trigger type. See [_](/workflows/jobs/jobs.md#continuous-jobs).\n\n.. azure-aws::\n\n  ## Trigger your <Databricks> job when new files arrive\n\n  **February 22, 2023**\n\n  You can now use a file arrival trigger to run your <Databricks> job when new files arrive in an external location such as Amazon S3 or Azure storage. See [_](/workflows/jobs/file-arrival-triggers.md).\n\n## Databricks Terraform provider updated to version 1.10.0\n\n**February 21, 2023**\n\nVersion 1.10.0 makes updates to `databricks_share`, various documentation updates, and more. For more details, see the changelog for version [1.10.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.10.0/CHANGELOG.md).\n\n.. azure-aws::\n\n  ## Legacy global init scripts and cluster-named init scripts are deprecated\n\n  **February 21, 2023**\n\n  Starting February 21, 2023, legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces. See [_](/clusters/init-scripts.md#init-script-types).\n\n## Improvements to the MLflow experiment UI\n\n**February 17, 2023**\n\nThe MLflow experiment UI now includes a configurable chart view providing visual model performance insights, a revamped parallel coordinates experience for tuning, and a streamlined table view with enhancements for search and filtering. See [_](/mlflow/tracking.md#mlflow-experiments).\n\n## <DBR> 12.2 (Beta)\n\n**February 17, 2023**\n\n<DBR> 12.2 and <DBR> 12.2 ML are now available as Beta releases.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n.. azure::\n\n  ## Use user-assigned managed identities to access storage in <UC> (Public Preview)\n\n  **February 15, 2023**\n\n  You can now use user-assigned managed identities instead of system-assigned managed identities to access storage containers on behalf of <UC> users. See [_](/data-governance/unity-catalog/azure-managed-identities.md).\n\n## Databricks extension for Visual Studio Code (Public Preview)\n\n**February 14, 2023**\n\nThe [_](/dev-tools/vscode-ext.md) lets developers leverage the powerful authoring capabilities of IDEs while connecting to <Databricks> clusters to run code remotely, employing best practices such as source control, modularized code, refactoring, and unit testing.\n\n.. azure-aws::\n\n  ## Serverless Real-Time Inference Public Preview now available to all customers\n\n  **February 9, 2023**\n\n  Serverless Real-Time Inference is now available in public preview to all customers. You no longer need to request access.\n\n  Serverless Real-Time Inference provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md)\n\n.. azure::\n\n  ## Serverless SQL warehouses are available to workspaces in supported regions (Public Preview)\n\n  **February 9, 2023**\n\n  Serverless SQL warehouses are now available to all customers with workspaces in the regions West US (`westus`), Central US (`centralus`), East US (`eastus`), East US 2 (`eastus2`), North Europe (`northeurope`), and West Europe (`westeurope`). See [_](/sql/admin/serverless.md).\n\n## Databricks Terraform provider updated to version 1.9.2\n\n**February 3, 2023**\n\nVersion 1.9.2 adds a `databricks_node_type` selector, a new file library type for `databricks_pipeline`, various documentation updates, and more. For more details, see the changelog for version [1.9.2](https://github.com/databricks/terraform-provider-databricks/blob/v1.9.2/CHANGELOG.md).\n\n.. gcp::\n\n  ## <PC> is GA\n\n  **February 3, 2023**\n\n  <PC> is now generally available. See [_](/partner-connect/index.md).\n\n\n.. azure::\n\n  ## Customer-managed keys help protect and control access to some types of data (GA)\n\n  <!-- WARNING: Do not remove \"some types of\" above, since not all types of data are included in CMK  -->\n\n  **February 3, 2023**\n\n  <!-- WARNING: The first sentence below was carefully negotiated with PM and legal teams for our CMK main page a while back. Do NOT modify it without legal review!  -->\n\n  Some <Databricks> services and data support adding a customer-managed key to help protect and control access to encrypted data. <Databricks> has three customer-managed key features that involve different types of data and locations, which you can compare in [_](/security/keys/customer-managed-keys.md). All three of these features are now generally available.\n\n## Variable explorer in <Databricks> notebooks\n\n**February 1, 2023**\n\nWith <DBR> 12.1 and above, you can now [directly observe current Python variables and their values in the notebook UI](/notebooks/notebooks-code.md#variable-explorer).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "question": "How can I secure a workspace with private connectivity on GCP using Google Private Service Connect?", "gold_answer": "To secure a workspace with private connectivity on GCP using Google Private Service Connect (PSC), you can enable PSC on the workspace to mitigate data exfiltration risks. Although the specific details on how to enable this are not provided in the excerpt, you would typically follow the Databricks documentation or administration guide for setting up private connectivity with PSC. For complete instructions, you should refer to the provided link in the release notes or consult the Databricks administration guide under the section for cloud configurations on GCP, specifically the page on Private Service Connect."}
{"hard_neg_docs": [{"page_content": "-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\nIf you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\n---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What does the INVALID_ARRAY_INDEX error class indicate in Databricks?", "gold_answer": "The INVALID_ARRAY_INDEX error class in Databricks indicates that an index value used to access an element in an array is out of bounds. This means that the requested index is either negative or greater than or equal to the size of the array. The array has a specific number of elements, and the index must be within the range of 0 (for the first element) to arraySize - 1 (for the last element). The error class suggests using the SQL function `get()` to access elements at an invalid index which would return NULL instead of throwing an error. Additionally, it mentions that setting the configuration setting `ansiConfig` to \"false\" can bypass this error."}
{"hard_neg_docs": [{"page_content": "-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\nIf you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**", "fmt_content": "Source: https://docs.databricks.com/error-messages/invalid-array-index-error-class.html\nContent:\n---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**", "metadata": {"source": "https://docs.databricks.com/error-messages/invalid-array-index-error-class.html", "seq_num": 1662, "description": "", "text": "---\ndescription: Documentation for the INVALID_ARRAY_INDEX error class on <Databricks>\n---\n\n# INVALID_ARRAY_INDEX error class\n\n[SQLSTATE: 22003](sqlstates.md#class-22-data-exception)\n\nThe index `<indexValue>` is out of bounds. The array has `<arraySize>` elements. Use the SQL function `get()` to tolerate accessing element at invalid index and return NULL instead. If necessary set `<ansiConfig>` to \"false\" to bypass this error.\n\n## Parameters\n\n- **indexValue**: The requested index into the array.\n- **arraySize**: The cardinality of the array.\n- **ansiConfig**: The configuration setting to alter ANSI mode.\n\n## Explanation\n\nUnlike [element_at](../sql/language-manual/functions/element_at.md) and [elt](../sql/language-manual/functions/elt.md), a reference `indexValue` into an array using the [arrayExpr[indexValue]](../sql/language-manual/functions/bracketsign.md) syntax must be between `0` for the first element and `arraySize - 1` for the last element.\n\nA negative `indexValue` or a value greater or equal to `arraySize` is not allowed.\n\n## Mitigation\n\nThe mitigation for this error depends on the intent:\n\n- **Does the provided `indexValue` assume 1-based indexing?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md), [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`, or [arrayExpr[indexValue - 1]](../sql/language-manual/functions/bracketsign.md) to resolve the correct array element.\n\n- **Is the `indexValue` negative expecting to retrieve element relative to the end of the array?**\n\n  Use [element_at(arrayExpr, indexValue)](../sql/language-manual/functions/element_at.md) or [elt(arrayExpr, indexValue)](../sql/language-manual/functions/elt.md)`.\n  Adjust for 1-based indexing if necessary.\n\n- **Do you expect to get a `NULL` value to be returned for elements outside the cardinality of the index?**\n \n  If you can change the expression, use [try_element_at(arrayExpr, indexValue + 1)](../sql/language-manual/functions/try_element_at.md) to tolerate references out of bound.\n  Note the 1-based indexing for `try_element_at`.\n\n  If you cannot change the expression, as a last resort, temporarily set the `ansiConfig` to `false` to tolerate references out of bound. \n\n## Examples\n\n```sql\n-- An INVALID_ARRAY_INDEX error because of mismatched indexing\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  [INVALID_ARRAY_INDEX] The index 3 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to false to bypass this error.\n  \n-- Using element_at instead for 1-based indexing\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(1), (3) AS T(index);\n  a\n  c\n\n-- Adjusting the index to be 0-based\n> SELECT array('a', 'b', 'c')[index -1] FROM VALUES(1), (3) AS T(index);\n\n-- Tolerating out of bound array index with adjustment to 1-based indexing\n> SELECT try_element_at(array('a', 'b', 'c'), index + 1) FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n  \n-- An INVALID_ARRAY_INDEX error because of negative index\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(-1), (2) AS T(index);\n  [INVALID_ARRAY_INDEX] The index -1 is out of bounds. The array has 3 elements. If necessary set \"ANSI_MODE\" to \"false\" to bypass this error.\n\n-- Using element_at to index relative to the end of the array\n> SELECT element_at(array('a', 'b', 'c'), index) FROM VALUES(-1), (2) AS T(index);\n  c\n  b\n  \n-- Tolerating an out of bound index by setting ansiConfig in Databricks SQL\n> SET ANSI_MODE = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET ANSI_MODE = true;\n\n-- Tolerating an out of bound index by setting ansiConfig in Databricks Runtime\n> SET spark.sql.ansi.enabled = false;\n> SELECT array('a', 'b', 'c')[index] FROM VALUES(1), (3) AS T(index);\n  b\n  NULL\n> SET spark.sql.ansi.enabled = true;\n```\n\n## Related\n\n- [_](../sql/language-manual/functions/bracketsign.md)\n- [_](../sql/language-manual/parameters/ansi_mode.md)\n- [_](../sql/language-manual/functions/element_at.md)\n- [_](../sql/language-manual/functions/elt.md)\n- [_](../sql/language-manual/functions/try_element_at.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I handle accessing an element at an invalid index in an array in Databricks?", "gold_answer": "To handle accessing an element at an invalid index in an array in Databricks, you can use the SQL function `get()` to tolerate accessing the element at the invalid index and return NULL instead. If necessary, you can also set the `<ansiConfig>` to \"false\" to bypass the INVALID_ARRAY_INDEX error."}
{"hard_neg_docs": [{"page_content": "## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How should date and time values be adjusted to avoid rendering issues in the cohort visualizer?", "gold_answer": "To avoid rendering issues in the cohort visualizer, date and time values should be adjusted by your local UTC offset since the cohort visualizer converts all date and time values to GMT before rendering."}
{"hard_neg_docs": [{"page_content": "## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/visualizations/cohorts.html\nContent:\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/visualizations/cohorts.html", "seq_num": 184, "description": "", "text": "---\ndescription: Learn about cohort visualizations in <Databricks> notebooks.\n---\n\n# Cohort visualization\n\nA cohort analysis examines the outcomes of predetermined groups, called cohorts, as they progress through a set of stages. The signature characteristic of a cohort chart is its comparison of the change in a variable across two different time series. For example, a common cohort definition is users by sign-up period and their usage pattern by day. Other examples include:\n\n- Monthly hard drive failure statistics by month\n- Weekly supplier delivery performance by week\n- Monthly average class GPA's by month\n\nWhile there are many ways to define the stages of a Cohort analysis, <Databricks> supports cohort visualizations with daily, weekly, or monthly stages. Also, <Databricks> cohort charts compare a cohort's measurements in a given period against that group's initial population size.\n\n## Data format\n\n<Databricks> expects your input samples to have the following fields:\n\n- **Cohort Date**: the date that uniquely identifies a cohort. Suppose you're visualizing monthly user activity by sign-up date, your cohort date for all users that signed-up in January 2018 would be January 1st, 2018. The cohort date for any user that signed-up in February would be February 1st, 2018.\n- **Period**: a count of how many periods transpired since the cohort date as of this sample. If you are grouping users by sign-up month, then your period will be the count of months since these users signed up. In the above example, a measurement of activity in July for users that signed up in January would yield a period value of 7 because seven periods have transpired between January and July.\n- **Count Satisfying Target**: your actual measurement of this cohort's performance in the given period. In the above example, if thirty users who signed up in January showed activity in July then the Count Satisfying Target would be 30.\n- **Total Cohort Size**: the denominator to use to calculate the percentage of a cohort's target satisfaction for a given period. Continuing the example above, if seventy-two users signed up in January then the Total Cohort Size would be 72. When the visualization is rendered, the value would be displayed as `41.67%` (`32 ÷ 72`).\n\n## Cohort date notes\n\nEven if you define your cohorts by month or week, <Databricks> expects the values in your **Cohort Date** column to be a full date value. If you are grouping by month, `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, **not** `2018-01`.\n\nThe cohort visualizer converts all date and time values to GMT before rendering. To avoid rendering issues, you should adjust the date times returned from your database by your local UTC offset.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What should the values in the Cohort Date column be if cohorts are defined by month or week?", "gold_answer": "If cohorts are defined by month or week, the values in the **Cohort Date** column should be a full date value. For example, if you are grouping by month, a date like `2018-01-18` should be shortened to `2018-01-01` or any other full date in January, not just `2018-01`."}
{"hard_neg_docs": [{"page_content": "The following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\n---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What privileges are required to COPY INTO an existing table in Unity Catalog?", "gold_answer": "To COPY INTO an existing table in Unity Catalog, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table."}
{"hard_neg_docs": [{"page_content": "The following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations", "fmt_content": "Source: https://docs.databricks.com/ingestion/copy-into/unity-catalog.html\nContent:\n---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations", "metadata": {"source": "https://docs.databricks.com/ingestion/copy-into/unity-catalog.html", "seq_num": 1610, "description": "", "text": "---\ndescription: Learn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO.\n---\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n# Use COPY INTO to load data with <UC>\n\nLearn how to use COPY INTO to ingest data to <UC> managed or external tables from any source and file format supported by COPY INTO. <UC> adds new options for configuring secure access to raw data, allowing you to use <UC> external locations or storage credentials to access data in cloud object storage.\n\n## COPY INTO a <UC> table\n\nYou can target a <UC> table using a three tier identifier (`<catalog_name>.<database_name>.<table_name>`). You can use the `USE CATALOG <catalog_name>` and `USE <database_name>` commands to set the default catalog and database for your current query or notebook.\n\nIf you wish to COPY INTO an existing table, you must have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table and `MODIFY` privileges on the table.\n\nYou can COPY INTO from any source location you can access, including cloud object storage locations configured with [temporary credentials](/ingestion/copy-into/temporary-credentials.md).\n\n## Load data from external locations\n\nTo load data from a <UC> external location, you must have the `READ FILES` privilege granted on that location. External location privileges apply to all nested directories under the specified location.\n\nFor example, if you have access to an external location defined with the URL `s3://landing-bucket/raw-data`, the following commands are valid:\n\n```sql\nCOPY INTO landing_table\nFROM 's3://landing-bucket/raw-data'\nFILEFORMAT = PARQUET;\n\nCOPY INTO json_table\nFROM 's3://landing-bucket/raw-data/json'\nFILEFORMAT = JSON;\n```\n\nPermissions on this external location do not grant any privileges on directories above or parallel to the location specified. For example, **neither of the following commands are valid**:\n\n```sql\nCOPY INTO parent_table\nFROM 's3://landing-bucket'\nFILEFORMAT = PARQUET;\n\nCOPY INTO sibling_table\nFROM 's3://landing-bucket/json-data'\nFILEFORMAT = JSON;\n```\n\n## Specifying storage credentials to access data\n\nDatabricks recommends using external locations to explicitly grant permissions on directories used in data ingestion. COPY INTO also supports using <UC> storage credentials to access data. The `READ FILES` privilege will grant you access to ingest data with COPY INTO from all cloud object storage locations accessible with the storage credential. Because storage credentials may have broader privileges than external locations, use caution when granting access to storage credentials.\n\nThe following example loads data from S3 and ADLS Gen2 using <UC> storage credentials to provide access to the source data.\n\n```sql\nCOPY INTO my_json_data\nFROM 's3://landing-bucket/json-data' WITH (\n  CREDENTIAL `bucket-creds`\n)\nFILEFORMAT = JSON;\n\nCOPY INTO my_json_data\nFROM 'abfss://container@storageAccount.dfs.core.windows.net/jsonData' WITH (\n  CREDENTIAL `adls-creds`\n)\nFILEFORMAT = JSON;\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I use COPY INTO to load data into a Unity Catalog table?", "gold_answer": "To use COPY INTO to load data into a Unity Catalog (UC) table, follow these steps:\n\n1. Ensure that you have `USE CATALOG` and `USE SCHEMA` privileges on the catalog and database (schema) containing the table you wish to load data into.\n2. You also need `MODIFY` privileges on the table itself.\n3. Identify the Unity Catalog table you want to target using a three-tier identifier format, which includes `<catalog_name>.<database_name>.<table_name>`.\n4. Optionally, set the default catalog and database for your current query or notebook by using the `USE CATALOG <catalog_name>` and `USE <database_name>` SQL commands.\n5. Execute the COPY INTO command from the source location you have access to, which can include cloud object storage locations. You may need to configure these storage locations with temporary credentials if required.\n\nBy following these guidelines, you can ingest data into Unity Catalog managed or external tables from any source and file format supported by the COPY INTO command, while also configuring secure access to raw data as needed."}
{"hard_neg_docs": [{"page_content": ".. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How should I approach tuning the `cloudFiles.maxFileAge` parameter?", "gold_answer": "You should approach tuning the `cloudFiles.maxFileAge` parameter by starting from a generous expiration time and then working downwards to a lower value. For example, you could start with a setting like `\"1 year\"` and then adjust it to a shorter period such as `\"9 months\"` based on your requirements. It's important to note that if you set this option when you first start the stream, you will not ingest data that is older than `cloudFiles.maxFileAge`. Therefore, if you wish to ingest older data, you should not set this option as you start your stream."}
{"hard_neg_docs": [{"page_content": ".. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/ingestion/auto-loader/production.html\nContent:\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/ingestion/auto-loader/production.html", "seq_num": 1603, "description": "", "text": "---\ndescription: Learn how to configure Auto Loader for production workloads\n---\n\n# Configure Auto Loader for production workloads\n\nDatabricks recommends that you follow [the streaming best practices](/structured-streaming/production.md) for running Auto Loader in production.\n\nDatabricks recommends using [Auto Loader in Delta Live Tables](dlt.md) for incremental data ingestion. <DLT> extends functionality in <AS> Structured Streaming and allows you to write just a few lines of declarative Python or SQL to deploy a production-quality data pipeline with:\n\n- Autoscaling compute infrastructure for cost savings\n- Data quality checks with [expectations](/delta-live-tables/expectations.md)\n- Automatic [schema evolution](schema.md) handling\n- Monitoring via metrics in the [event log](/delta-live-tables/observability.md#event-log)\n\n## Monitoring Auto Loader\n\n<a id=\"cloud-files-state\"></a>\n\n### Querying files discovered by Auto Loader\n\n.. note:: The `cloud_files_state` function is available in <DBR> 10.5 and above.\n\nAuto Loader provides a SQL API for inspecting the state of a stream. Using the `cloud_files_state` function, you can find metadata about files that have been discovered by an Auto Loader stream. Simply query from `cloud_files_state`, providing the checkpoint location associated with an Auto Loader stream.\n\n```sql\nSELECT * FROM cloud_files_state('path/to/checkpoint');\n```\n\n### Listen to stream updates\n\nTo further monitor Auto Loader streams, Databricks recommends using <AS>'s [Streaming Query Listener interface](/structured-streaming/stream-monitoring.md).\n\nAuto Loader reports metrics to the Streaming Query Listener at every batch. You can view how many files exist in the backlog and how large the backlog is in the `numFilesOutstanding` and `numBytesOutstanding` metrics under the **Raw Data** tab in the [streaming query progress dashboard](/getting-started/streaming.md#stream-dashboard):\n\n```json\n{\n  \"sources\" : [\n    {\n      \"description\" : \"CloudFilesSource[/path/to/source]\",\n      \"metrics\" : {\n        \"numFilesOutstanding\" : \"238\",\n        \"numBytesOutstanding\" : \"163939124006\"\n      }\n    }\n  ]\n}\n```\n\nIn <DBR> 10.1 and later, when using file notification mode, the metrics will also include the approximate number of file events that are in the cloud queue as `approximateQueueSize` for AWS and Azure.\n\n## Cost considerations\n\nWhen running Auto Loader, your main source of costs would be the cost of compute resources and file discovery.\n\nTo reduce compute costs, Databricks recommends using Databricks Jobs to schedule Auto Loader as batch jobs using `Trigger.AvailableNow` instead of running it continuously as long as you don't have low latency requirements. See [_](/structured-streaming/triggers.md).\n\nFile discovery costs can come in the form of LIST operations on your storage accounts in directory listing mode and API requests on the subscription service, and queue service in file notification mode. To reduce file discovery costs, Databricks recommends:\n\n- Providing a `ProcessingTime` trigger when running Auto Loader continuously in directory listing mode\n- Architecting file uploads to your storage account in lexical ordering to leverage [_](/ingestion/auto-loader/directory-listing-mode.md#incremental-listing) when possible\n- Using <DBR> 9.0 or later in directory listing mode, especially for deeply nested directories\n- [Leveraging file notifications](/ingestion/auto-loader/file-notification-mode.md) when incremental listing is not possible\n- Using [resource tags](/ingestion/auto-loader/options.md#file-notification-options) to tag resources created by Auto Loader to track your costs\n\n## Using Trigger.AvailableNow and rate limiting\n\n.. note:: Available in <DBR> 10.1 for Scala only.\n\n   Available in <DBR> 10.2 and above for Python and Scala.\n\nAuto Loader can be scheduled to run in Databricks Jobs as a batch job by using `Trigger.AvailableNow`. The `AvailableNow` trigger will instruct Auto Loader to process all files that arrived **before** the query start time. New files that are uploaded after the stream has started will be ignored until the next trigger.\n\nWith `Trigger.AvailableNow`, file discovery will happen asynchronously with data processing and data can be processed across multiple micro-batches with rate limiting. Auto Loader by default processes a maximum of 1000 files every micro-batch. You can configure `cloudFiles.maxFilesPerTrigger` and `cloudFiles.maxBytesPerTrigger` to configure how many files or how many bytes should be processed in a micro-batch. The file limit is a hard limit but the byte limit is a soft limit, meaning that more bytes can be processed than the provided `maxBytesPerTrigger`. When the options are both provided together, Auto Loader will process as many files that are needed to hit one of the limits.\n\n<a id=\"max-file-age\"></a>\n\n## Event retention\n\n.. note:: Available in <DBR> 8.4 and above.\n\nAuto Loader keeps track of discovered files in the checkpoint location using RocksDB to provide exactly-once ingestion guarantees. For high volume datasets, you can use the `cloudFiles.maxFileAge` option to expire events from the checkpoint location to reduce your storage costs and Auto Loader start up time. The minimum value that you can set for `cloudFiles.maxFileAge` is `\"14 days\"`. Deletes in RocksDB appear as tombstone entries, therefore you should expect the storage usage to increase temporarily as events expire before it starts to level off.\n\n.. warning::\n    `cloudFiles.maxFileAge` is provided as a cost control mechanism for high volume datasets, ingesting in the order of millions of files every hour. Tuning `cloudFiles.maxFileAge` incorrectly can lead to data quality issues. Therefore, Databricks doesn't recommend tuning this parameter unless absolutely required.\n\nTrying to tune the `cloudFiles.maxFileAge` option can lead to unprocessed files being ignored by Auto Loader or already processed files expiring and then being re-processed causing duplicate data. Here are some things to consider when choosing a `cloudFiles.maxFileAge`:\n\n- If your stream restarts after a long time, file notification events that are pulled from the queue that are older than `cloudFiles.maxFileAge` are ignored. Similarly, if you use directory listing, files that may have appeared during the down time that are older than `cloudFiles.maxFileAge` are ignored.\n- If you use directory listing mode and use `cloudFiles.maxFileAge`, for example set to `\"1 month\"`, you stop your stream and restart the stream with `cloudFiles.maxFileAge` set to `\"2 months\"`, all files that are older than 1 month, but more recent than 2 months are reprocessed.\n\nThe best approach to tuning `cloudFiles.maxFileAge` would be to start from a generous expiration, for example, `\"1 year\"` and working downwards to something like `\"9 months\"`. If you set this option the first time you start the stream, you will not ingest data older than `cloudFiles.maxFileAge`, therefore, if you want to ingest old data you should not set this option as you start your stream.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What happens to file notification events that are older than `cloudFiles.maxFileAge` if the stream restarts after a long time?", "gold_answer": "If the stream restarts after a long time, file notification events that are pulled from the queue and are older than `cloudFiles.maxFileAge` are ignored. This also applies to files that may have appeared during the downtime if you use directory listing mode; those that are older than `cloudFiles.maxFileAge` are ignored as well."}
{"hard_neg_docs": [{"page_content": "```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How do I create secrets in a Databricks-backed scope?", "gold_answer": "To create secrets in a Databricks-backed scope, you can use the following commands and enter the secret values in the editor that opens:\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\nThese commands are used for adding the secrets `username` and `password` to the scope named `jdbc`."}
{"hard_neg_docs": [{"page_content": "```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:", "fmt_content": "Source: https://docs.databricks.com/security/secrets/example-secret-workflow.html\nContent:\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:", "metadata": {"source": "https://docs.databricks.com/security/secrets/example-secret-workflow.html", "seq_num": 530, "description": "", "text": "---\ndescription: Learn how to use secrets to set up JDBC credentials by creating a secret scope, creating secrets, and using them in a notebook.\n---\n\n\n# Secret workflow example\n\n.. gcp::\n\n  .. note:: This article mentions the CLI, which is not available in this release of <Databricks> on <GCP>. For secrets you can also use the [_](/dev-tools/api/latest/secrets.md).\n\n<!--  @TODO  @GCP-secrets-api should be mentioned more here, right? expand it , and that way it's more useful for GCP folks who don't have CLI yet       -->\n\nIn this workflow example, we use secrets to set up JDBC credentials for connecting to an Azure Data Lake Store.\n\n## Create a secret scope\n\nCreate a secret scope called `jdbc`.\n\n.. aws-gcp::\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n.. azure::\n    To create a Databricks-backed secret scope:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc\n    ```\n\n    To create an Azure Key Vault-backed secret scope, follow the instructions in [_](/security/secrets/secret-scopes.md#akv-ss).\n\n.. note::\n    If your account does not have the <Security plan>, you must create the scope with `MANAGE` permission granted to all users (\"users\"). For example:\n\n    ```bash\n    databricks secrets create-scope --scope jdbc --initial-manage-principal users\n    ```\n\n## Create secrets\n\n.. azure::\n    The method for creating the secrets depends on whether you are using an Azure Key Vault-backed scope or a Databricks-backed scope.\n\n    ### Create the secrets in an Azure Key Vault-backed scope\n\n    Add the secrets `username` and `password` using the Azure [Set Secret](https://learn.microsoft.com/rest/api/keyvault/secrets/set-secret/set-secret?tabs=HTTP) REST API or Azure portal UI:\n\n    ![Add secrets to Azure Key Vault](/_static/images/secrets/azure-kv-secrets.png)\n\n    ### Create the secrets in a Databricks-backed scope\n\nAdd the secrets `username` and `password`. Run the following commands and enter the secret values in the opened editor.\n\n```bash\ndatabricks secrets put --scope jdbc --key username\ndatabricks secrets put --scope jdbc --key password\n```\n\n<a id=\"secret-example-notebook\"></a>\n\n## Use the secrets in a notebook\n\nIn a notebook, read the secrets that are stored in the secret scope `jdbc` to configure a JDBC connector:\n\n```scala\nval driverClass = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\nval connectionProperties = new java.util.Properties()\nconnectionProperties.setProperty(\"Driver\", driverClass)\n\nval jdbcUsername = dbutils.secrets.get(scope = \"jdbc\", key = \"username\")\nval jdbcPassword = dbutils.secrets.get(scope = \"jdbc\", key = \"password\")\nconnectionProperties.put(\"user\", s\"${jdbcUsername}\")\nconnectionProperties.put(\"password\", s\"${jdbcPassword}\")\n```\n\nYou can now use these `ConnectionProperties` with the JDBC connector to talk to your data source.\nThe values fetched from the scope are never displayed in the notebook (see [_](/security/secrets/redaction.md)).\n\n## Grant access to another group\n\n.. note::\n    This step requires that your account have the <Security plan>.\n\nAfter verifying that the credentials were configured correctly, share these credentials with the `datascience` group to use for their analysis.\n\nGrant the `datascience` group read-only permission to these credentials by making the following request:\n\n```bash\ndatabricks secrets put-acl --scope jdbc --principal datascience --permission READ\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I read secrets stored in a secret scope in a notebook?", "gold_answer": "To read secrets that are stored in a secret scope in a notebook, you would use the functionality provided by Databricks to configure, for example, a JDBC connector with the secrets. While the specific method to read the secrets is not directly mentioned in the provided text, typically in Databricks notebooks, you would use the Databricks utilities API (dbutils) to retrieve secrets stored in a secret scope. An example command in a Databricks notebook would look like this:\n\n```python\nusername = dbutils.secrets.get(scope=\"jdbc\", key=\"username\")\npassword = dbutils.secrets.get(scope=\"jdbc\", key=\"password\")\n```\n\nHere, `dbutils.secrets.get` is used to retrieve the secrets with the specified `scope` and `key`. Replace \"jdbc\" with the name of your secret scope and \"username\" or \"password\" with the respective key of the secret you want to access."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the \"Package name\" field in the setup.py script?", "gold_answer": "The purpose of the \"Package name\" field in the `setup.py` script is to specify the name of the Python package that you are creating. When you enter `my_test_package` as the package name, this value is used to identify the package in the context of Python package management and distribution. It is the name that you assign to the `name` variable in the `setup.py` script, and it is used to refer to the package when installing it, resolving dependencies, or including it in a project."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>", "fmt_content": "Source: https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html\nContent:\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>", "metadata": {"source": "https://docs.databricks.com/workflows/jobs/how-to/use-python-wheels-in-workflows.html", "seq_num": 364, "description": "", "text": "---\ndescription: Learn how to package your Python code in a Python wheel and use it in <a Databricks> Jobs workflow.\n---\n\n# Use a Python wheel in <a Databricks> job\n\nA Python [wheel](https://peps.python.org/pep-0427/) is a standard way to package and distribute the files required to run a Python application. Using the Python wheel task, you can ensure fast and reliable installation of Python code in your <Databricks> jobs. This article provides an example of creating a Python wheel and a job that runs the application packaged in the wheel. In this example, you will:\n\n- Create the Python files defining an example application.\n- Bundle the example files into a Python wheel.\n- Create a job to run the Python wheel.\n- Run the job and view the results.\n\n<a id=\"requirements\"></a>\n\n## Before you begin\n\nYou need the following to complete this example:\n\n- Python3\n- The Python `wheel` and `setuptool` packages. You can use `pip` to install these packages. For example, you can run the following command to install these packages:\n\n  ```bash\n  pip install wheel setuptools\n  ```\n\n<a id=\"create-local-dir\"></a>\n\n## Step 1: Create a local directory for the example\n\nCreate a local directory to hold the example code and generated artifacts, for example, `databricks_wheel_test`.\n\n<a id=\"create-python-example\"></a>\n\n## Step 2: Create the example Python script\n\nThe following Python example is a simple script that reads input arguments and prints out those arguments. Copy this script and save it to a path called `my_test_code/__main__.py` in the directory you created in the previous step.\n\n```python\n\"\"\"\nThe entry point of the Python Wheel\n\"\"\"\n\nimport sys\n\ndef main():\n  # This method will print the provided arguments\n  print('Hello from my func')\n  print('Got arguments:')\n  print(sys.argv)\n\nif __name__ == '__main__':\n  main()\n```\n\n<a id=\"create-metadata\"></a>\n\n## Step 3: Create a metadata file for the package\n\nThe following file contains metadata describing the package. Save this to a path called `my_test_code/__init__.py` in the directory you created in step 1.\n\n```python\n__version__ = \"0.0.1\"\n__author__ = \"Databricks\"\n```\n\n<a id=\"create-wheel\"></a>\n\n## Step 4: Create the wheel\n\nConverting the Python artifacts into a wheel requires specifying package metadata such as the package name and entry points. The following script defines this metadata.\n\n.. note:: The `entry_points` defined in this script are used to run the package in the <Databricks> workflow. In each value in `entry_points`, the value before `=` (in this example, `run`) is the name of the entry point and is used to configure the wheel task.\n\n#. Save this script in a file named `setup.py` in the root of the directory you created in step 1:\n\n```python\nfrom setuptools import setup, find_packages\n\nimport my_test_code\n\nsetup(\n  name='my_test_package',\n  version=my_test_code.__version__,\n  author=my_test_code.__author__,\n  url='https://databricks.com',\n  author_email='john.doe@databricks.com',\n  description='my test wheel',\n  packages=find_packages(include=['my_test_code']),\n  entry_points={\n    'group_1': 'run=my_test_code.__main__:main'\n  },\n  install_requires=[\n    'setuptools'\n  ]\n)\n```\n\n#. Change into the directory you created in step 1, and run the following command to package your code into the wheel distribution:\n\n   ```bash\n   python3 setup.py bdist_wheel\n   ```\n\nThis command creates the wheel and saves it to the `dist/my_test_package-0.0.1-py3.none-any.whl` file in your directory.\n\n<a id=\"create-job\"></a>\n\n## Step 5. Create <a Databricks> job to run the Python wheel\n\n#. Go to your <Databricks> landing page and do one of the following:\n\n  - In the sidebar, click <Jobs Icon> **Workflows** and click <Create Job Button>.\n  - In the sidebar, click <New Icon> **New** and select **Job** from the menu.\n\n#. In the task dialog box that appears on the **Tasks** tab, replace **Add a name for your job...** with your job name, for example, `Python wheel example`.\n\n#. In **Task name**, enter a name for the task, for example, `python_wheel_task`.\n\n#. In **Type**, select **Python wheel**.\n\n#. In **Package name**, enter `my_test_package`. The package name is the value assigned to the `name` variable in the `setup.py` script.\n\n#. In **Entry point**, enter `run`. The entry point is one of the values specified in the `entry_points` collection in the `setup.py` script. In this example, `run` is the only entry point defined.\n\n#. Click **Add** under **Dependent Libraries**. In the **Add dependent library** dialog, under **Library Type**, click **Python Whl**. Click **Drop WHL here** and select the `my_test_package-0.0.1-py3-none-any.whl` file created in step 4.\n\n#. Click **Add**.\n\n#. In **Parameters**, select **Positional arguments** or **Keyword arguments** to enter the key and the value of each parameter. Both positional and keyword arguments are passed to the Python wheel task as command-line arguments.\n\n   - To enter positional arguments, enter parameters as a JSON-formatted array of strings, for example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n\n   - To enter keyword arguments, click **+ Add** and enter a key and value. Click **+ Add** again to enter more arguments.\n\n#. Click **Save task**.\n\n<a id=\"run-job\"></a>\n\n## Step 6: Run the job and view the job run details\n\nClick <Run Now Button> to run the workflow. To view [details for the run](/workflows/jobs/jobs.md#job-run-details), click **View run** in the **Triggered run** pop-up or click the link in the **Start time** column for the run in the [job runs](/workflows/jobs/jobs.md#view-job-run-list) view.\n\nWhen the run completes, the output displays in the **Output** panel, including the arguments passed to the task.\n\n<a id=\"learn-more\"></a>\n\n## Next steps\n\nTo learn more about creating and running <Databricks> jobs, see [_](/workflows/jobs/jobs.md).\n\n.. include:: /shared/replacements.md\n\n", "timestamp": 0, "next": ""}}], "question": "How can I pass parameters to the Python wheel task in Databricks workflows?", "gold_answer": "You can pass parameters to the Python wheel task in Databricks workflows by selecting either \"Positional arguments\" or \"Keyword arguments\" in the Parameters section of the task configuration. Here's how you do it:\n\n- For positional arguments, enter the parameters as a JSON-formatted array of strings. For example: `[\"first argument\",\"first value\",\"second argument\",\"second value\"]`.\n  \n- For keyword arguments, click **+ Add** and enter a key and a value for each parameter you want to pass. You can click **+ Add** again to enter more arguments.\n\nOnce you've entered all your arguments, you can click **Save task** to finalize the configuration."}
{"hard_neg_docs": [{"page_content": "<a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\n<a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}, {"page_content": "Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\nRecreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\n### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}], "question": "How do I delete a Databricks account?", "gold_answer": "To delete a Databricks account, you must follow these steps:\n\n1. Cancel your Databricks subscription.\n2. Delete all Unity Catalog metastores in the account, as indicated in the provided document link to the instructions on how to do so.\n3. After deleting all metastores associated with your organization's account, you can initiate the process to delete your account.\n\nIf you need assistance with deleting your Databricks account, you should reach out to your account team for help or file a ticket at [help.databricks.com](https://help.databricks.com)."}
{"hard_neg_docs": [{"page_content": "<a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\n<a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}, {"page_content": "Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\nRecreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings/account.html\nContent:\n### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings/account.html", "seq_num": 1481, "description": "", "text": "---\ndescription: Learn how to manage your <Databricks> account subscription. Including deleting the account.\n---\n\n# Manage your subscription\n\n.. aws-gcp::\n\n  This article describes how to manage your billing, subscription level, as well as how to cancel your subscription and delete your account. Only the Databricks account administrators can manage a Databricks subscription.\n\n.. azure::\n\n  This article explains how to assign <Databricks> admins and how to upgrade, downgrade, or cancel your <Databricks> subscription.\n\n  ## Assign account admins\n\n  The first <Databricks> account admin must be an <AAD> (Azure AD) Global Administrator at the time that they first log in to the <Databricks> account console. Upon first login, that user becomes <a Databricks> account admin and no longer needs the <AAD> Global Administrator role to access the <Databricks> account.\n\n  After account admins sign in to the <Databricks> account, they can [add new account admins and users](/administration-guide/users-groups/users.md). Additional account admins do not require specific roles in <AAD>.\n\n  ## Assign workspace admins\n\n  The user who creates the <Databricks> workspace is automatically assigned as an admin in the workspace. In addition, any user with the Contributor or Owner role on the <Databricks> workspace in the Azure portal will be created as an admin in the workspace when they click **Launch Workspace**. Upon first login, that user becomes <a Databricks> workspace admin and no longer needs the Contributor or Owner role to access the workspace. Removing the Contributor or Owner role in Azure will not automatically remove the user as a workspace admin. For more details about Azure roles, see [Managing access using RBAC in the Azure portal](https://learn.microsoft.com/azure/role-based-access-control/role-assignments-portal).\n\n  After workspace admins sign in to <Databricks>, they can [add new workspace admins and users](/administration-guide/users-groups/users.md). Additional workspace admins do not require specific roles in <AAD>.\n\n  .. warning:: The Contributor or Owner role should not be assigned to users that should not have admin access to the workspace.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade or Downgrade <a Databricks> Workspace\n\n  <Databricks> is available with two [pricing options](https://azure.microsoft.com/pricing/details/databricks/), Standard and Premium, which offer features for different types of workloads. You specify an option when you create <a Databricks> workspace. If you change your mind about the features you want in the workspace, you can change its pricing tier. This section describes how to upgrade <a Databricks> workspace from Standard to Premium and downgrade from Premium to Standard.\n\n  For workspaces in Databricks-managed virtual networks, you can perform an upgrade or downgrade using the Azure portal, an Azure Resource Manager (ARM) template, or the Azure REST API and CLI.\n\n  For workspaces in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) (that is, workspaces created using \"VNet injection\"), you can perform an upgrade or downgrade using the Azure portal or an Azure Resource Manager (ARM) template.\n\n  .. note:: Notebooks, users, and cluster configurations are retained when you upgrade or downgrade a workspace, but active clusters might be terminated.\n\n  ### Azure portal\n\n  To upgrade or downgrade the <Databricks> workspace pricing option in the Azure portal, use [the same procedure](https://learn.microsoft.com/azure/azure-databricks/quickstart-create-databricks-workspace-portal) that you use to create new Databricks workspaces.\n\n  Recreate the workspace with all the same parameters (Workspace name, Subscription, Resource Group, and Location), only changing the **Pricing Tier** setting.\n\n  ### ARM template\n\n  To upgrade, use either the [base template](https://azure.microsoft.com/resources/templates/databricks-workspace/) or [custom CIDR range](https://azure.microsoft.com/resources/templates/databricks-workspace-with-custom-vnet-address/) template, depending on what was used for the Standard workspace. Recreate the workspace with exactly the same parameters:\n\n  - Subscription\n  - Resource Group\n  - Workspace name\n  - Location\n  - CIDR range (only if you use a template that specifies a _custom_ CIDR range)\n\n  Set the `pricingTier` parameter to Premium.\n\n  To downgrade from Premium to Standard, follow the same procedure and set the `pricingTier` to Standard.\n\n  ### REST API or CLI\n\n  .. note:: You cannot upgrade or downgrade the pricing plan for workspaces created in [your own virtual network](/administration-guide/cloud-configurations/azure/vnet-inject.md) using the Azure Databricks workspace creation API or Azure CLI. You must use the [Azure portal](#azure-portal) or an [ARM template](#arm-template).\n\n  To upgrade, use the [Azure Databricks workspace creation API](https://learn.microsoft.com/rest/api/databricks/workspaces/createorupdate) to recreate the workspace with exactly the same parameters as the Standard workspace, specifying the `sku` property as Premium. To use the API, you must register a client application with Azure AD and [get an access token](https://learn.microsoft.com/rest/api/azure/).\n\n  You can also perform the upgrade using the Azure CLI [Resource Update](https://learn.microsoft.com/cli/azure/resource?view=azure-cli-latest#az-resource-update&preserve-view=true) command. You won't need to register a client application or get an access token separately.\n\n  To downgrade from Premium to Standard, follow the same procedure and specify the `sku` property as Standard.\n\n  ## Delete <a Databricks> service\n\n  To delete an Azure Databricks service:\n\n  #. Log into your Azure Databricks workspace as the account owner (the user who created the service).\n  #. Click your username in the top bar of the workspace, and select **Azure** from the dropdown menu.\n  #. In the Azure Databricks service, click **Delete** and then **OK**.\n\n  ## Cancel an Azure subscription\n\n  To cancel your Azure subscription, see [Cancel your subscription for Azure](https://learn.microsoft.com/azure/billing/billing-how-to-cancel-azure-subscription).\n\n.. aws::\n\n  ## Manage billing\n\n  The way you manage billing depends on the type of business relationship you have with Databricks.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  If you subscribed to Databricks using [AWS Marketplace](/getting-started/index.md#aws-pay), your Databricks charges appear alongside your other AWS costs. To manage your payment methods, use the AWS console.\n\n  ### Pay-as-you-go accounts paid by credit card to Databricks\n\n  If you pay Databricks directly by [credit card](/getting-started/index.md#credit-card), follow these instructions to manage billing:\n\n  #. As the Databricks account owner or an account-level administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Click the **Settings** icon in the sidebar.\n  #. If no payment method has been added, click **Add billing information** and fill in the form.\n  #. To edit your existing payment method, click **Edit**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, Databricks has your billing information on file. Contact your Databricks representative to make updates.\n\n  <a id=\"upgrade-downgrade\"></a>\n\n  ## Upgrade your subscription\n\n  Databricks has multiple pricing [plans](https://databricks.com/product/aws-pricing). If your current plan is no longer meeting all of your organization's needs, you can upgrade to a higher-tier plan. The instructions for upgrading your account are different for pay-as-you-go accounts and contract accounts.\n\n  ### Pay-as-you-go accounts\n\n  If you have a pay-as-you-go account, follow these instructions to upgrade to another plan.\n\n  .. note:: To upgrade from Databricks Community Edition to the full Databricks platform, sign up for the [Databricks Free Trial](https://databricks.com/try-databricks) or [contact a Databricks representative](https://databricks.com/company/contact). For more information, see [_](/getting-started/index.md).\n\n  #. As <a Databricks> account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login)\n  #. Click the **Settings** icon in the sidebar.\n\n     Your current plan is indicated on the page.\n\n    #. To upgrade to a new plan, click **Change plan**.\n  #. To select a plan or to see details for a plan, click the tile for that plan.\n  #. Click **Save**.\n\n  You cannot downgrade your plan directly in the account console. To request a downgrade:\n\n  #. Click **Change plan**.\n  #. Click the tile for the plan. For example, if you are subscribed to **Enterprise**, click **Premium** or **Standard**.\n  #. Click **Contact us**. A Databricks representative will contact you.\n\n  For more information, see [the plans and pricing page](https://databricks.com/product/aws-pricing).\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription level.\n\n  <a id=\"cancel\"></a>\n\n  ## Cancel your Databricks subscription\n\n  After you cancel your subscription:\n\n  - You can no longer access workspaces, notebooks, or data in your Databricks account.\n  - In accordance with the Databricks [terms of service](https://databricks.com/mcsa), any Customer Content contained within workspaces tied to your subscription will be deleted within 30 days of cancellation.\n  - You can't sign up for a new subscription using the same email address. You must provide a new email address in the sign-up form.\n\n  ### Pay-as-you-go accounts through AWS Marketplace\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account administrator, log in to the [Databricks account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Log in to AWS as a user with the `aws-marketplace:Unsubscribe` permission in their IAM role.\n  #. Go to AWS Marketplace.\n  #. From the account name at the top right, select **Your Marketplace software**.\n  #. Click **Databricks Lakehouse Platform**.\n  #. From **Actions**, click **Cancel subscription**.\n\n  ### Pay-as-you-go accounts through Databricks\n\n  To cancel your subscription:\n\n  #. As the <Databricks> account owner or an account-level administrator, log in to the [account console](https://accounts.cloud.databricks.com/login).\n  #. Go to each workspace and export any data or notebooks you want to import into a different account.\n  #. Delete all workspaces.\n  #. Click the **Settings** icon in the sidebar.\n  #. Click **Cancel plan**.\n  #. In the confirmation dialog, click **Confirm delete**.\n\n  ### Contract accounts\n\n  If you have a contract with Databricks, contact your Databricks representative to manage your subscription.\n\n  ## Delete your Databricks account\n\n  When you delete an account, the account owner and all history associated with the management of your organization's account are deleted. Before you delete a Databricks account, you must first cancel your Databricks subscription and **[delete all Unity Catalog metastores](/data-governance/unity-catalog/create-metastore.md#delete) in the account**. After you delete all metastores associated with your organization's account, you can start the process to delete your account.\n\n  If you need to delete your Databricks account, reach out to your account team for assistance or file a ticket at [help.databricks.com](https://help.databricks.com).\n\n.. gcp::\n\n  <a id=\"plan\"></a>\n\n  ## Confirm or change your subscription plan\n\n  .. note:: This topic assumes you have a pay-as-you-go subscription that started as a free trial subscription. If you have a contract with Databricks, contact your Databricks representative for billing updates and questions.\n\n  You can compare the different <Databricks> [pricing", "timestamp": 0, "next": ""}}], "question": "How can I confirm or change my subscription plan in Databricks?", "gold_answer": "To confirm or change your subscription plan in Databricks, if you have a pay-as-you-go subscription that started as a free trial, you can compare the different Databricks pricing options. However, if you have a contract with Databricks, you should contact your Databricks representative for billing updates and questions. If the details on how to compare pricing options are not clear, you may need to visit the Databricks documentation or contact their support for further guidance."}
{"hard_neg_docs": [{"page_content": "## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}], "question": "When does Databricks end support for Internet Explorer 11?", "gold_answer": "Databricks ended support for Internet Explorer 11 on August 15, 2020."}
{"hard_neg_docs": [{"page_content": "## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/june.html\nContent:\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/june.html", "seq_num": 1931, "description": "", "text": "---\ndescription: June 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# June 2020\n\nThese features and <Databricks> platform improvements were released in June 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. aws::\n\n  ## Billable usage logs delivered to your own S3 bucket (Public Preview)\n\n  **June 30, 2020**\n\n  Databricks account owners can now configure daily delivery of billable usage logs in CSV file format to an AWS S3 storage bucket, where you can make the data available for usage analysis. Databricks delivers a separate monthly CSV file for each workspace in your account. This CSV file includes detailed data about the workspace's cluster usage in Databricks Units (DBUs) by cluster ID, billing SKU, cluster creator, cluster tags, and more. For a description of each CSV file column, see [_](/archive/admin-guide/usage.md#download).\n\n  This file has been available for download from the [Usage Overview tab](/archive/admin-guide/usage.md) in the Databricks account console, where it is accessible only by Databricks account owners. Delivery to your S3 buckets lets you give access to the users who need it and provide the data programmatically to your analysis tools, so you can view usage trends, perform chargebacks, and identify cost optimization opportunities.\n\n  For more information, see [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n## Databricks Connect now supports <DBR> 6.6\n\n**June 26, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.6.\n\n## <DBR> 7.0 ML GA\n\n**Jun 22, 2020**\n\n<DBR> 7.0 ML is built on top of <DBR> 7.0 and includes the following new features:\n\n- Notebook-scoped Python libraries and custom environments managed by conda and pip commands.\n- Updates for major Python packages including tensorflow, tensorboard, pytorch, xgboost, sparkdl, and hyperopt.\n- Newly added Python packages lightgbm, nltk, petastorm, and plotly.\n- RStudio Server Open Source v1.2.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0ml.md) release notes.\n\n## <DBR> 7.0 GA, powered by Apache Spark 3.0\n\n**June 18, 2020**\n\n<DBR> 7.0 is powered by Apache Spark 3.0 and now supports [Scala 2.12](https://github.com/scala/scala/releases/tag/v2.12.11).\n\nSpark 3.0 brings many additional features and improvements, including:\n\n* Adaptive Query Execution, a flexible framework to do adaptive execution in Spark SQL and support changing the number of reducers at runtime.\n* Redesigned pandas UDFs with type hints.\n* Structured Streaming web UI.\n* Better compatibility with ANSI SQL standards.\n* Join hints.\n\n<DBR> 7.0 adds:\n\n* Improved [Auto Loader](/ingestion/auto-loader/index.md) for processing new data files incrementally as they arrive on a cloud blob store during ETL.\n* Improved [COPY INTO command](/sql/language-manual/delta-copy-into.md) for loading data into <Delta> with idempotent retries.\n* Many improvements, library additions and upgrades, and bug fixes.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0.md) release notes.\n\n## <DBR> 7.0 for Genomics GA\n\n**June 18, 2020**\n\n<DBR> 7.0 for Genomics is built on top of <DBR> 7.0 and includes the following library changes:\n\n- The ADAM library has been updated from version 0.30.0 to 0.32.0.\n- The Hail library is not included in <DBR> 7.0 for Genomics, because there is no release based on <AS> 3.0.\n\nFor more information, see the complete [_](/release-notes/runtime/7.0genomics.md) release notes.\n\n## Stage-dependent access controls for MLflow models\n\n**June 16-23, 2020: Version 3.22**\n\nYou can now assign stage-dependent access controls to users or groups, allowing them to manage [MLflow Models](/mlflow/models.md) registered in the [MLflow Model Registry](/mlflow/model-registry.md) at the Staging or Production stage. We introduced two new permission levels, **Can Manage Staging Versions** and **Can Manage Production Versions**. Users with these permissions can perform transitions between stages allowed for the level.\n\nFor details, see [_](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n\n## Notebooks now support disabling auto-scroll\n\n**June 16-23, 2020: Version 3.22**\n\nWhen you [run a notebook cell](/notebooks/run-notebook.md) using **shift+enter**, the default notebook behavior is to auto-scroll to the next cell if the cell is not visible. You can now disable auto-scroll in <User Settings Icon> **> User Settings > Editor settings**. If you disable auto-scroll, on **shift+enter** the focus moves to the next cell, but the notebook does not scroll to that cell.\n\n.. aws::\n\n  ## Skipping instance profile validation now available in the UI\n\n  **June 16-23, 2020: Version 3.22**\n\n  The [Add Instance Profile](/aws/iam/add-instance-profile-databricks.md) dialog now has a checkbox that allows you to skip validation. If validation fails, you can select this checkbox to skip the validation and forcibly add the instance profile.\n\n  ## Account ID is displayed in account console\n\n  **June 16-23, 2020, Version 3.22**\n\n  Your Databricks account ID is now displayed on the [Usage Overview tab](/archive/admin-guide/usage.md) in the account console.\n\n.. azure::\n\n  ## Metastore IP addresses to change on June 30, 2020\n\n  **June 11, 2020**\n\n  The default metastore for Azure Databricks uses Azure Database for MySQL. All Azure Database for MySQL IP addresses for Azure Databricks metastores are changing on June 30, 2020. If you have an Azure Databricks workspace deployed in your own virtual network, your route table for that deployment may include an Azure Databricks metastore IP address or route to a firewall or proxy appliance with an access list that includes that address. If that is the case, you must update your Azure Databricks route tables or firewalls with new MySQL IPs before June 30, 2020 to avoid disruption.\n\n## Internet Explorer 11 support ends on August 15\n\n**June 9, 2020**\n\nIn keeping with industry trends and to ensure a stable and consistent user experience for our customers, <Databricks> will end support for Internet Explorer 11 on August 15, 2020.\n\n## <DBR> 6.2 series support ends\n\n**June 3, 2020**\n\nSupport for <DBR> 6.2, <DBR> 6.2 for Machine Learning, and <DBR> 6.2 for Genomics ended on June 3. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Simplify and control cluster creation using cluster policies (Public Preview)\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n  .. note:: Databricks is rolling out this public preview over two releases. It may not be deployed to your workspace until the next release. Contact your Databricks representative with any questions.\n\nCluster policies are admin-defined, reusable cluster templates that enforce rules on cluster\nattributes and thus ensure that users create clusters that conform to those rules. As <a Databricks> admin,\nyou can now create cluster policies and give users policy permissions. By doing that, you have more\ncontrol over the resources created, give users the level of flexibility they need to do their work, and\nconsiderably simplify the cluster creation experience.\n\nFor details, see [_](/administration-guide/clusters/policies.md).\n\n## SCIM Me endpoint now returns SCIM compliant response\n\n**June 2-9, 2020: Version 3.21**\n\n.. aws::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups, entitlements, and roles.\n\n.. azure::\n\n  The SCIM Me endpoint now returns the same information as the `/users/{id}` endpoint, including information such as groups and entitlements.\n\n\nSee [_](/dev-tools/api/latest/scim/scim-me.md).\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances now available for machine learning application deployments (Beta)\n\n  **June 2-9, 2020: Version 3.21**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are optimized for deploying machine learning models in production. To use TensorRT on these instance types with the current releases of <DBR> for Machine Learning (as of June 2, 2020), you must manually install `libnvinfer` using an init script. We anticipate that future [GPU-enabled versions](/clusters/gpu.md) of <DBR> ML will contain this package.\n\n  ## Deploy multiple workspaces in your Databricks account (Public Preview)\n\n  **June 1, 2020**\n\n  The new Multi-workspace API (renamed Account API on September 1, 2020) introduces an administrative and management layer (the account layer) on top of Databricks workspaces that gives an account owner a single pane of glass to create, configure, and manage multiple workspaces for your organization. Use the API to create one or more workspaces for each team in your organization that needs to use Databricks, or one workspace for each of your dev, staging, and production environments. Databricks provisions a ready-to-use workspace within minutes. Workspaces are completely isolated from each other. You can choose to deploy a workspace in the same underlying AWS account or in different AWS accounts, depending on your operational plan. The Multi-workspace API (Account API) is available on the `accounts.cloud.databricks.com` endpoint.\n\n  For more information, see [_](/administration-guide/workspace/create-workspace-api.md).\n\n  Contact your Databricks representative to request access to this public preview.\n\n  ## Deploy Databricks workspaces in your own VPC (Public Preview)\n\n  **June 1, 2020**\n\n  By default, clusters are created in a single AWS VPC (Virtual Private Cloud) that Databricks creates and configures in your AWS account. Now you have the option to create your Databricks workspaces in your own VPC, a feature known as _customer-managed VPC_, which can allow you to exercise more control over the infrastructure and help you comply with specific cloud security and governance standards your organization may require. You simply provide your VPC ID, security group ID and subnet IDs when you create your workspace using the [Multi-workspace API (Account API)](/administration-guide/workspace/create-workspace-api.md).\n\n  For more information, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n  This feature is available only on [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), not on the existing enterprise platform. Contact your Databricks representative to request access.\n\n  ## Secure cluster connectivity with no open ports on your VPCs and no public IP addresses on Databricks workers (Public Preview)\n\n  **June 1, 2020**\n\n  With the release of the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), Databricks is providing a new network architecture for connectivity between the Databricks control plane (delivered as SaaS) and the data plane (your own AWS VPC). With this new architecture, you no longer have to open inbound ports on cluster VMs: cluster VMs launched in your customer-managed-VPC now initiate an outbound TLS 1.2 connection to the Databricks control plane. Not only is this architecture compliant with common InfoSec requirements, but it eliminates the need for VPC peering and gives you more flexibility in how you connect your environment to the Databricks control plane.\n\n  For more information, see [_](/security/network/secure-cluster-connectivity.md).\n\n  Contact your Databricks representative to request access to this public preview.\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of cluster policies in Databricks?", "gold_answer": "The purpose of cluster policies in Databricks is to provide admin-defined, reusable cluster templates that enforce rules on cluster attributes. This ensures that users create clusters that conform to the rules set by the administrators. By creating cluster policies and giving users policy permissions, administrators have more control over the resources created, allow users the flexibility they need to do their work, and greatly simplify the cluster creation experience."}
{"hard_neg_docs": [{"page_content": "You can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}, {"page_content": "- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}], "question": "What are the consequences of deleting users from a Databricks account?", "gold_answer": "The consequences of deleting users from a Databricks account include:\n\n- Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API.\n- Jobs owned by the user will fail.\n- Clusters owned by the user will stop.\n- Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing."}
{"hard_neg_docs": [{"page_content": "You can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}, {"page_content": "- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/groups.html\nContent:\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/groups.html", "seq_num": 1451, "description": "", "text": "---\ndescription: Learn how to manage groups in <Databricks>.\n---\n\n# Manage groups\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation and <UC>, which are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article explains how admins create and manage <Databricks> groups. For an overview of the <Databricks> identity model, see [_](index.md#identity-model).\n\n## Overview of group management\n\nGroups simplify identity management by making it easier to assign access to workspaces, data, and other securable objects. All Databricks identities can be assigned as members of groups.\n\n<a id=\"account-vs-workspace-group\"></a>\n\n### Difference between account groups and workspace-local groups\n\nWhile <principals> created at the workspace level are [automatically synchronized to the account](index.md#account-level-identity-sync), groups created at the workspace level are not. Instead, <Databricks> has the concept of _account groups_ and _workspace-local groups_.\n\n- **Account groups** can be granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore and permissions to [identity federated](/administration-guide/users-groups/index.md#assign-users-to-workspaces) workspaces. Account groups can be created only by account admins using the account console and the SCIM (Account) REST API.\n\n- **Workspace-local groups** are legacy groups. These groups are identified as _workspace-local_ in the workspace admin console. Workspace-local groups cannot be assigned to additional workspaces or granted access to data in a [Unity Catalog](/data-governance/unity-catalog/index.md) metastore.\n\n.. note::\n  In identity federated workspaces, workspace-local groups can only be managed by workspace admins using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\n<Databricks> recommends using account groups instead of workspace-local groups. You must enable your workspace for identity federation to use account groups. If you enable identity federation in an existing workspace, you can use both account groups and workspace-local groups side-by-side, but <Databricks> recommends turning workspace-local groups into account groups to take advantage of centralized workspace assignment and data access management using <UC>.  See [_](#migrate).\n\n### Who can manage groups?\n\nTo manage groups in <Databricks>, you must be either an _account admin_ or a _workspace admin_.\n\n- **Account admins** can add groups to the account and manage group members. They can also assign groups to workspaces and configure data access for them across workspaces, as long as those workspaces use [identity federation](/administration-guide/users-groups/index.md#assign-users-to-workspaces).\n\n- **Workspace admins** can add account groups to their identity federated workspace and manage access to objects and functionality in the workspace, such as the ability to create clusters or pools. Workspace admins cannot create or manage account groups. Workspace admins can create and manage workspace-local groups. In identity federated workspaces, workspace-local groups can only be managed using the [_](/dev-tools/api/latest/scim/scim-groups.md) API.\n\nWorkspace admins are members of the `admins` group in the workspace, which is a reserved group that cannot be deleted.\n\n.. azure::\n  .. note::\n    Users with the Contributor or Owner role on the workspace resource in Azure are automatically assigned to the workspace `admins` group. For more information, see [_](/administration-guide/account-settings/account.md).\n\n<a id=\"add-group\"></a>\n\n## Add groups to your account\n\nAs an account admin, you can add groups to your <Databricks> account using the account console, a provisioning connector for your identity provider, or the SCIM (Account) API.\n\n<a id=\"add-group-account-console\"></a>\n\n### Add groups to your account using the account console\n\nTo add a group to the account using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, click **Add group**.\n#. Enter a name for the group.\n#. Click **Confirm**.\n#. When prompted, add <identities> to the group.\n\n### Add <identities> to an existing group using the account console\n\nTo add <identities> to an existing group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, select the group you want to update.\n#. Click **Add members**.\n#. Search for the <identity> you want to add and select it.\n#. Click **Add**.\n\nTo give group members access to a workspace, you need to add the group to the workspace using identity federation. See [_](#add-groups-workspace).\n\n.. aws-gcp::\n\n  ### Sync groups to your <Databricks> account from an identity provider\n\n  You can sync groups from your identity provider (IdP) to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/index.md#provision-account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n.. azure::\n\n  ### Provision groups to your <Databricks> account from your Azure Active Directory (Azure AD) tenant\n\n  You can sync groups from your Azure Active Directory (Azure AD) tenant to your <Databricks> account using a SCIM provisioning connector. For instructions, see [_](/administration-guide/users-groups/scim/aad.md#account).\n\n  .. important:: If you already have SCIM connectors that sync identities directly to your workspaces **and those workspaces are enabled for identity federation**, we recommend that you disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not using identity federation, you must continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Add groups to your account using the SCIM APIs\n\nAccount admins can add and manage groups in the <Databricks> account using the SCIM API for Accounts.\n\nWorkspace admins can't add groups using this API, but they can list and view groups. To do this, they must invoke the API using a different endpoint URL:\n\n- Account admins use <SCIMAccountAPIURI>.\n- Workspace admins use `{workspace-domain}/api/2.0/account/scim/v2/`.\n\nWorkspace admins cannot create groups using the SCIM API for Accounts. For details, see [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"account-admin\"></a>\n\n## Assign the account admin role to a group\n\nYou cannot assign the account admin role to a group using the account console, but you can assign it to groups using the SCIM API for Accounts. See [_](/dev-tools/api/latest/scim/account-scim.md).\n\n## Remove groups from your <Databricks> account\n\nAccount admins can remove groups from <a Databricks> account. Workspace admins cannot.\n\n.. important::  When you remove a group, all users in that group are deleted from the account and lose access to any workspaces they had access to, _unless they are members of another group or have been directly granted access to the account or any workspaces_. We recommend that you refrain from deleting account-level groups unless you want them to lose access to all workspaces in the account. Be aware of the following consequences of deleting users:\n\n  - Applications or scripts that use the tokens generated by the user will no longer be able to access the Databricks API\n  - Jobs owned by the user will fail\n  - Clusters owned by the user will stop\n  - Queries or dashboards created by the user and shared using the Run as Owner credential will have to be assigned to a new owner to prevent sharing from failing\n\nTo remove a group using the account console, do the following:\n\n#. As an account admin, log in to the <accountConsoleURL>.\n#. Click <User management icon> **User management**.\n#. On the **Groups** tab, find the group you want to remove.\n#. Click the <Kebab menu> kebab menu at the far right of the user row and select **Delete**.\n#. In the confirmation dialog box, click **Confirm delete**.\n\nIf you remove a group using the account console, you must ensure that you also remove the group using any SCIM provisioning connectors or SCIM API applications that have been set up for the account. If you don't, SCIM provisioning will simply add the group and its members back the next time it syncs. See [_](/administration-guide/users-groups/scim/index.md).\n\nTo remove a group from <a Databricks> account using SCIM APIs, see [_](/administration-guide/users-groups/scim/index.md#provision-account) and [_](/dev-tools/api/latest/scim/account-scim.md).\n\n<a id=\"migrate\"></a>\n\n## Migrate workspace-local groups to account groups\n\nThis section applies only to workspaces that are enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation).\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation and <UC> are in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nGroups created at the workspace level (workspace-local groups) are not automatically synchronized to the account as account groups. You can use workspace-local groups in the workspace they are defined in, but you cannot manage them using account-level interfaces, and you cannot use them to manage data access across workspaces using <UC>. Therefore <Databricks> recommends that you convert them to account groups.\n\nYou can use any of the following methods to migrate workspace-local groups to the account level:\n\n- **Convert them manually.** Create a new account group using the account console and add each member to the new account. Then use the workspace admin console to delete the workspace-local group.\n\n  See [_](#add-group-account-console).\n\n- **Convert them using a SCIM provisioning connector.** Set up or modify a SCIM provisioning connector to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API. If you have an active SCIM provisioning connector for the workspace, you should shut it down. You should be provisioning all users and groups at the account level.\n\n  See [_](/administration-guide/users-groups/scim/index.md).\n\n- **Convert them using the SCIM APIs.** Use the SCIM (Account) API to add a group to the account that replicates the workspace-local group. Then delete the group using the workspace admin console or workspace-level SCIM (Groups) API.\n\n  See [_](/dev-tools/api/latest/scim/account-scim.md) and [_](/dev-tools/api/latest/scim/scim-groups.md).\n\nAfter you migrate the workspace-local group to the account, you need to grant the new account group access to the workspace and the objects, and the functionality that the workspace-local group originally had access to so that the group members maintains that access. Follow [_](#add-groups-workspace) to assign workspace permissions to the new account groups, and use [_](/dev-tools/api/latest/permissions.md) to grant the group access to objects within the workspace.\n\n<a id=\"add-groups-workspace\"></a>\n\n## Add groups to workspaces\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Dat", "timestamp": 0, "next": ""}}], "question": "Can workspace admins remove groups from a Databricks account?", "gold_answer": "No, workspace admins cannot remove groups from a Databricks account. This action is reserved for account admins."}
{"hard_neg_docs": [{"page_content": "If the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\n---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}], "question": "Can I share the URL of a Shiny app with other users in Databricks?", "gold_answer": "Yes, you can share the URL of a Shiny app with other users in Databricks. Any Databricks user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running. However, if the cluster that the app is running on terminates, the app is no longer accessible. To prevent the cluster from terminating automatically, you can [disable automatic termination](/clusters/configure.md) in the cluster settings."}
{"hard_neg_docs": [{"page_content": "If the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\n---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.", "fmt_content": "Source: https://docs.databricks.com/sparkr/shiny.html\nContent:\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.", "metadata": {"source": "https://docs.databricks.com/sparkr/shiny.html", "seq_num": 347, "description": "", "text": "---\ndescription: Learn how to develop interactive applications on <Databricks> using the Shiny package.\n---\n\n# Shiny on <Databricks>\n\n.. azure-aws::\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can use Shiny inside [RStudio Server](rstudio.md) hosted on <Databricks> clusters. You can also develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\n.. gcp:\n  [Shiny](https://shiny.rstudio.com/) is an R package, available on CRAN, used to build interactive R applications and dashboards. You can develop, host, and share Shiny applications directly from <a Databricks> notebook.\n\nTo get started with Shiny, see the [Shiny tutorials](https://shiny.rstudio.com/tutorial/). You can run these tutorials on <Databricks> notebooks.\n\nThis article describes how to run Shiny applications on <Databricks> and use <AS> inside Shiny applications.\n\n## Shiny inside R notebooks\n\n### Requirements\n\n* <DBR> 8.3 and above.\n\n### Get started with Shiny inside R notebooks\n\nThe Shiny package is included with <DBR>. You can interactively develop and test Shiny applications inside <Databricks> R notebooks similarly to hosted RStudio.\n\nFollow these steps to get started:\n\n#. Create an R notebook.\n#. Import the Shiny package and run the example app `01_hello` as follows:\n\n  ```r\n    library(shiny)\n    runExample(\"01_hello\")\n  ```\n\n#. When the app is ready, the output includes the Shiny app URL as a clickable link which opens a new tab. To share this app with other users, see [_](#share-shiny-app-url).\n\n  ![Example Shiny app](/_static/images/shiny/shiny-01-notebook.png)\n\n.. note::\n\n  - Log messages appear in the command result, similar to the default log message (`Listening on http://0.0.0.0:5150`) shown in the example.\n  - To stop the Shiny application, click **Cancel**.\n  - The Shiny application uses the notebook R process. If you detach the notebook from the cluster, or if you cancel the cell running the application, the Shiny application terminates. You cannot run other cells while the Shiny application is running.\n\n### Run Shiny apps from Databricks Repos\n\nYou can run Shiny apps that are checked into [Databricks Repos](/repos/index.md).\n\n1. [Clone a remote Git repository](/repos/git-operations-with-repos.md).\n#. Run the application.\n\n  ```r\n  library(shiny)\n  runApp(\"006-tabsets\")\n  ```\n\n### Run Shiny apps from files\n\nIf your Shiny application code is part of a project managed by version control, you can run it inside the notebook.\n\n.. note:: You must use the absolute path or set the working directory with `setwd()`.\n\n1. Check out the code from a repository using code similar to:\n\n  ```\n    %sh git clone https://github.com/rstudio/shiny-examples.git\n    cloning into 'shiny-examples'...\n  ```\n\n#. To run the application, enter code similar to the following in another cell:\n\n  ```r\n  library(shiny)\n  runApp(\"/databricks/driver/shiny-examples/007-widgets/\")\n  ```\n\n### Share Shiny app URL\n\nThe Shiny app URL generated when you start an app is shareable with other users. Any <Databricks> user with **Can Attach To** permission on the cluster can view and interact with the app as long as both the app and the cluster are running.\n\nIf the cluster that the app is running on terminates, the app is no longer accessible. You can [disable automatic termination](/clusters/configure.md) in the cluster settings.\n\nIf you attach and run the notebook hosting the Shiny app on a different cluster, the Shiny URL changes. Also, if you restart the app on the same cluster, Shiny might pick a different random port. To ensure a stable URL, you can set the `shiny.port` option, or, when restarting the app on the same cluster, you can specify the `port` argument.\n\n.. azure-aws::\n\n  ## Shiny on hosted RStudio Server\n\n  ### Requirements\n\n  - [_](rstudio.md).\n\n  .. important::\n    With RStudio Server Pro, you must disable proxied authentication.\n    Make sure `auth-proxy=1` is not present inside `/etc/rstudio/rserver.conf`.\n\n  ### Get started with Shiny on hosted RStudio Server\n\n  #. Open RStudio on <Databricks>.\n\n  #. In RStudio, import the Shiny package and run the example app `01_hello` as follows:\n\n    ```r\n    > library(shiny)\n    > runExample(\"01_hello\")\n\n    Listening on http://127.0.0.1:3203\n    ```\n\n    A new window appears, displaying the Shiny application.\n\n    ![First Shiny app](/_static/images/shiny/shiny-01-hello.png)\n\n  ### Run a Shiny app from an R script\n\n  To run a Shiny app from an R script, open the R script in the RStudio editor and click the **Run App** button on the top right.\n\n  ![Shiny run App](/_static/images/shiny/shiny-run-app.png)\n\n<a id=\"use-spark-inside-shiny-app\"></a>\n\n## Use <AS> inside Shiny apps\n\nYou can use <AS> inside Shiny applications with either SparkR or sparklyr.\n\n### Use SparkR with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(SparkR)\nsparkR.session()\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({ nrow(createDataFrame(iris)) })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n### Use sparklyr with Shiny in a notebook\n\n```r\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\n\nui <- fluidPage(\n  mainPanel(\n    textOutput(\"value\")\n  )\n)\n\nserver <- function(input, output) {\n  output$value <- renderText({\n    df <- sdf_len(sc, 5, repartition = 1) %>%\n      spark_apply(function(e) sum(e)) %>%\n      collect()\n    df$result\n  })\n}\n\nshinyApp(ui = ui, server = server)\n```\n\n```r\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(shiny)\nlibrary(sparklyr)\n\nsc <- spark_connect(method = \"databricks\")\ndiamonds_tbl <- spark_read_csv(sc, path = \"/databricks-datasets/Rdatasets/data-001/csv/ggplot2/diamonds.csv\")\n\n# Define the UI\nui <- fluidPage(\n  sliderInput(\"carat\", \"Select Carat Range:\",\n              min = 0, max = 5, value = c(0, 5), step = 0.01),\n  plotOutput('plot')\n)\n\n# Define the server code\nserver <- function(input, output) {\n  output$plot <- renderPlot({\n    # Select diamonds in carat range\n    df <- diamonds_tbl %>%\n      dplyr::select(\"carat\", \"price\") %>%\n      dplyr::filter(carat >= !!input$carat[[1]], carat <= !!input$carat[[2]])\n\n    # Scatter plot with smoothed means\n    ggplot(df, aes(carat, price)) +\n      geom_point(alpha = 1/2) +\n      geom_smooth() +\n      scale_size_area(max_size = 2) +\n      ggtitle(\"Price vs. Carat\")\n  })\n}\n\n# Return a Shiny app object\nshinyApp(ui = ui, server = server)\n```\n\n![Spark Shiny app](/_static/images/shiny/shiny-spark.png)\n\n## Frequently asked questions (FAQ)\n\n.. contents::\n  :local:\n  :depth: 1\n\n### Why is my Shiny app grayed out after some time?\n\n.. aws::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 10 minutes.\n\n.. azure::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 4 minutes.\n\n.. gcp::\n\n  If there is no interaction with the Shiny app, the connection to the app closes after about 5 minutes.\n\nTo reconnect, refresh the Shiny app page. The dashboard state resets.\n\n### Why does my Shiny viewer window disappear after a while?\n\nIf the Shiny viewer window disappears after idling for several minutes, it is due to the same timeout as the “gray out” scenario.\n\n### Why do long Spark jobs never return?\n\nThis is also because of the idle timeout. Any Spark job running for longer than the previously mentioned timeouts is not able to render its result because the connection closes before the job returns.\n\n### How can I avoid the timeout?\n\n- There is a workaround suggested in [Feature request: Have client send keep alive message to prevent TCP timeout on some load balancers](https://github.com/rstudio/shiny/issues/2110#issuecomment-419971302) on Github. The workaround sends heartbeats to keep the WebSocket connection alive when the app is idle. However, if the app is blocked by a long running computation, this workaround does not work.\n- Shiny does not support long running tasks. A Shiny blog post recommends using [promises and futures](https://blog.rstudio.com/2018/06/26/shiny-1-1-0/) to run long tasks asynchronously and keep the app unblocked. Here is an example that uses heartbeats to keep the Shiny app alive, and runs a long running Spark job in a `future` construct.\n\n  ```r\n  # Write an app that uses spark to access data on Databricks\n  # First, install the following packages:\n  install.packages(‘future’)\n  install.packages(‘promises’)\n\n  library(shiny)\n  library(promises)\n  library(future)\n  plan(multisession)\n\n  HEARTBEAT_INTERVAL_MILLIS = 1000  # 1 second\n\n  # Define the long Spark job here\n  run_spark <- function(x) {\n    # Environment setting\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n\n    irisDF <- createDataFrame(iris)\n    collect(irisDF)\n    Sys.sleep(3)\n    x + 1\n  }\n\n  run_spark_sparklyr <- function(x) {\n    # Environment setting\n    library(sparklyr)\n    library(dplyr)\n    library(\"SparkR\", lib.loc = \"/databricks/spark/R/lib\")\n    sparkR.session()\n    sc <- spark_connect(method = \"databricks\")\n\n    iris_tbl <- copy_to(sc, iris, overwrite = TRUE)\n    collect(iris_tbl)\n    x + 1\n  }\n\n  ui <- fluidPage(\n    sidebarLayout(\n      # Display heartbeat\n      sidebarPanel(textOutput(\"keep_alive\")),\n\n      # Display the Input and Output of the Spark job\n      mainPanel(\n        numericInput('num', label = 'Input', value = 1),\n        actionButton('submit',", "timestamp": 0, "next": ""}}], "question": "How can I run a Shiny application from a file in Databricks?", "gold_answer": "To run a Shiny application from a file in Databricks, you should follow these steps:\n\n1. Clone the Git repository containing your Shiny application code to your Databricks workspace using a command similar to:\n   ```sh\n   %sh git clone https://github.com/your-repository/shiny-app.git\n   ```\n\n2. Use the `library(shiny)` function to load the Shiny library in R.\n\n3. If necessary, set the working directory to the location of your Shiny application using `setwd()` to specify the absolute path.\n\n4. Run the Shiny application by using the `runApp()` function with the path to the directory containing your Shiny app code. For example:\n   ```r\n   runApp(\"/databricks/driver/your-shiny-app-directory/\")\n   ```\n\nRemember that for other users to access and interact with the Shiny app, they must have **Can Attach To** permission on the cluster, and both the app and the cluster must be running. If the cluster terminates, the Shiny app becomes inaccessible, but you can disable automatic termination in the cluster settings to prevent this."}
{"hard_neg_docs": [{"page_content": "<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}, {"page_content": "To navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}], "question": "What are the default settings for creating a SQL warehouse using the web UI?", "gold_answer": "The default settings for creating a SQL warehouse using the web UI in Databricks are:\n\n- **Cluster Size**: The default size is set to **X-Large**.\n- **Auto Stop**: \n  - For Pro and classic SQL warehouses, the default auto stop time is set to 45 minutes.\n  - For Serverless SQL warehouses, the default auto stop time is set to 10 minutes.\n- **Scaling**: The default scaling is set to a minimum of one and a maximum of one cluster."}
{"hard_neg_docs": [{"page_content": "<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}, {"page_content": "To navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.", "fmt_content": "Source: https://docs.databricks.com/sql/admin/sql-endpoints.html\nContent:\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.", "metadata": {"source": "https://docs.databricks.com/sql/admin/sql-endpoints.html", "seq_num": 563, "description": "", "text": "---\ndescription: Learn about SQL warehouse requirements, how to configure and manage SQL warehouses using the <DBSQL> UI, and advance configuration options.\n---\n\n# Configure SQL warehouses\n\nThis article explains how to configure and manage SQL warehouses (formerly SQL endpoints) using the <DBSQL> UI.\n\n.. gcp::\n\n  ## What is a SQL Warehouse?\n\n  A SQL warehouse is a compute resource that lets you run [SQL commands](../language-manual/index.md) on data objects within <DBSQL>. Compute resources are infrastructure resources that provide processing capabilities in the cloud.\n\n  ## SQL endpoints name changed to SQL warehouses\n\n  Databricks changed the name from SQL endpoint to SQL warehouse because it is more than just an API entry point for running SQL commands. A SQL warehouse is a compute resource for all your data warehousing needs, an integral part of the Lakehouse Platform.\n\n  Other compute resource types include <Databricks> [clusters](/workspace/workspace-assets.md#ws-clusters). To work with SQL warehouses using the API, see [_](/sql/api/sql-endpoints.md).\n\n## Requirements\n\n.. aws::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, <a Databricks> account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - For serverless SQL warehouses, you must not exceed your account's serverless quota for each region. Serverless quotas are a safety measure for serverless compute. See [_](/serverless-compute/serverless-quotas.md). \n\n.. azure::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n  - Before you can create a serverless SQL warehouse, an account admin must first [enable serverless SQL warehouses for the workspace](/sql/admin/serverless.md).\n\n  - You must have adequate vCPU quota in your Azure account. The default vCPU quota is usually adequate to create the starter warehouse that is created automatically on new workspaces. However, your Azure account may not have adequate vCPU quota to scale the starter warehouse or to create additional warehouses. See [_](#required-azure-vcpu-quota). You can [request additional vCPU quota](https://learn.microsoft.com/azure/azure-portal/supportability/per-vm-quota-requests). Your Azure account may have limitations on how much vCPU quota you can request. Contact your Azure account representative for more information.\n\n.. gcp::\n\n  - To create and manage a SQL warehouse you must be a workspace admin and have the <DBSQL> entitlement.\n\n  - To manage a SQL warehouse if you are not a workspace admin, you must have **[Can Manage](/security/auth-authz/access-control/sql-endpoint-acl.md)** permission in <DBSQL>.\n\n## View SQL warehouses\n\nTo navigate to the SQL warehouse dashboard, click <Warehouses Icon> **SQL Warehouses** in the sidebar.\n\nBy default, warehouses are sorted by state (running warehouses first), then in alphabetical order.\n\nTo help you get started quickly, <Databricks> creates a SQL warehouse called **Starter Warehouse** automatically. This SQL warehouse is sized [Small](#cluster-size). You can [edit](#edit-warehouse) or delete this SQL warehouse.\n\n.. azure-aws::\n\n  .. note:: If serverless is enabled, a starter serverless SQL warehouse is created automatically.\n\n<a id=\"create-endpoint\"></a>\n<a id=\"create-warehouse\"></a>\n\n## Create a SQL warehouse\n\nYou can create a SQL warehouse using the **New SQL Warehouse** page in the web UI or using the [SQL Warehouse API](/sql/api/sql-endpoints.md#create-warehouse).\n\n.. azure-aws::\n\n   For more information about managing serverless SQL warehouses with the API, see [_](/sql/api/sql-endpoints.md#serverless).\n\nBy accepting all the defaults on the **New SQL Warehouse** page, you can create an efficient and high-performing SQL warehouse quickly and easily. If your workload or environment requires different capabilities, you can override those defaults.\n\nTo create a SQL warehouse using the web UI:\n\n.. azure-aws::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n       - **Pro and classic SQL warehouses**: The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes.\n\n       - **Serverless SQL warehouses**: The default is 10 minutes, which is recommended for typical use. The minimum is 5 minutes when you use the UI. Note that you can create a serverless SQL warehouse using the [SQL warehouses API](/sql/api/sql-endpoints.md), in which case you can set the Auto Stop value as low as 1 minute.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n    - **Type** determines the types of warehouse. See [_](/sql/index.md#warehouse-types).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\n.. gcp::\n\n  #. Click ![Endpoints Icon-1](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n\n  #. Click **Create SQL Warehouse**.\n\n  #. Enter a name for the warehouse.\n\n  #. Accept the default warehouse settings or edit them.\n\n     - **Cluster Size** represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**. To reduce query latency, increase the size. For details, see [_](#cluster-size).\n\n     - **Auto Stop** determines whether the warehouse stops if it's idle for the specified number of minutes. The default is 45 minutes, which is recommended for typical use. The minimum is 10 minutes. Idle SQL warehouses continue to accumulate DBU and cloud instance charges until they are stopped.\n\n     - **Scaling** sets the minimum and maximum number of clusters over which queries sent to the warehouse are distributed.\n\n       The default is a minimum of one and maximum of one cluster.\n\n       To handle more concurrent users for a given query, increase the cluster count. Databricks recommends a cluster for every ten concurrent queries. To learn how <Databricks> adds clusters to and removes clusters from a warehouse, see [_](#queueing-and-autoscaling).\n\n  #. (Optional) Configure advanced options. For details, see [_](#advanced).\n\n  #. Click **Create**.\n\n  #. You can then configure [warehouse permissions](/security/auth-authz/access-control/sql-endpoint-acl.md) if you'd like.\n\nYour SQL warehouse is now created and started. You can also create a SQL warehouse with the [_](/dev-tools/terraform/index.md) and [databricks_sql_endpoint](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/sql_endpoint).\n\n<a id=\"advanced\"></a>\n\n## Advanced options\n\nYou can configure the following advanced options by expanding the **Advanced options** area when you create a new SQL warehouse or edit an existing SQL warehouse. You can also configure these options using the [_](/sql/api/sql-endpoints.md).\n\n.. azure-aws::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Unity Catalog**: If <UC> is enabled for the workspace, it is the default for all new warehouses in the workspace. If <UC> is not enabled for your workspace, you do not see this option. For more information about <UC>, see [Unity Catalog](/data-governance/unity-catalog/index.md).\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. gcp::\n\n  - **Tags**: Tags allow you to easily monitor the cost of cloud resources used by users and groups in your organization. You specify tags as key-value pairs.\n  - **Spot instance policy**: The spot instance policy determines whether workers use only on-demand instances or a combination of on-demand and spot instances. **Cost Optimized** (the default) uses mostly spot instances and one on-demand instance. **Reliability Optimized** uses only on-demand instances.\n  - **Channel**: Use the Preview channel to test upcoming features. The preview version lets you try out functionality before it becomes the <DBSQL> standard. You can use it to test your queries and dashboards against upcoming changes.\n\n   Use the [release notes](/sql/release-notes/index.md#channels) to learn what's in the latest preview version.\n\n.. important:: Databricks recommends against using a preview version for production workloads. Because only admins can view a warehouse's properties, including its channel, consider indicating that an SQL warehouse uses a preview version in the warehouse's name so that users do not inadvertently use it for production workloads.\n\n<a id=\"start\"></a>\n\n## Start, stop, or delete a SQL warehouse\n\n#. Click ![Warehouse Icon-2](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. To stop a running warehouse, click **Stop**.\n#. To start a stopped warehouse, click **Start**.\n#. To delete an warehouse, click the vertical ellipsis <Vertical Ellipsis> in the Actions column, then click **Delete**.\n\n<a id=\"edit-endpoint\"></a>\n<a id=\"edit-warehouse\"></a>\n\n## Edit a SQL warehouse\n\nYou can choose to edit a SQL warehouse using the web UI or the [SQL Warehouse API](/sql/api/sql-endpoints.md#edit-warehouse).\n\nTo edit a SQL warehouse using the web UI:\n\n#. Click ![Endpoints Icon-3](/_static/images/icons/warehouse-icon.png) **SQL Warehouses** in the sidebar.\n#. In the Actions column, click the vertical ellipsis <Vertical Ellipsis> and click **Edit**.\n#. Edit the warehouse properties.\n#. Click **Save**.\n\n.. azure-aws::\n\n  <a id=\"convert-to-serverless\"> </a>\n\n  ## Upgrade a pro or classic SQL warehouse to a <serverlessSQLWarehouse>\n\n  To learn about <serverlessSQLWarehouse>, see [_](/serverless-compute/index.md). The serverless SQL warehouse option is available only if it has been [enabled for the workspace](/sql/admin/serverless.md).\n\n  .. include:: /shared/serverless-sql-preview.md", "timestamp": 0, "next": ""}}], "question": "How can I change the cluster size of a SQL warehouse in Databricks?", "gold_answer": "To change the cluster size of a SQL warehouse in Databricks, you can follow these steps using the web UI:\n\n1. Click on the **SQL Warehouses** icon in the sidebar.\n2. Select and open the SQL warehouse you want to edit.\n3. Once you are in the SQL warehouse settings, look for the **Cluster Size** option.\n4. You can then accept the default warehouse settings or edit them to choose a different cluster size, which represents the number of cluster workers and size of compute resources available to run your queries and dashboards. The default is **X-Large**.\n5. After selecting the desired cluster size, save your changes.\n\nIf you need to access more detailed instructions or configurations, you should refer to the Databricks documentation or the specific settings within the SQL warehouse configuration in the Databricks UI."}
{"hard_neg_docs": [{"page_content": "## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}, {"page_content": ".. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}], "question": "What platforms does Terraform support for automating Unity Catalog setup?", "gold_answer": "Terraform supports automating Unity Catalog (UC) setup on the following platforms:\n- Google Cloud Platform (GCP)\n- Amazon Web Services (AWS)\n- Microsoft Azure\n\nThese platforms are indicated by the use of \".. gcp::\", \".. aws::\", and \".. azure::\" directives in the provided document."}
{"hard_neg_docs": [{"page_content": "## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}, {"page_content": ".. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/automate.html\nContent:\n---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/automate.html", "seq_num": 1533, "description": "", "text": "---\ndescription: Learn how to automate <UC> setup using Terraform.\n---\n\n.. gcp::\n  ---\n  orphan: 1\n  ---\n\n# Automate <UC> setup using Terraform\n\nYou can automate <UC> setup by using the [_](/dev-tools/terraform/index.md). This article shows one approach to deploying an end-to-end <UC> implementation. If you already have some <UC> infrastructure components in place, you can also use this article to deploy additional <UC> infrastructure components as needed.\n\n<!-- GCP on UC: add instructions -->\n\n.. aws::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog) in the Databricks Terraform provider documentation.\n\n.. azure::\n\n   For more information, see [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure) in the Databricks Terraform provider documentation.\n\n## Requirements\n\nTo automate <UC> setup using Terraform, you must have the following:\n\n.. aws::\n\n   - Your <Databricks> account must be on the [Premium plan or above](https://databricks.com/product/aws-pricing).\n   - In AWS, you must have the ability to create Amazon S3 buckets, AWS IAM roles, AWS IAM policies, and cross-account trust relationships.\n   - You must have at least one <Databricks> workspace that you want to use with <UC>. See [_](/administration-guide/workspace/create-workspace.md).\n\n.. azure::\n\n   - Your <Databricks> account must be on the [Premium plan](https://azure.microsoft.com/pricing/details/databricks/).\n   - In your Azure tenant, you must have permission to create:\n\n     - A storage account to use with <ADLS2>. See [Create a storage account to use with Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/create-data-lake-storage-account).\n     - A new resource to hold a system-assigned managed identity. This requires that you be a `Contributor` or `Owner` of a resource group in any subscription in the tenant.\n\nTo use the Databricks Terraform provider to configure a metastore for <UC>, storage for the metastore, any external storage, and all of their related access credentials, you must have the following:\n\n.. include:: /shared/terraform/requirements-account.md\n\nTo use the Databricks Terraform provider to configure all other <UC> infrastructure components, you must have the following:\n\n.. include:: /shared/terraform/requirements-workspace.md\n\n## Configure Terraform authentication\n\n.. aws::\n\n   This section shows how to configure Terraform authentication to deploy end-to-end <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog#provider-initialization).\n\n.. azure::\n\n   This section shows how to configure Terraform authentication to deploy <UC> infrastructure. See also [Provider initialization](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog-azure#provider-initialization).\n\n   To use this configuration, you must have the ID of the Azure resource for the <Databricks> workspace to enable with <UC>. This ID should look like this. A line break is added here to accommodate screen width. Your ID must not have any line breaks.\n\n   ```text\n   /subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/\n   <resource-group-name>/providers/Microsoft.Databricks/workspaces/<workspace-name>\n   ```\n\n   To find this ID, navigate to your <Databricks> workspace in the Azure portal, and then select the `JSON View` link on the `Overview` page.\n\nTo configure Terraform authentication to deploy end-to-end <UC> infrastructure, create a file named `auth.tf`.\n\n.. aws::\n\n   The code that you run depends on your authentication method.\n\n   To use a Databricks CLI connection profile for workspace authentication, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.tf\n\n   To use environment variables for workspace authentication instead, use the following code:\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-env-vars.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.tf\n\n   .. note:: If you are using an Azure AD service principal, the syntax of the `azurerm` and `databricks` providers in `auth.tf` are different. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n.. aws::\n\n   To use a Databricks CLI connection profile for workspace authentication, also create a file named `auth.auto.tfvars` with the following configuration code, and replace the Databricks CLI connection profile name as needed. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-aws-cli-profile.auto.tfvars\n\n.. azure::\n\n   Also create a file named `auth.auto.tfvars` with the following configuration code, replacing the sample Azure resource ID with the resource ID of the <Databricks> workspace you want to enable with <UC>. This enables you to reuse `auth.tf` in other projects without changing this value in the `auth.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/auth-unity-catalog-azure.auto.tfvars\n\n   .. note:: If you are using an Azure AD service principal, you must also include variables with their values for the `azurerm` and `databricks` providers in `auth.tf` to use. See [Authenticating with Azure Service Principal](https://registry.terraform.io/providers/databricks/databricks/latest/docs#authenticating-with-azure-service-principal).\n\n## Configure storage for a metastore\n\n.. aws::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Amazon S3 bucket along with an IAM role that gives <UC> permissions to access and manage data in the bucket. See also [aws_s3_bucket](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket), [aws_s3_bucket_public_access_block](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_public_access_block), [aws_iam_policy_document](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/data-sources/iam_policy_document), [aws_iam_policy](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_policy), and [aws_iam_role](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role).\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.tf\n\n   Also create a file named `metastore-storage.auto.tfvars` with the following configuration code, and replace the metastore storage label as needed. This enables you to reuse `metastore-storage.tf` in other projects without changing this value in the `metastore-storage.tf` file itself.\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-aws.auto.tfvars\n\n.. azure::\n\n   This section shows how to configure the deployment of root storage for a metastore. This storage consists of an Azure storage account, which is the default storage location for managed tables in <UC>, along with a Databricks Access Connector that provides <UC> permissions to access and manage data in the storage account. See also [azapi_resource](https://registry.terraform.io/providers/Azure/azapi/latest/docs/resources/azapi_resource), [azurerm_storage_account](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_account), [azurerm_storage_container](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/storage_container), and [azurerm_role_assignment](https://registry.terraform.io/providers/hashicorp/azurerm/latest/docs/resources/role_assignment).\n\n   .. note:: Databricks recommends that you use a dedicated Azure storage account for each metastore.\n\n   To configure the metastore storage deployment, create a file named `metastore-storage.tf` with the following configuration code:\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-storage-azure.tf\n\n## Configure a metastore\n\nThis section shows how to configure the deployment of a metastore into an account. See also [databricks_metastore](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore), [databricks_metastore_data_access](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_data_access), and [databricks_metastore_assignment](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/metastore_assignment).\n\nTo configure the metastore deployment, create a file named `metastore.tf` with the following configuration code:\n\n.. aws::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-aws.tf\n\n.. azure::\n\n   .. literalinclude:: /shared/code-examples/terraform/metastore-azure.tf\n\nAlso create a file named `metastore.auto.tfvars` with the following configuration code, and replace the values as needed. This enables you to reuse `metastore.tf` in other projects without changing these values in the `metastore.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/metastore.auto.tfvars\n\n## Configure a catalog\n\nThis section shows how to configure the deployment of a catalog into an existing metastore. See also [databricks_catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/catalog).\n\nTo configure the catalog deployment, create a file named `catalog.tf` with the following configuration code:\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.tf\n\nAlso create a file named `catalog.auto.tfvars` with the following configuration code, and replace the catalog's name as needed. This enables you to reuse `catalog.tf` in other projects without changing this value in the `catalog.tf` file itself.\n\n.. literalinclude:: /shared/code-examples/terraform/catalog.auto.tfvars\n\nIf you have an existing metastore that you want to use, replace `databricks_metastore.metastore.id` with the existing metastore's programmatic ID. To get this ID, you can call the `GET /api/2.1/unity-catalog/metastores` operation in the [", "timestamp": 0, "next": ""}}], "question": "Where can I find more information on deploying pre-requisite resources and enabling Unity Catalog using Terraform for AWS?", "gold_answer": "You can find more information on deploying pre-requisite resources and enabling Unity Catalog using Terraform for AWS in the Databricks Terraform provider documentation at [Deploying pre-requisite resources and enabling Unity Catalog](https://registry.terraform.io/providers/databricks/databricks/latest/docs/guides/unity-catalog)."}
{"hard_neg_docs": [{"page_content": "## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\n---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "When you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the significance of the G4 family of GPU-accelerated EC2 instances?", "gold_answer": "The significance of the G4 family of GPU-accelerated EC2 instances is that they are optimized for deploying machine learning models in production. As of January 20, 2021, G4 instances are generally available, which means customers can use these instances to efficiently run their machine learning workloads on the cloud, leveraging the GPU acceleration for better performance."}
{"hard_neg_docs": [{"page_content": "## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\n---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "When you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2021/january.html\nContent:\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2021/january.html", "seq_num": 1954, "description": "", "text": "---\ndescription: January 2021 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# January 2021\n\nThese features and <Databricks> platform improvements were released in January 2021.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n    This month saw the release of <Databricks> platform version 3.37. There was no release of versions 3.35 or 3.36.\n\n\n.. azure::\n\n  ## Secure cluster connectivity (no public IPs) is GA\n\n  **January 28, 2021**\n\n  Secure cluster connectivity (no public IPs), now GA, lets you launch clusters in which all nodes have only private IP addresses, providing enhanced security. You can enable secure cluster connectivity for new workspaces. If you have workspaces with public IPs that you would like to migrate, you should create new workspaces enabled for secure cluster connectivity and migrate your resources to the new workspaces. Contact your Microsoft or Databricks account team for details.\n\n  If you use secure cluster connectivity, there are changes for egress from your workspace's subnets:\n\n  - If you use the default (managed) VNet, <Databricks> now creates a NAT gateway, rather than using the default SNAT. The NAT gateway is created within the managed resource group that <Databricks> creates and manages. You cannot modify this resource group nor any resources provisioned in it.\n\n  - If you use the optional [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) feature to provide your own VNet, you have several options for controlling egress. The documentation has been updated to reflect the updated guidance for workspace deployments.\n\n  For details, see [_](/security/network/secure-cluster-connectivity.md).\n\n## <DBR> 7.1 series support ends\n\n**January 21, 2021**\n\nSupport for <DBR> 7.1, <DBR> 7.1 for Machine Learning, and <DBR> 7.1 for Genomics ended on January 21. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n## Start clusters faster with Docker images preloaded into instance pools\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create an instance pool using the API, you can now specify preloaded Docker images. Pool clusters that use a preloaded Docker image start faster\nas they do not have to wait for the image to download. See [Create instance pool](/dev-tools/api/latest/instance-pools.md#clusterinstancepoolservicecreateinstancepool).\n\n## Notebook find and replace now supports changing all occurrences of a match\n\n**January 20-28, 2021: Version 3.37**\n\nYou can now choose to replace all matches when using the find and replace capability in a notebook. For details, see [_](/notebooks/notebooks-code.md#find-and-replace-text).\n\n## Single Node clusters (GA)\n\n**January 20-28, 2021: Version 3.37**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## Free form cluster policy type renamed to Unrestricted\n\n**January 20-28, 2021: Version 3.37**\n\nThe Free form cluster policy type has been renamed to Unrestricted. For details, see [_](/administration-guide/clusters/policies.md) and [_](/clusters/configure.md#cluster-policy).\n\n## Cluster policy field not shown if a user only has access to one policy\n\n**January 20-28, 2021: Version 3.37**\n\nWhen you create a standard or jobs cluster, the **Cluster Policy** field does not appear if you have access to only one policy or if no policies have been defined.\n\n.. aws::\n\n  ## G4 family of GPU-accelerated EC2 instances GA\n\n  **January 20, 2021**\n\n  [G4 instances](https://aws.amazon.com/ec2/instance-types/g4/) are now generally available. G4 instances are optimized for deploying machine learning models in production.\n\n## <DBR> 7.0 series support ends\n\n**January 14, 2021**\n\nSupport for <DBR> 7.0, <DBR> 7.0 for Machine Learning, and <DBR> 7.0 for Genomics ended on January 14. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. aws::\n\n  ## Billable usage and audit log S3 bucket policy and object ACL changes\n\n  **January 14, 2021**\n\n  Databricks now supports automatic usage and audit log delivery to an S3 bucket that is hosted in an AWS account that is different from the account of the IAM role that you specify for storage access. Databricks provides a template from which to create the S3 bucket policy required for cross-account log delivery.\n\n  In addition, the ownership of usage and audit logs delivered to your S3 bucket can now leverage the `BucketOwnerFullControl` Canned ACL instead of an IAM role, giving account owners and designees full ownership.\n\n  See [_](/administration-guide/account-settings/audit-logs.md#delivery) and [_](/administration-guide/account-settings/billable-usage-delivery.md).\n\n  ## E2 platform comes to the Asia Pacific region\n\n  **January 4, 2021**\n\n  We are excited to announce that the [E2 version of the platform](/getting-started/overview.md#e2-architecture) is now available in AWS Asia Pacific regions. You can now create multiple workspaces per account in:\n\n  - ap-southeast-2 (Sydney\n  - ap-south-1 (Mumbai)\n  - ap-northeast-1 (Tokyo)\n\n  This is in addition to the existing region support for E2:\n\n  - us-west-2 (Oregon)\n  - us-west-1 (N. California)\n  - us-east-1 (N. Virginia)\n  - us-east-2 (Ohio)\n  - ca-central-1 (Montreal)\n  - eu-west-2 (Ireland)\n  - eu-central-1 (Frankfurt)\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I configure automatic usage and audit log delivery to an S3 bucket in Databricks?", "gold_answer": "To configure automatic usage and audit log delivery to an S3 bucket in Databricks, you'll need to create an S3 bucket policy that allows for cross-account log delivery. Databricks supports this functionality, and they provide a template from which you can create the required S3 bucket policy. You can set up the S3 bucket in an AWS account that is different from the account associated with the IAM role specified for storage access. Additionally, the ownership of the usage and audit logs delivered to your S3 bucket can leverage the `BucketOwnerFullControl` Canned ACL, which gives account owners and their designees full ownership of the logs."}
{"hard_neg_docs": [{"page_content": "## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\nauth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I securely store credentials for accessing external data sources in Databricks?", "gold_answer": "In Databricks, you can securely store credentials for accessing external data sources by using Databricks secrets. Databricks recommends storing your credentials as secrets instead of directly entering them into a notebook. For more information on how to use secret management in Databricks, you are directed to visit their documentation on secret management at [_](/security/secrets/index.md)."}
{"hard_neg_docs": [{"page_content": "## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\nauth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance", "fmt_content": "Source: https://docs.databricks.com/security/index.html\nContent:\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance", "metadata": {"source": "https://docs.databricks.com/security/index.html", "seq_num": 502, "description": "", "text": "---\ndescription: Learn about how <Databricks> secures your data and privacy and how you can secure your <Databricks> account and data.\n---\n\n# <SecurityArticleTitle>\n\n.. aws-gcp::\n  .. <SecurityArticleTitle> replace:: Security and compliance guide\n.. azure::\n  .. <SecurityArticleTitle> replace:: Security guide\n\nThis guide provides an overview of security features and capabilities that an enterprise data team can use to harden their <Databricks> environment according to their risk profile and governance policy.\n\nThis guide does not cover information about securing your data. For that information, see [_](/data-governance/best-practices.md).\n\n.. aws::\n  .. note:: This article focuses on [the most recent (E2) version](/getting-started/overview.md#e2-architecture) of the Databricks platform. Some of the features described here may not be supported on legacy deployments that have not migrated to the E2 platform.\n\n.. azure-aws::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    keys/index\n    secrets/index\n    privacy/index\n\n.. gcp::\n  .. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    auth-authz/index\n    network/index\n    secrets/index\n    privacy/index\n\n## Authentication and access control\n\nIn <Databricks>, a _workspace_ is <a Databricks> deployment in the cloud that functions as the unified environment that a specified set of users use for accessing all of their <Databricks> [assets](/workspace/workspace-assets.md). Your organization can choose to have multiple workspaces or just one, depending on your needs. <A Databricks> _account_ represents a single entity for purposes of billing, user management, and support. An account can include multiple workspaces and <UC> metastores.\n\nAccount admins handle general account management, and workspace admins manage the settings and features of individual workspaces in the account. Both account and workspace admins manage <Databricks> <identities>, as well as authentication settings and access control.\n\n<Databricks> provides security features to configure strong authentication. Admins can configure these settings to help prevent account takeovers, in which credentials belonging to a user are compromised using methods like phishing or brute force, giving an attacker access to all of the data accessible from the environment.\n\nAccess control lists determine who can view and perform operations on objects in <Databricks> workspaces, such as notebooks and SQL warehouses.\n\nTo learn more about authentication and access control in <Databricks>, see [_](/security/auth-authz/index.md).\n\n## Network access\n\n<Databricks> provides network protections that enable you to secure <Databricks> workspaces and help prevent users from exfiltrating sensitive data. You can use IP access lists to enforce the network location of <Databricks> users. Using a customer-managed VPC, you can lock down outbound network access. To learn more, see [_](/security/network/index.md).\n\n.. azure-aws::\n\n  ## Data security and encryption\n\n  Security-minded customers sometimes voice a concern that Databricks itself might be compromised, which could result in the compromise of their environment. <Databricks> has an extremely strong security program which manages the risk of such an incident. See the [Security and Trust Center](https://databricks.com/trust) for an overview on the program. That said, no company can completely eliminate all risk, and <Databricks> provides encryption features for additional control of your data. See [_](/security/keys/index.md).\n\n## Secret management\n\nSometimes accessing data requires that you authenticate to external data sources. <Databricks> recommends that you  use <Databricks> secrets to store your credentials instead of directly entering your credentials into a notebook. For more infromation, see [_](/security/secrets/index.md).\n\n## Auditing, privacy, and compliance\n\n<Databricks> provides auditing features to enable admins to monitor user activities to detect security anomalies. For example, you can monitior account takeovers by alerting on unusual time of logins or simultaneous remote logins.\n\n.. aws::\n  <Databricks> also provides controls that help meet security requirements for many compliance standards, such as HIPAA and PCI.\n\nFor more information, see [_](/security/privacy/index.md).\n\n## Learn more\n\nHere are some resources to help you build a comprehensive security solution that meets your organization's needs:\n\n.. aws::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-aws.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. azure::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://cms.databricks.com/sites/default/files/2023-01/azure_databricks-security_best_practices_and_threat_model.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. gcp::\n  - The [Databricks Security and Trust Center](https://databricks.com/trust), which provides information about the ways in which security is built into every layer of the Databricks Lakehouse Platform.\n  - [Security Best Practices](https://www.databricks.com/wp-content/uploads/2022/09/security-best-practices-databricks-on-gcp.pdf), which provides a checklist of security practices, considerations, and patterns that you can apply to your deployment, learned from our enterprise engagements.\n  - [Data governance best practices](/data-governance/best-practices.md) to implement data governance controls for your organization.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the purpose of using IP access lists in Databricks workspaces?", "gold_answer": "The purpose of using IP access lists in Databricks workspaces is to secure Databricks workspaces and help prevent users from exfiltrating sensitive data by enforcing the network location of Databricks users. This means that access to the workspace can be restricted based on the originating network locations of user connections, enhancing the security of the data and the workspace."}
{"hard_neg_docs": [{"page_content": "\"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n\"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "\"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n\"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What does the \"REFERENTIAL_CONSTRAINTS\" table in the information schema describe?", "gold_answer": "The \"REFERENTIAL_CONSTRAINTS\" table in the information schema describes referential (foreign key) constraints defined in the catalog."}
{"hard_neg_docs": [{"page_content": "\"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n\"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "\"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html\nContent:\n\"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-information-schema.html", "seq_num": 740, "description": "", "text": "---\ndescription: Learn about the information schema in <DBSQL> and <DBR>. The purpose of the information schema is to provide a SQL-based, self-describing API to the metadata.\n---\n\n# Information schema\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 10.2 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nThe `INFORMATION_SCHEMA` is a SQL standard based schema, provided in every catalog created on <UC>.\n\nWithin the information schema, you can find a set of views describing the objects known to the schema's catalog that you are privileged the see.\nThe information schema of the `SYSTEM` catalog returns information about objects across all catalogs within the metastore. Information schema system tables do not contain metadata about `hive_metastore` objects.\n\nThe purpose of the information schema is to provide a SQL based, self describing API to the metadata.\n\n## Entity relationship diagram of the information schema\n\nThe following entity relationship (ER) diagram provides an overview of the information schema views and how they relate to each other.\n\n<!-- Original Lucid Chart: https://lucid.app/lucidchart/47fad937-b1a2-4264-834f-5897145ede8f/edit  -->\n\n![ER diagram of information schema](/_static/images/sql/information-schema.png)\n\n## Information schema views\n\n.. csv-table::\n   :header: Name, Description\n\n   \"[CATALOG_PRIVILEGES](information-schema/catalog_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the catalogs.\"\n   \"[CATALOG_PROVIDER_SHARE_USAGE](information-schema/catalog_provider_share_usage.md)\", \"Describes provider share mounted onto catalogs.\"\n   \"[CATALOGS](information-schema/catalogs.md)\", \"Describes catalogs.\"\n   \"[CHECK_CONSTRAINTS](information-schema/check_constraints.md)\", \"Reserved for future use.\"\n   \"[COLUMNS](information-schema/columns.md)\", \"Describes columns of tables and views in the catalog.\"\n   \"[CONSTRAINT_COLUMN_USAGE](information-schema/constraint_column_usage.md)\", \"Describes the constraints referencing columns in the catalog.\"\n   \"[CONSTRAINT_TABLE_USAGE](information-schema/constraint_table_usage.md)\", \"Describes the constraints referencing tables in the catalog.\"\n   \"[EXTERNAL_LOCATION_PRIVILEGES](information-schema/external_location_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the external locations.\"\n   \"[EXTERNAL_LOCATIONS](information-schema/external_locations.md)\", \"Describes external locations.\"\n   \"[INFORMATION_SCHEMA_CATALOG_NAME](information-schema/information_schema_catalog_name.md)\", \"Returns the name of this information schema's catalog.\"\n   \"[KEY_COLUMN_USAGE](information-schema/key_column_usage.md)\", \"Lists the columns of the primary or foreign key constraints within the catalog.\"\n   \"[METASTORE_PRIVILEGES](information-schema/metastore_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the current metastore.\"\n   \"[METASTORES](information-schema/metastores.md)\", \"Describes the current metastore.\"\n   \"[PARAMETERS](information-schema/parameters.md)\", \"Describes parameters of routines (functions) in the catalog.\"\n   \"[PROVIDERS](information-schema/providers.md)\", \"Describes providers.\"\n   \"[RECIPIENT_ALLOWED_IP_RANGES](information-schema/recipient_allowed_ip_ranges.md)\", \"Lists allowed IP ranges for recipients.\"\n   \"[RECIPIENT_TOKENS](information-schema/recipient_tokens.md)\", \"Lists tokens for recipients.\"\n   \"[RECIPIENTS](information-schema/recipients.md)\", \"Describes recipients.\"\n   \"[REFERENTIAL_CONSTRAINTS](information-schema/referential_constraints.md)\", \"Describes referential (foreign key) constraints defined in the catalog.\"\n   \"[ROUTINE_COLUMNS](information-schema/routine_columns.md)\", \"Describes result columns of table valued functions.\"\n   \"[ROUTINE_PRIVILEGES](information-schema/routine_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the routines in the catalog.\"\n   \"[ROUTINES](information-schema/routines.md)\", \"Describes routines (functions) in the catalog.\"\n   \"[SCHEMA_PRIVILEGES](information-schema/schema_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the schemas in the catalog.\"\n   \"[SCHEMATA](information-schema/schemata.md)\", \"Describes schemas within the catalog.\"\n   \"[SHARE_RECIPIENT_PRIVILEGES](information-schema/share_recipient_privileges.md)\", \"Describes the recipients granted access to shares.\"\n   \"[SHARES](information-schema/shares.md)\", \"Describes shares.\"\n   \"[STORAGE_CREDENTIAL_PRIVILEGES](information-schema/storage_credential_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the storage credentials.\"\n   \"[STORAGE_CREDENTIALS](information-schema/storage_credentials.md)\", \"Describes storage credentials.\"\n   \"[TABLE_CONSTRAINTS](information-schema/table_constraints.md)\", \"Describes metadata for all primary and foreign key constraints within the catalog.\"\n   \"[TABLE_PRIVILEGES](information-schema/table_privileges.md)\", \"Lists [principals](sql-ref-principal.md) that have [privileges](sql-ref-privileges.md) on the tables and views in the catalog.\"\n   \"[TABLE_SHARE_USAGE](information-schema/table_share_usage.md)\", \"Describes the tables referenced in shares.\"\n   \"[TABLES](information-schema/tables.md)\", \"Describes tables and views defined within the catalog.\"\n   \"[VIEWS](information-schema/views.md)\", \"Describes view specific information about the views in the catalog.\"\n\n## Notes\n\nWhile identifiers are case-insensitive when referenced in SQL statements, they are stored in the information schema as `STRING`.\nThis implies that you must either search for them using the case in which the identifier is stored, or use functions such as [ilike](functions/ilike.md).\n\n## Examples\n\n```sql\n> SELECT table_name, column_name\n    FROM information_schema.columns\n    WHERE data_type = 'DOUBLE'\n      AND table_schema = 'information_schema';\n```\n\nThe following are examples of workflows that use the system level information schema tables.\n\nIf you want to view all tables that have been created in the last 24 hours, your query could look like the following.\n\n```sql\n> SELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\n    FROM system.information_schema.tables\n    WHERE  datediff(now(), last_altered) < 1;\n```\n\nIf you want to view how many tables you have in each schema, consider the following example.\n\n```sql\n> SELECT table_schema, count(table_name)\n    FROM system.information_schema.tables\n    WHERE table_schema = 'tpch'\n    GROUP BY table_schema\n    ORDER BY 2 DESC\n```\n\n## Related articles\n\n- [SHOW](index.md#show-statements)\n- [DESCRIBE](index.md#describe-statements)\n\n.. toctree::\n    :maxdepth: 1\n    :glob:\n    :hidden:\n\n    information-schema/*\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What does the \"ROUTINES\" table in the information schema describe?", "gold_answer": "The \"ROUTINES\" table in the information schema describes routines (functions) in the catalog."}
{"hard_neg_docs": [{"page_content": "## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the recommended workflow for merging changes from a feature branch to the main branch in Databricks Repos?", "gold_answer": "The recommended workflow for merging changes from a feature branch to the main branch in Databricks Repos is as follows:\n\n1. Clone your existing Git repository to your Databricks workspace.\n2. Use the Repos UI to create a feature branch from the main branch. The example provided mentions a single feature branch named _feature-b_ for simplicity, but multiple feature branches can be used.\n3. Make modifications to Databricks notebooks and other files in the Repo.\n4. Commit and push your changes to your Git provider.\n5. Coworkers can clone the Git repository into their own user folder, make changes on a new branch, and then commit and push their changes to the Git provider.\n6. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Afterward, use the Repos UI to pull changes into the _feature-b_ branch in the Databricks Repo.\n7. When you are ready to merge your work to the main branch, use your Git provider to create a pull request (PR) to merge the changes from _feature-b_.\n8. In the Repos UI, pull changes to the main branch after the PR is completed."}
{"hard_neg_docs": [{"page_content": "## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I clone an existing Git repository to my Databricks workspace?", "gold_answer": "To clone an existing Git repository to your Databricks workspace, you can follow these steps:\n\n1. Navigate to the [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md) documentation.\n2. Follow the instructions provided in the documentation to perform the cloning operation within your Databricks workspace.\n\nPlease note that the actual link to the documentation is not active in this interface, but in practice, you would access the Databricks documentation and locate the section on cloning a Git repository to find detailed steps and guidance on the process."}
{"hard_neg_docs": [{"page_content": "Fast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}, {"page_content": "<!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\n<!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}], "question": "What is the difference between the serverless data plane and classic data plane for Databricks SQL in AWS?", "gold_answer": "The document provided doesn't go into specific details about the differences between the serverless data plane and the classic data plane for Databricks SQL in AWS. However, it does mention a couple of key points:\n\n- Worker nodes in the serverless data plane are private, meaning they do not have public IP addresses.\n- For Databricks SQL Serverless, communication between the control plane and the serverless data plane uses private connectivity.\n- When reading or writing to AWS S3 buckets in the same region as your workspace, serverless SQL warehouses use direct access to S3 using AWS gateway endpoints.\n\nThe document also references diagrams that compare the classic and serverless data planes for both serverless features, but since the diagrams are not described in the text, we can only infer that they illustrate additional differences not explicitly described in the provided content. To get a full understanding of the differences, one would need to view the actual diagrams included in the source."}
{"hard_neg_docs": [{"page_content": "Fast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}, {"page_content": "<!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\n<!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)", "fmt_content": "Source: https://docs.databricks.com/serverless-compute/index.html\nContent:\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)", "metadata": {"source": "https://docs.databricks.com/serverless-compute/index.html", "seq_num": 1590, "description": "", "text": "---\ndescription: Learn about the <ServerlessCompute> version of the <Databricks> platform architecture.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# <ServerlessCompute>\n\nWith the <ServerlessCompute> version of the Databricks platform architecture, the compute layer exists in your <Databricks> account rather than your <CloudAccount>.\n\n.. aws::\n\n  .. toctree::\n      :maxdepth: 1\n      :hidden:\n\n      /serverless-compute/serverless-quotas\n\n## <DBSQL> serverless \n\n.. include:: /shared/serverless-sql-preview.md\n\n[Databricks SQL Serverless](/sql/admin/index.md) supports <ServerlessCompute>. Admins can create <ServerlessSQLWarehouses> (formerly SQL endpoints) that enable instant compute and are managed by <Databricks>. <ServerlessSQLWarehouses> use compute clusters in the <CloudAccount> of <Databricks>. Use them with <DBSQL> queries just like you normally would with the original customer-hosted SQL warehouses, which are now called classic SQL warehouses.\n\nDatabricks changed the name from SQL endpoint to SQL warehouse because, in the industry, endpoint refers to either a remote computing device that communicates with a network that it's connected to, or an entry point to a cloud service. A data warehouse is a data management system that stores current and historical data from multiple sources in a business friendly manner for easier insights and reporting. SQL warehouse accurately describes the full capabilities of this compute resource.\n\nBefore you can create <ServerlessSQLWarehouses>, you must [enable serverless Databricks SQL warehouses for your workspace](/sql/admin/serverless.md). If <ServerlessSQLWarehouses> are enabled for your workspace:\n\n- New SQL warehouses are serverless by default when created from the [UI](/sql/admin/sql-endpoints.md#create-warehouse) or the [API](/sql/api/sql-endpoints.md#create-warehouse), but you can also create new pro or classic SQL warehouses.\n\n- You can [upgrade a pro or classic SQL warehouse to a serverless SQL warehouse](/sql/admin/sql-endpoints.md#convert-to-serverless) or a classic SQL warehouse to a pro SQL warehouse. You can also downgrade from serverless to pro or classic.\n\n- This feature only affects <DBSQL>. It does not affect how <DBR> clusters work with notebooks and jobs in the <DSE> or <DML> workspace environments. <DBR> clusters always run in the classic data plane in your <cloud> account. \n\nFor regional support, see [_](/resources/supported-regions.md).\n\n## Model Serving\n\nModel Serving deploys your MLflow machine learning (ML) models and exposes them as REST API endpoints that run in your <Databricks> account. The <ServerlessCompute> resources run as <Databricks> <cloud> resources in what is known as the serverless data plane. \n\nIn contrast, the legacy model serving architecture is a single-node cluster that runs in your  <CloudAccount> within the classic data plane.\n\n- **Easy configuration and compute resource management**: <Databricks> automatically prepares a production-ready environment for your model and makes it easy to switch its compute configuration.\n\n- **High availability and scalability**:  Serverless model endpoints autoscale, which means that the number of server replicas automatically adjusts based on the volume of scoring requests.\n\n- **Dashboards**: Use the built-in serverless model endpoint dashboard to monitor the health of your model endpoints using metrics such as queries-per-second (QPS), latency, and error rate.\n\nFor regional support, see [_](/resources/supported-regions.md).\n\n.. aws:: \n\n  Before you can use serverless Model Serving, you must [enable serverless compute for your account](/machine-learning/model-serving/migrate-model-serving.md).\n\n  <!-- currently only AWS requires enablement -- eventually we probably will add switch azure account level but pre-enabled, so we'd need to here mention that for azure -->\n\n.. aws::\n\n  <a id=\"quotas\"></a>\n\n  ## Serverless quotas\n\n  .. include:: /shared/serverless-quotas.md\n\n<a id=\"compare\"></a>\n\n## Compare <ServerlessCompute> to other <Databricks> architectures\n\n<Databricks> operates out of a *control plane* and a *data plane*:\n\n- The control plane includes the backend services that <Databricks> manages in its own <CloudAccount>. Databricks SQL queries, notebook commands, and many other workspace configurations are stored in the control plane and encrypted at rest.\n\n- The data plane is where data is processed by clusters of compute resources.\n\nThere are important differences between the _classic data plane_ (the original <Databricks> platform architecture) and the _serverless data plane_:\n\n- For a classic data plane, <Databricks> compute resources run in the customer’s <CloudAccount>. Clusters perform distributed data analysis using queries (in <DBSQL>) or notebooks (in the <DSE> or <DML> environments):\n\n  - New clusters are created within each workspace’s virtual network in the customer’s <CloudAccount>.\n\n  - A classic data plane has natural isolation because it runs in each customer’s own <CloudAccount>. A classic data plane is not a shared resource for multiple customers.\n\n- For a serverless data plane, <Databricks> compute resources run in a special compute layer within the <CloudAccount> for <Databricks>:\n\n  - The serverless data plane is used for <ServerlessSQLWarehouses> and Model Serving. Enabling serverless compute does not change how <DBR> clusters work in the <DSE> or <DML> environments.\n\n  - The serverless data plane is a shared resource in your <Databricks> account.\n\n  - To protect customer data within the serverless data plane, serverless compute runs within a network boundary for the workspace, with various layers of security to isolate different <Databricks> customer workspaces and additional network controls between clusters of the same customer.\n\n.. aws::\n  .. <CloudAccount> replace:: AWS account\n.. azure::\n  .. <CloudAccount> replace:: Azure subscription\n\n<Databricks> creates a serverless data plane in the same <cloud> region as your workspace’s classic data plane.\n\n.. aws::\n\n  Worker nodes are private, which means they do not have public IP addresses.\n\n  For communication between the <Databricks> control plane and the serverless data plane:\n\n  - For Databricks SQL Serverless, the communication uses private connectivity.\n  - For Model Serving, the communication uses mTLS encrypted communication with connection initiated from the control plane with access limited to control plane IP addresses.\n\n  When reading or writing to AWS S3 buckets in the same region as your workspace, [serverless SQL warehouses](/serverless-compute/index.md) now use direct access to S3 using AWS gateway endpoints. This applies when a serverless SQL warehouse reads and writes to your workspace's root S3 bucket in your AWS account and to other S3 data sources in the same region.\n\n\nThe following diagram shows important differences between the serverless data plane and classic data plane for both serverless features.\n\n.. aws::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-aws.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-aws.png)\n\n.. azure::\n\n  ![Compare classic and serverless data plane for Databricks SQL](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-dbsql-azure.png)\n\n  ![Compare classic and serverless data plane for Model Serving](/_static/images/serverless-compute/serverless-compute-data-plane-comparison-models-azure.png)\n\nFor more information about secure cluster connectivity, which is mentioned in the diagram, see [_](/security/network/secure-cluster-connectivity.md).\n\nThe table below summarizes differences between <ServerlessCompute> and the classic data plane architecture of <Databricks>, focusing on product security. It is not a complete explanation of those security features or a detailed comparison. For more details about <ServerlessCompute> security, or if you have questions about items in this table, contact your <Databricks> representative.\n\n<!--  WRITER NOTE: as of Wed, Jul 27, 2022 , the compare tables look very different for AWS and Azure. the AWS one wa first and legal team at the time wanted all those rows, and also wanted the references to azure in the table even though it was an AWS only feature then, for complicated legal reasons and especially knowing people were potentially pointing early azure customers to that article, which is a WEIRD reason but whatever.\n\nFast forward to early 2022 and the Azure Private Preview docs came out and the serverless team lobbied for a much simplified one for the azure private preview docs.\n\nit'd be nice to get legal team approval to simplify these tables for AWS too. I guess the question is are there things that are there that either field security or legal think is important to still have there for AWS?   -->\n\n.. aws::\n\n  .. list-table::\n    :header-rows: 1\n\n    * - Item\n      - Serverless data plane (AWS only)\n      - Classic data plane (AWS and Azure)\n    * - **Location of control plane resources**\n      - Databricks cloud account\n      - Databricks cloud account\n    * - **Location of data plane compute resources**\n      - Serverless data plane (VPC in the Databricks AWS account)\n      - Classic data plane (VPC in the customer’s cloud provider account)\n    * - **Data plane compute resources**\n      - Databricks-managed Kubernetes (EKS) clusters\n      - Databricks-managed standalone VMs\n    * - **Customer access to data plane**\n      - Access through Databricks control plane\n      -\n        - **AWS**: Direct access in customer's AWS account. Additional indirect access through Databricks control plane.\n        - **Azure**: Direct read-only access to clusters, even with VNet injection (customer-managed VNet). Additional indirect access through Databricks control plane.\n    * - **Who pays for unassigned VMs for Databricks SQL?**\n      - Databricks\n      - Not applicable. For pro and classic SQL warehouses, there is no concept of unassigned VMs. In Databricks SQL, there is no direct equivalent to warm instance pools for notebooks and jobs.\n    * - **Who pays for VMs after starting a warehouse or running a query in Databricks SQL?**\n      - Customer pays based on DBUs until Auto Stop stops the SQL warehouse.\n      - Customer pays AWS for the VMs, and customer pays Databricks based on DBUs.\n    * - **Virtual private network (VPC) for data plane**\n      - VPC in Databricks account is shared among customers, with additional network boundaries between workspaces and between clusters.\n      -\n        - **AWS**: Exclusive, the VPC is in customer's account.\n        - **Azure**: Exclusive, the VNet is in customer's account.\n    * - **OS image**\n      - Databricks-modified cloud-managed Amazon-linux2\n      - Databricks-managed Ubuntu or CentOS\n    * - **Technology that manages default egress from the VPC**\n      - Databricks-created AWS internet gateway\n      - Default internet gateway or load balancer provided by the cloud\n    * - **Customize VPC and firewall settings**\n      - No\n      - Yes\n    * - **Customize CIDR ranges**\n      - No\n      - Yes\n    * - **Public IPs**\n      - No\n      - Secure cluster connectivity\n        - When disabled, one public IP for each VM.\n        - When enabled (the default), no public IPs for VMs.\n", "timestamp": 0, "next": ""}}], "question": "How does communication between Databricks control plane and serverless data plane work for Model Serving?", "gold_answer": "For Model Serving, the communication between the Databricks control plane and the serverless data plane uses mTLS encrypted communication. The connection is initiated from the control plane with access limited to control plane IP addresses."}
{"hard_neg_docs": [{"page_content": "## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I ensure that each row in the training dataset represents the latest known feature values?", "gold_answer": "To ensure that each row in the training dataset represents the latest known feature values, especially when feature values change over time, you should use time series feature tables. When creating a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This facilitates point-in-time lookups when you use the `create_training_set` or `score_batch` functions. The system will perform an as-of timestamp join, using the `timestamp_lookup_key` you specify, to ensure that each row in the training dataset represents the latest known feature values as of the row's timestamp."}
{"hard_neg_docs": [{"page_content": "## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the offline feature store in the feature store architecture?", "gold_answer": "The purpose of the offline feature store in Databricks feature store architecture is for feature discovery, model training, and batch inference. It contains feature tables that are materialized as Delta tables."}
{"hard_neg_docs": [{"page_content": "For more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "See also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\n---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are the supported storage formats for managed Unity Catalog tables?", "gold_answer": "The supported storage format for managed Unity Catalog tables is Delta."}
{"hard_neg_docs": [{"page_content": "For more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "See also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.", "fmt_content": "Source: https://docs.databricks.com/release-notes/unity-catalog/20220825.html\nContent:\n---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.", "metadata": {"source": "https://docs.databricks.com/release-notes/unity-catalog/20220825.html", "seq_num": 2110, "description": "", "text": "---\ndescription: <UC> general availability release notes (25 August 2022).\norphan: 1\n---\n\n# <UC> GA release note\n\n**August 25, 2022**\n\n<UC> is now generally available on <Databricks>.\n\n**This article describes <UC> as of the date of its GA release.** It focuses primarily on the features and updates added to <UC> since the Public Preview. For current information about <UC>, see [_](/data-governance/unity-catalog/index.md). For release notes that describe updates to <UC> since GA, see [_](/release-notes/product/index.md) and [_](/release-notes/runtime/index.md).\n\n## Metastore limits and resource quotas\n\nAs of August 25, 2022\n\n- Your <Databricks> account can have only one metastore per region\n- A metastore can have up to 1000 catalogs.\n- A catalog can have up to 10,000 schemas.\n- A schema can have up to 10,000 tables.\n\nFor current <UC> quotas, see [_](/data-governance/unity-catalog/index.md#quotas).\n\n## Supported storage formats at GA\n\nAs of August 25, 2022:\n\n- All managed <UC> tables store data with <Delta>\n- External <UC> tables and external locations support <Delta>, JSON, CSV, Avro, Parquet, ORC, and text data.\n\nFor current <UC> supported table formats, see [_](/data-governance/unity-catalog/index.md#file-formats).\n\n## Manage <UC> resources from the account console\n\nUse the <Databricks> account console UI to:\n\n- [Manage the metastore lifecycle](/data-governance/unity-catalog/create-metastore.md) (create, update, delete, and view <UC>-managed metastores)\n- Assign and remove metastores for workspaces\n\n## Supported cluster types and <DBR> versions\n\n<UC> requires clusters that run <DBR> 11.1 or above. <UC> is supported by default on all [SQL warehouse](/sql/admin/sql-endpoints.md) compute versions.\n\nEarlier versions of <DBR> supported preview versions of <UC>. Clusters running on earlier versions of <DBR> do not provide support for all <UC> GA features and functionality.\n\n<UC> requires one of the following access modes when you create a new cluster:\n\n- Shared\n    - Languages: SQL or Python\n    - A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials.\n- Single user\n    - Languages: SQL, Scala, Python, R\n    - A secure cluster that can be used exclusively by a specified single user.\n\nFor more information about cluster access modes, see [_](/data-governance/unity-catalog/compute.md).\n\nFor information about updated <UC> functionality in later <DBR> versions, see the [release notes](/release-notes/runtime/releases.md) for those versions.\n\n## System tables\n\n`information_schema` is fully supported for <UC> data assets. Each metastore includes a catalog referred to as `system` that includes a metastore scoped `information_schema`. See [_](/sql/language-manual/sql-ref-information-schema.md). You can use `information_schema` to answer questions like the following:\n\n**“Count the number of tables per catalog”**\n\n```sql\nSELECT table_catalog, count(table_name)\nFROM system.information_schema.tables\nGROUP BY 1\nORDER by 2 DESC\n```\n\n**“Show me all of the tables that have been altered in the last 24 hours“**\n\n```sql\nSELECT table_name, table_owner, created_by, last_altered, last_altered_by, table_catalog\nFROM system.information_schema.tables\nWHERE  datediff(now(), last_altered) < 1\n```\n\n## <SS> support\n\n<SS> workloads are now supported with <UC>. For details and limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\nSee also [_](/structured-streaming/unity-catalog.md).\n\n## SQL functions\n\nUser-defined SQL functions are now fully supported on <UC>. For information about how to create and use SQL UDFs, see [_](/sql/language-manual/sql-ref-syntax-ddl-create-sql-function.md).\n\n## SQL syntax for external locations in <UC>\n\nStandard data definition and data definition language commands are now supported in Spark SQL for external locations, including the following:\n\n```sql\nCREATE | DROP | ALTER | DESCRIBE | SHOW EXTERNAL LOCATION\n```\n\nYou can also manage and view permissions with `GRANT`, `REVOKE`, and `SHOW` for external locations with SQL. See [_](/sql/language-manual/sql-ref-external-locations.md).\n\nExample Syntax:\n\n```sql\nCREATE EXTERNAL LOCATION <your_location_name>\n  URL `<your_location_path>'\n  WITH (CREDENTIAL <your_credential_name>);\n\nGRANT READ_FILE\n  ON EXTERNAL LOCATION <your_location_name>\n  TO <group>;\n```\n\n<a id=\"limitations\"></a>\n\n## <UC> limitations at GA\n\nAs of August 25, 2022, <UC> had the following limitations. For current limitations, see [_](/data-governance/unity-catalog/index.md#limitations).\n\n- Scala, R, and workloads using the Machine Learning Runtime are supported only on clusters using the single user access mode. Workloads in these languages do not support the use of dynamic views for row-level or column-level security.\n\n- Shallow clones are not supported when using <UC> as the source or target of the clone.\n\n- Bucketing is not supported for <UC> tables. If you run commands that try to create a bucketed table in <UC>, it will throw an exception.\n\n- Writing to the same path or <Delta> table from workspaces in multiple regions can lead to unreliable performance if some clusters access <UC> and others do not.\n\n- Overwrite mode for DataFrame write operations into <UC> is supported only for Delta tables, not for other file formats. The user must have the `CREATE` privilege on the parent schema and must be the owner of the existing object. <!-- DOC-6832 sez this isn't true-->\n\n- Streaming currently has the following limitations:\n\n  - It is not supported in clusters using shared access mode. For streaming workloads, you must use single user access mode.\n\n  - Asynchronous checkpointing is not yet supported.\n\n  - On <DBR> version 11.2 and below, streaming queries that last more than 30 days on all-purpose or jobs clusters will throw an exception. For long-running streaming queries, configure [automatic job retries](/workflows/jobs/jobs.md#retries) or use <DBR> 11.3 and above.\n\n- Referencing <UC> tables from Delta Live Tables pipelines is currently not supported.\n\n- Groups previously created in a workspace cannot be used in <UC> GRANT statements. This is to ensure a consistent view of groups that can span across workspaces. To use groups in GRANT statements, create your groups in the account console and update any automation for principal or group management (such as SCIM, Okta and AAD connectors, and Terraform) to reference account endpoints instead of workspace endpoints.\n\n.. aws::\n\n  - <UC> requires the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are on E2. If you are unsure which account type you have, contact your <Databricks> representative.\n\n## <UC> availability regions at GA\n\nAs of August 25, 2022, <UC> was available in the following regions. For the list of currently supported regions, see [_](/resources/supported-regions.md).\n\n.. aws::\n\n  - `us-east-1`\n  - `us-east-2`\n  - `us-west-2`\n  - `ap-northeast-1`\n  - `ap-northeast-2`\n  - `ap-south-1`\n  - `ap-southeast-1`\n  - `ap-southeast-2`\n  - `ca-central-1`\n  - `eu-central-1`\n  - `eu-west-1`\n  - `eu-west-2`\n\n.. azure::\n\n  - `canadacentral`\n  - `centralus`\n  - `francecentral`\n  - `germanywestcentral`\n  - `japaneast`\n  - `norwayeast`\n  - `southafricanorth`\n  - `swedencentral`\n  - `switzerlandnorth`\n  - `switzerlandwest`\n  - `uaenorth`\n  - `westcentralus`\n  - `westus3`\n  - `australiaeast`\n  - `brazilsouth`\n  - `centralindia`\n  - `eastus`\n  - `eastus2`\n  - `koreacentral`\n  - `northcentralus`\n  - `northeurope`\n  - `southeastasia`\n  - `ukwest`\n  - `westeurope`\n  - `westus`\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How many catalogs can a metastore have in Databricks?", "gold_answer": "A metastore in Databricks can have up to 1000 catalogs."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the constraint violation mentioned in PARAMETER_CONSTRAINT_VIOLATION?", "gold_answer": "The constraint violation mentioned in PARAMETER_CONSTRAINT_VIOLATION is that the `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`). The specific constraint is not listed in the provided document, as the placeholders `<constraint>`, `<leftExprName>`, `<leftExprValue>`, `<rightExprName>`, and `<rightExprValue>` would be replaced by the actual values pertaining to the context in which the error is encountered."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER", "fmt_content": "Source: https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html\nContent:\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER", "metadata": {"source": "https://docs.databricks.com/error-messages/datatype-mismatch-error-class.html", "seq_num": 1638, "description": "", "text": "---\ndescription: Documentation for the DATATYPE_MISMATCH error class on <Databricks>\n---\n\n# DATATYPE_MISMATCH error class\n\n[SQLSTATE: 42K09](sqlstates.md#class-42-syntax-error-or-access-rule-violation)\n\nCannot resolve `<sqlExpr>` due to data type mismatch:\n\nThis error class has the following derived error classes:\n\n## ARRAY_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_ARRAY_DIFF_TYPES\n\nInput to function `<functionName>` should have been two `<arrayType>` with same element type, but it's [`<leftType>`, `<rightType>`].\n\n## BINARY_OP_DIFF_TYPES\n\nthe left and right operands of the binary operator have incompatible types (`<left>` and `<right>`).\n\n## BINARY_OP_WRONG_TYPE\n\nthe binary operator requires the input type `<inputType>`, not `<actualDataType>`.\n\n## BLOOM_FILTER_BINARY_OP_WRONG_TYPE\n\nThe Bloom filter binary input to `<functionName>` should be either a constant value or a scalar subquery expression, but it's `<actual>`.\n\n## BLOOM_FILTER_WRONG_TYPE\n\nInput to function `<functionName>` should have been `<expectedLeft>` followed by value with `<expectedRight>`, but it's [`<actual>`].\n\n## CANNOT_CONVERT_TO_JSON\n\nUnable to convert column `<name>` of type `<type>` to JSON.\n\n## CANNOT_DROP_ALL_FIELDS\n\nCannot drop all fields in struct.\n\n## CAST_TO_NAMED_PARAMETER\n\nThe function `<functionName>` parameter `<parameterName>` at position `<pos>` requires `<requiredType>`. The argument given is `<argumentType>`.\n\n## CAST_WITHOUT_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\n## CAST_WITH_CONF_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>` with ANSI mode on.\n\nIf you have to cast `<srcType>` to `<targetType>`, you can set `<config>` as `<configVal>`.\n\n## CAST_WITH_FUNC_SUGGESTION\n\ncannot cast `<srcType>` to `<targetType>`.\n\nTo convert values from `<srcType>` to `<targetType>`, you can use the functions `<functionNames>` instead.\n\n## CREATE_MAP_KEY_DIFF_TYPES\n\nThe given keys of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_MAP_VALUE_DIFF_TYPES\n\nThe given values of function `<functionName>` should all be the same type, but they are `<dataType>`.\n\n## CREATE_NAMED_STRUCT_WITHOUT_FOLDABLE_STRING\n\nOnly foldable `STRING` expressions are allowed to appear at odd position, but they are `<inputExprs>`.\n\n## DATA_DIFF_TYPES\n\nInput to `<functionName>` should all be the same type, but it's `<dataType>`.\n\n## FILTER_NOT_BOOLEAN\n\nFilter expression `<filter>` of type `<type>` is not a boolean.\n\n## HASH_MAP_TYPE\n\nInput to the function `<functionName>` cannot contain elements of the \"MAP\" type. In Spark, same maps may have different hashcode, thus hash expressions are prohibited on \"MAP\" elements. To restore previous behavior set \"spark.sql.legacy.allowHashOnMapType\" to \"true\".\n\n## INPUT_SIZE_NOT_ONE\n\nLength of `<exprName>` should be 1.\n\n## INVALID_ARG_VALUE\n\nThe `<inputName>` value must to be a `<requireType>` literal of `<validValues>`, but got `<inputValue>`.\n\n## INVALID_JSON_MAP_KEY_TYPE\n\nInput schema `<schema>` can only contain STRING as a key type for a MAP.\n\n## INVALID_JSON_SCHEMA\n\nInput schema `<schema>` must be a struct, an array or a map.\n\n## INVALID_MAP_KEY_TYPE\n\nThe key of map cannot be/contain `<keyType>`.\n\n## INVALID_ORDERING_TYPE\n\nThe `<functionName>` does not support ordering on type `<dataType>`.\n\n## IN_SUBQUERY_DATA_TYPE_MISMATCH\n\nThe data type of one or more elements in the left hand side of an IN subquery is not compatible with the data type of the output of the subquery. Mismatched columns: [`<mismatchedColumns>`], left side: [`<leftType>`], right side: [`<rightType>`].\n\n## IN_SUBQUERY_LENGTH_MISMATCH\n\nThe number of columns in the left hand side of an IN subquery does not match the number of columns in the output of subquery. Left hand side columns(length: `<leftLength>`): [`<leftColumns>`], right hand side columns(length: `<rightLength>`): [`<rightColumns>`].\n\n## MAP_CONCAT_DIFF_TYPES\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`.\n\n## MAP_FUNCTION_DIFF_TYPES\n\nInput to `<functionName>` should have been `<dataType>` followed by a value with same key type, but it's [`<leftType>`, `<rightType>`].\n\n## MAP_ZIP_WITH_DIFF_TYPES\n\nInput to the `<functionName>` should have been two maps with compatible key types, but it's [`<leftType>`, `<rightType>`].\n\n## NON_FOLDABLE_INPUT\n\nthe input `<inputName>` should be a foldable `<inputType>` expression; however, got `<inputExpr>`.\n\n## NON_STRING_TYPE\n\nall arguments must be strings.\n\n## NULL_TYPE\n\nNull typed values cannot be used as arguments of `<functionName>`.\n\n## PARAMETER_CONSTRAINT_VIOLATION\n\nThe `<leftExprName>`(`<leftExprValue>`) must be `<constraint>` the `<rightExprName>`(`<rightExprValue>`).\n\n## RANGE_FRAME_INVALID_TYPE\n\nThe data type `<orderSpecType>` used in the order specification does not match the data type `<valueBoundaryType>` which is used in the range frame.\n\n## RANGE_FRAME_MULTI_ORDER\n\nA range window frame with value boundaries cannot be used in a window specification with multiple order by expressions: `<orderSpec>`.\n\n## RANGE_FRAME_WITHOUT_ORDER\n\nA range window frame cannot be used in an unordered window specification.\n\n## SEQUENCE_WRONG_INPUT_TYPES\n\n`<functionName>` uses the wrong parameter type. The parameter type must conform to:\n\n1. The start and stop expressions must resolve to the same type.\n\n2. If start and stop expressions resolve to the `<startType>` type, then the step expression must resolve to the `<stepType>` type.\n\n3. Otherwise, if start and stop expressions resolve to the `<otherStartType>` type, then the step expression must resolve to the same type.\n\n## SPECIFIED_WINDOW_FRAME_DIFF_TYPES\n\nWindow frame bounds `<lower>` and `<upper>` do not have the same type: `<lowerType>` <> `<upperType>`.\n\n## SPECIFIED_WINDOW_FRAME_INVALID_BOUND\n\nWindow frame upper bound `<upper>` does not follow the lower bound `<lower>`.\n\n## SPECIFIED_WINDOW_FRAME_UNACCEPTED_TYPE\n\nThe data type of the `<location>` bound `<exprType>` does not match the expected data type `<expectedType>`.\n\n## SPECIFIED_WINDOW_FRAME_WITHOUT_FOLDABLE\n\nWindow frame `<location>` bound `<expression>` is not a literal.\n\n## SPECIFIED_WINDOW_FRAME_WRONG_COMPARISON\n\nThe lower bound of a window frame must be `<comparison>` to the upper bound.\n\n## STACK_COLUMN_DIFF_TYPES\n\nThe data type of the column (`<columnIndex>`) do not have the same type: `<leftType>` (`<leftParamIndex>`) <> `<rightType>` (`<rightParamIndex>`).\n\n## UNEXPECTED_CLASS_TYPE\n\nclass `<className>` not found.\n\n## UNEXPECTED_INPUT_TYPE\n\nparameter `<paramIndex>` requires `<requiredType>` type, however, `<inputSql>` is of `<inputType>` type.\n\n## UNEXPECTED_NULL\n\nThe `<exprName>` must not be null.\n\n## UNEXPECTED_RETURN_TYPE\n\nThe `<functionName>` requires return `<expectedType>` type, but the actual is `<actualType>` type.\n\n## UNEXPECTED_STATIC_METHOD\n\ncannot find a static method `<methodName>` that matches the argument types in `<className>`.\n\n## UNSUPPORTED_INPUT_TYPE\n\nThe input of `<functionName>` can't be `<dataType>` type data.\n\n## VALUE_OUT_OF_RANGE\n\nThe `<exprName>` must be between `<valueRange>` (current value = `<currentValue>`).\n\n## WRONG_NUM_ARGS\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\n## WRONG_NUM_ARGS_WITH_SUGGESTION\n\nThe `<functionName>` requires `<expectedNum>` parameters but the actual number is `<actualNum>`.\n\nIf you have to call this function with `<legacyNum>` parameters, set the legacy configuration `<legacyConfKey>` to `<legacyConfValue>`.\n\n## WRONG_NUM_ENDPOINTS\n\nThe number of endpoints must be >= 2 to construct intervals but the actual number is `<actualNumber>`.\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the error message for when the input to MAP_CONCAT_DIFF_TYPES is not of type map?", "gold_answer": "The error message for when the input to `MAP_CONCAT_DIFF_TYPES` is not of type map is:\n\nThe `<functionName>` should all be of type map, but it's `<dataType>`."}
{"hard_neg_docs": [{"page_content": "- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the result of the query \"SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null)\"?", "gold_answer": "The result of the query \"SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\" would be:\n\n```\n NULL | 1 | 1\n```\n\nHere's the breakdown of the result:\n- The first column selects the SQL `NULL` keyword and shows `NULL`.\n- The second column uses backticks to escape the word `null`, treating it as an identifier rather than a keyword, and since it corresponds to the value provided in the `VALUES` clause, it shows `1`.\n- The third column refers to the column named `null` in the table alias `T`, which also has the value `1` as specified in the `VALUES` clause."}
{"hard_neg_docs": [{"page_content": "- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html\nContent:\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/sql-ref-reserved-words.html", "seq_num": 684, "description": "", "text": "---\ndescription: Learn about reserved words and databases in <DBSQL>.\n---\n\n\n# Reserved words and schemas\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReserved words are literals used as keywords by the SQL language\nwhich should not be used as identifiers to avoid unexpected behavior.\n\nReserved schema names have special meaning to <Databricks>.\n\n## Reserved words\n\n<Databricks> does not formally disallow any specific literals from being used as [identifiers](sql-ref-identifiers.md).\n\nHowever, to use any of the following list of [identifiers](sql-ref-identifiers.md) as a [table alias](sql-ref-names.md#table-alias), you must surround the name with back-ticks (\\`).\n\n- ANTI\n- CROSS\n- EXCEPT\n- FULL\n- INNER\n- INTERSECT\n- JOIN\n- LATERAL\n- LEFT\n- MINUS\n- NATURAL\n- ON\n- RIGHT\n- SEMI\n- UNION\n- USING\n\n## Special words in expressions\n\nThe following list of [identifiers](sql-ref-identifiers.md) can be used anywhere, but <Databricks> treats them preferentially as keywords within [expressions](sql-ref-expression.md) in certain contexts:\n\n- `NULL`\n\n  The SQL `NULL` value.\n\n- `DEFAULT`\n\n  Indicates a column default.\n\n- `TRUE`\n\n  The SQL boolean `true` value.\n\n- `FALSE`\n\n  The SQL boolean `false` value.\n\n- `LATERAL`\n\n  Used as a column qualifier to indicate explicit lateral correlation.\n\nUse back-ticks (&#96;`NULL`&#96; and &#96;`DEFAULT`&#96;) or qualify the column names with a table name or alias.\n\n<Databricks> uses the `CURRENT_` prefix to refer to some **configuration settings** or other context variables.\nThe underbar (`_`) prefix is intended for <Databricks> **pseudo columns**.\nIn <DBR>, an existing pseudo column is the [_metadata column](/ingestion/file-metadata-column.md).\n\nIdentifiers with these prefixes are **not** treated preferentially.\nHowever, avoid columns or [column aliases](sql-ref-names.md#column-alias) using these prefixes to avoid unexpected behavior.\n\n## Reserved schema names\n\n<Databricks> reserves the following list of [schema names](sql-ref-names.md#schema-name) for current or future use:\n\n- `BUILTIN`\n\n  Future use to qualify builtin functions.\n\n- `SESSION`\n\n  Future use to qualify temporary views and functions.\n\n- `INFORMATION_SCHEMA`\n\n  Holds the SQL Standard [information schema](sql-ref-information-schema.md).\n\n- Database names starting with `SYS`\n\nAvoid using these names.\n\n## ANSI Reserved words\n\n<Databricks> does not enforce ANSI reserved words.\nThe following list of **SQL2016** keywords is provided for informational purposes only.\n\n- **A**\n\n  ALL, ALTER, AND, ANY, ARRAY, AS, AT, AUTHORIZATION\n\n- **B**\n\n  BETWEEN, BOTH, BY\n\n- **C**\n\n  CASE, CAST, CHECK, COLLATE, COLUMN, COMMIT, CONSTRAINT, CREATE, CROSS, CUBE, CURRENT, CURRENT_DATE,\n  CURRENT_TIME, CURRENT_TIMESTAMP, CURRENT_USER\n\n- **D**\n\n  DELETE, DESCRIBE, DISTINCT, DROP\n\n- **E**\n\n  ELSE, END, ESCAPE, EXCEPT, EXISTS, EXTERNAL, EXTRACT\n\n- **F**\n\n  FALSE, FETCH, FILTER, FOR, FOREIGN, FROM, FULL, FUNCTION\n\n- **G**\n\n  GLOBAL, GRANT, GROUP, GROUPING\n\n- **H**\n\n  HAVING\n\n- **I**\n\n  IN, INNER, INSERT, INTERSECT, INTERVAL, INTO, IS\n\n- **J**\n\n  JOIN\n\n- **L**\n\n  LEADING, LEFT, LIKE, LOCAL\n\n- **N**\n\n  NATURAL, NO, NOT, NULL\n\n- **O**\n\n  OF, ON, ONLY, OR, ORDER, OUT, OUTER, OVERLAPS\n\n- **P**\n\n  PARTITION, POSITION, PRIMARY\n\n- **R**\n\n  RANGE, REFERENCES, REVOKE, RIGHT, ROLLBACK, ROLLUP, ROW, ROWS\n\n- **S**\n\n  SELECT, SESSION_USER, SET, SOME, START\n\n- **T**\n\n  TABLE, TABLESAMPLE, THEN, TIME, TO, TRAILING, TRUE, TRUNCATE\n\n- **U**\n\n  UNION, UNIQUE, UNKNOWN, UPDATE, USER, USING\n\n- **V**\n\n  VALUES\n\n- **W**\n\n  WHEN, WHERE, WINDOW, WITH\n\n## Examples\n\n```sql\n-- Using SQL keywords\n> CREATE TEMPORARY VIEW where(where) AS (VALUES (1));\n\n> SELECT where from FROM where select;\n  1\n\n-- Usage of NULL\n> SELECT NULL, `null`, T.null FROM VALUES(1) AS T(null);\n NULL\t1\t1\n\n-- current_date is eclipsed by the column alias T.current_date\n> SELECT (SELECT current_date), current_date, current_date()\n    FROM VALUES(1) AS T(current_date);\n 2021-10-23\t1\t2021-10-23\n\n-- Reserved keyword ANTI cannot be used as table alias\n> SELECT * FROM VALUES(1) AS ANTI;\nError in query: no viable alternative at input 'ANTI'\n\n> SELECT * FROM VALUES(1) AS `ANTI`;\n  1\n```\n\n## Related articles\n\n- [names](sql-ref-names.md)\n- [identifiers](sql-ref-identifiers.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Can the reserved keyword \"ANTI\" be used as a table alias in a query?", "gold_answer": "No, the reserved keyword \"ANTI\" cannot be used as a table alias in a query without enclosing it in backticks. Attempting to use \"ANTI\" as a table alias directly will result in an error. However, if you enclose \"ANTI\" in backticks, it can be used as a table alias. For example:\n\n```sql\n-- This will result in an error:\nSELECT * FROM VALUES(1) AS ANTI;\n\n-- This is valid and will not result in an error:\nSELECT * FROM VALUES(1) AS `ANTI`;\n```"}
{"hard_neg_docs": [{"page_content": "For instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I compare different versions of a registered model?", "gold_answer": "To compare different versions of a registered model, you would navigate to the registered model page for the model you are interested in. From there, you can use the feature to \"[Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions)\" which is listed among the actions you can perform on the model's page."}
{"hard_neg_docs": [{"page_content": "For instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.", "fmt_content": "Source: https://docs.databricks.com/mlflow/model-registry.html\nContent:\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.", "metadata": {"source": "https://docs.databricks.com/mlflow/model-registry.html", "seq_num": 1766, "description": "", "text": "---\ndescription: Learn about the MLflow Model Registry on <Databricks>, along with how to navigate its interface from the <DML> UI.\n---\n\n<a id=\"mlflow-model-registry\"></a>\n\n# MLflow Model Registry on <Databricks>\n\n.. gcp::\n\n  .. include:: /shared/mlflow-gcp-requirement.md\n\n[MLflow Model Registry](https://www.mlflow.org/docs/latest/model-registry.html) is a centralized model repository and a UI and set of APIs that enable you to manage the full lifecycle of MLflow Models. Model Registry provides:\n- Chronological model lineage (which MLflow experiment and run produced the model at a given time).\n- [Model Serving](/machine-learning/model-serving/index.md).\n- Model versioning.\n- Stage transitions (for example, from staging to production or archived).\n- [Webhooks](/mlflow/model-registry-webhooks.md) so you can automatically trigger actions based on registry events.\n- Email notifications of model events.\n\nYou can also create and view model descriptions and leave comments.\n\nYou can work with the model registry using either the Model Registry UI or the Model Registry API. This page presents conceptual information and also includes an example notebook illustrating what you can do with the Model Registry.\n\nFor instructions on how to use the Model Registry, see [_](/machine-learning/manage-model-lifecycle/index.md).\n\n## Model Registry concepts\n\n- **Model**: An MLflow Model logged from an experiment or run that is logged with one of the model flavor's `mlflow.<model-flavor>.log_model` methods. Once logged, you can register the model with the Model Registry.\n\n- **Registered model**: An MLflow Model that has been registered with the Model Registry. The registered model has a unique name, versions, model lineage, and other metadata.\n\n- **Model version**: A version of a registered model. When a new model is added to the Model Registry, it is added as Version 1. Each model registered to the same model name increments the version number.\n\n- **Model stage**: A model version can be assigned one or more stages. MLflow provides predefined stages for the common use-cases **None**, **Staging**, **Production**, and **Archived**. With the appropriate permission you can transition a model version between stages or you can request a model stage transition.\n\n- **Description**: You can annotate a model's intent, including description and any relevant information useful for the team such as algorithm description, dataset employed, or methodology.\n\n- **Activities**: Each registered model's activities---such as request for stage transition---is recorded. The trace of activities provides lineage and auditability of the model's evolution, from experimentation to staged versions to production.\n\n## Registered models page\n\nThe registered models page displays when you click <Models Icon> **Models** in the sidebar. This page shows all of the models in the registry with their current stage, last modified time, tags, and serving status. To display only models that have serving enabled, select **Serving enabled** from the **All models** dropdown.\n\nYou can [create a new model](/machine-learning/manage-model-lifecycle/index.md#create-a-new-registered-model-and-assign-a-logged-model-to-it) from this page.\n\nAlso from this page, workspace administrators can [set permissions for all models in the model registry](/security/auth-authz/access-control/workspace-acl.md#configure-permissions-for-all-mlflow-models-in-model-registry).\n\n![Registered models](/_static/images/mlflow/registered-models.png)\n\n## Registered model page\n\nTo display the registered model page for a model, click a model name in the registered models page. The registered model page shows information about the selected model and a table with information about each version of the model. From this page, you can also:\n- Set up [Model Serving](/machine-learning/model-serving/index.md).\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Configure email notifications](/machine-learning/manage-model-lifecycle/index.md#email-notification).\n- [Compare model versions](/machine-learning/manage-model-lifecycle/index.md#compare-model-versions).\n- [Set permissions for the model](/security/auth-authz/access-control/workspace-acl.md#assign-mlflow-model-permissions).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n  ![Registered model](/_static/images/mlflow/registered-model.png)\n\n## Model version page\n\nTo view the model version page, do one of the following:\n- Click a version name in the **Latest Version** column on the registered models page.\n- Click a version name in the **Version** column in the registered model page.\n\nThis page displays information about a specific version of a registered model and also provides a link to the source run (the version of the notebook that was run to create the model). From this page, you can also:\n- [Automatically generate a notebook to use the model for inference](/machine-learning/manage-model-lifecycle/index.md#use-model-for-inference).\n- [Delete a model](/machine-learning/manage-model-lifecycle/index.md#delete-a-model-or-model-version).\n\n![Model version](/_static/images/mlflow/model-version.png)\n\n## Example\n\nFor an example that illustrates how to use the Model Registry to build a machine learning application that forecasts the daily power output of a wind farm, see:\n\n.. toctree::\n  :maxdepth: 1\n\n  model-registry-example\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Where can I find the information about each version of a registered model?", "gold_answer": "You can find the information about each version of a registered model on the registered model page. This page displays information about the selected model and includes a table with information about each version of the model. You can access this page by clicking a model name in the registered models page."}
{"hard_neg_docs": [{"page_content": "- [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n- [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}, {"page_content": ".. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}], "question": "What libraries are included in the GPU-enabled \"<DBR> Version\"?", "gold_answer": "The GPU-enabled \"<DBR> Version\" in Databricks includes libraries that are related to NVIDIA, such as CUDA, cuDNN, and the Tesla libraries. Additionally, the NCCL library is included, as referenced by the NVIDIA End User License Agreement (EULA) terms. Specific versions of these libraries can be found in the release notes for the particular <DBR> version being used."}
{"hard_neg_docs": [{"page_content": "- [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n- [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}, {"page_content": ".. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md", "fmt_content": "Source: https://docs.databricks.com/clusters/gpu.html\nContent:\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md", "metadata": {"source": "https://docs.databricks.com/clusters/gpu.html", "seq_num": 1679, "description": "", "text": "---\ndescription: Learn about GPU-enabled <Databricks> clusters, when to use them, what they require, and how to create them.\nkeywords: single-node\n---\n<a id=\"gpu-clusters\"></a>\n\n# GPU-enabled clusters\n\n.. gcp::\n\n  .. include:: /shared/preview.md\n\n.. note::\n  Some GPU-enabled instance types are in **Beta** and are marked as such in the drop-down list when you select the driver and worker types during cluster creation.\n\n## Overview\n\n<Databricks> supports clusters accelerated with graphics processing units (GPUs).\nThis article describes how to create clusters with GPU-enabled instances and describes\nthe GPU drivers and libraries installed on those instances.\n\nTo learn more about deep learning on GPU-enabled clusters, see [_](/machine-learning/train-model/deep-learning.md).\n\n## Create a GPU cluster\n\nCreating a GPU cluster is similar to creating any Spark cluster. You should keep in mind the following:\n\n- The **Databricks Runtime Version** must be a GPU-enabled version, such as **Runtime 9.1 LTS ML (GPU, Scala 2.12, Spark 3.1.2)**.\n- The **Worker Type** and **Driver Type** must be GPU instance types.\n- For single-machine workflows without Spark, you can set the number of workers to zero.\n\n### Supported instance types\n\n.. aws::\n  <Databricks> supports the following GPU-accelerated instance types:\n\n  - [P2 instance type series](https://aws.amazon.com/ec2/instance-types/p2/): p2.xlarge, p2.8xlarge, and p2.16xlarge\n\n    - P2 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n    - The [default on-demand limit for P2 instances is one](https://aws.amazon.com/ec2/faqs/#How_many_instances_can_I_run_in_Amazon_EC2).\n\n    - P2 instances require EBS volumes for storage.\n\n  - [P3 instance type series](https://aws.amazon.com/ec2/instance-types/p3/): p3.2xlarge, p3.8xlarge, and p3.16xlarge.\n\n    - P3 instances are available only in select AWS regions. For information, see [Amazon EC2 Pricing](https://aws.amazon.com/ec2/pricing/on-demand/). Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n  - [P4d instance type series](https://aws.amazon.com/ec2/instance-types/p4/): p4d.24xlarge.\n\n    - P4d instances require <DBR> 9.1 LTS ML or above.\n\n  - [G4 instance type series](https://aws.amazon.com/ec2/instance-types/g4/), which are optimized for deploying machine learning models in production.\n\n  - [G5 instance type series](https://aws.amazon.com/ec2/instance-types/g5/), which can be used for a wide range of graphics-intensive and machine learning use cases.\n\n    - G5 instances require <DBR> 9.1 LTS ML or above.\n\n  #### Considerations\n\n  For all GPU-accelerated instance types, keep the following in mind:\n\n  - Due to Amazon spot instance price surges, GPU spot instances are difficult to retain. Use on-demand if needed.\n  - You might need to [request a limit increase](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html) in order to create\n    GPU-enabled clusters.\n\n  See [Supported Instance Types](https://databricks.com/product/aws-pricing/instance-types) for a list of supported GPU instance types and their attributes.\n\n.. azure::\n  <Databricks> supports the following instance types:\n\n  - **[NC instance type series](https://learn.microsoft.com/azure/virtual-machines/nc-series)**: Standard_NC12, Standard_NC24\n  - **[NC v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/ncv3-series)**: Standard_NC6s_v3, Standard_NC12s_v3, Standard_NC24s_v3\n  - **[NC T4 v3 instance type series](https://learn.microsoft.com/azure/virtual-machines/nct4-v3-series)**: Standard_NC4as_T4_v3, Standard_NC8as_T4_v3, Standard_NC16as_T4_v3, Standard_NC64as_T4_v3\n  - **[ND A100 v4 instance type series](https://learn.microsoft.com/azure/virtual-machines/nda100-v4-series)**: Standard_ND96asr_v4\n\n  See [Azure Databricks Pricing](https://azure.microsoft.com/pricing/details/databricks/#instances)\n  for an up-to-date list of supported GPU instance types and their availability regions.\n  Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n.. gcp::\n  <Databricks> supports the following instance types:\n\n  - **A2 machine family**: a2-highgpu-1g, a2-highgpu-2g, a2-highgpu-4g, a2-highgpu-8g, a2-megagpu-16g\n\n  See [GCP accelerator-optimized machines](https://cloud.google.com/compute/docs/accelerator-optimized-machines)\n  for more information on these instance types, and [GCP regions](https://cloud.google.com/compute/docs/regions-zones) to check where these instances are available. Your <Databricks> deployment must reside in a supported region to launch GPU-enabled clusters.\n\n<a id=\"gpu-scheduling\"></a>\n\n## GPU scheduling\n\n.. azure-aws::\n  <DBR> supports [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters.\n\n.. gcp::\n  <DBR> 9.1 LTS ML and above support [GPU-aware scheduling](https://spark.apache.org/docs/3.0.0-preview/configuration.html#custom-resource-scheduling-and-configuration-overview) from <AS> 3.0. <Databricks> preconfigures it on GPU clusters for you.\n\nGPU scheduling is not enabled on Single Node clusters.\n\n`spark.task.resource.gpu.amount` is the only Spark config related to GPU-aware scheduling that you might need to change.\nThe default configuration uses one GPU per task, which is ideal for distributed inference workloads and distributed training, if you use all GPU nodes.\nTo do distributed training on a subset of nodes, which helps reduce communication overhead during distributed training, Databricks recommends setting `spark.task.resource.gpu.amount` to the number of GPUs per worker node\nin the cluster [Spark configuration](/clusters/configure.md#spark-configuration).\n\nFor PySpark tasks, <Databricks> automatically remaps assigned GPU(s) to indices 0, 1, ....\nUnder the default configuration that uses one GPU per task, your code can simply use the default GPU without checking which GPU is assigned to the task.\nIf you set multiple GPUs per task, for example 4, your code can assume that the indices of the assigned GPUs are always 0, 1, 2, and 3. If you do need the physical indices of the assigned GPUs, you can get them from the `CUDA_VISIBLE_DEVICES` environment variable.\n\nIf you use Scala, you can get the indices of the GPUs assigned to the task from `TaskContext.resources().get(\"gpu\")`.\n\n.. azure-aws::\n  For <DBR> releases below 7.0, to avoid conflicts among multiple Spark tasks trying to use the same GPU, <Databricks> automatically configures GPU clusters so that there is at most one running task per node.\n  That way the task can use all GPUs on the node without running into conflicts with other tasks.\n\n<a id=\"nvidia\"></a>\n\n## NVIDIA GPU driver, CUDA, and cuDNN\n\n<Databricks> installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances:\n\n- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit), installed under `/usr/local/cuda`.\n- [cuDNN](https://developer.nvidia.com/cudnn): NVIDIA CUDA Deep Neural Network Library.\n- [NCCL](https://developer.nvidia.com/nccl): NVIDIA Collective Communications Library.\n\n.. azure-aws::\n  The version of the NVIDIA driver included is 470.57.02, which supports CUDA 11.0.\n\n.. gcp::\n  The version of the NVIDIA driver included is 450.119.04, which supports CUDA 11.0.\n\nFor the versions of the libraries included, see the [release notes](/release-notes/runtime/index.md#runtime-release-notes) for the specific <DBR> version you are using.\n\n.. note::\n  This software contains source code provided by NVIDIA Corporation. Specifically, to support GPUs, <Databricks> includes code from [CUDA Samples](https://docs.nvidia.com/cuda/eula/#nvidia-cuda-samples-preface).\n\n### NVIDIA End User License Agreement (EULA)\n\nWhen you select a GPU-enabled \"<DBR> Version\" in <Databricks>, you implicitly agree to the terms and conditions outlined in the\n[NVIDIA EULA](/_extras/documents/nvidia-cloud-end-user-license-agreement_clean.pdf) with respect to the CUDA, cuDNN, and Tesla libraries,\nand the [NVIDIA End User License Agreement (with NCCL Supplement)](https://docs.nvidia.com/deeplearning/sdk/nccl-sla/index.html#supplement/) for the NCCL library.\n\n.. azure-aws::\n\n  <a id=\"databricks-container-services-on-gpu\"></a>\n\n  ## Databricks Container Services on GPU clusters\n\n  .. include:: /shared/preview.md\n\n  You can use [Databricks Container Services](/clusters/custom-containers.md) on clusters with GPUs to create portable deep learning environments with customized libraries. See [_](/clusters/custom-containers.md) for instructions.\n\n  To create custom images for GPU clusters, you must select a standard runtime version instead of <DBR> ML for GPU. When you select **Use your own Docker container**, you can choose GPU clusters with a standard runtime version. The custom images for GPU clusters are based on the [official CUDA containers](https://hub.docker.com/r/nvidia/cuda/), which is different from <DBR> ML for GPU.\n\n  When you create custom images for GPU clusters, you cannot change the NVIDIA driver version, because it must match the driver version on the host machine.\n\n  The `databricksruntime` [Docker Hub](https://hub.docker.com/u/databricksruntime) contains example base images with GPU capability. The Dockerfiles used to generate these images are located in the [example containers GitHub repository](https://github.com/databricks/containers/tree/master/ubuntu/gpu), which also has details on what the example images provide and how to customize them.\n\n.. aws::\n\n  ## Error messages\n\n  - The following error indicates that the AWS cloud provider does not have enough capacity for the requested compute resource.\n    `Error: Cluster terminated. Reason: AWS Insufficient Instance Capacity Failure`\n\n    To resolve, you can try to create a cluster in a different availability zone. The availability zone is in the [cluster configuration](/clusters/configure.md#cluster-aws-config), under **Advanced options**. You can also review [AWS reserved instances pricing](https://aws.amazon.com/ec2/pricing/reserved-instances/pricing/) to purchase additional quota.\n\n  - If your cluster uses P4d or G5 instance types and <DBR> 7.3 LTS ML, the CUDA package version in 7.3 is incompatible with newer GPU instances", "timestamp": 0, "next": ""}}], "question": "What version of the NVIDIA driver is included in the GPU-enabled \"<DBR> Version\"?", "gold_answer": "The version of the NVIDIA driver included in the GPU-enabled \"<DBR> Version\" is 450.119.04, which supports CUDA 11.0."}
{"hard_neg_docs": [{"page_content": "The example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\n---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}], "question": "Can the parameters, resources, and outputs from the example ARM template be merged into an existing template?", "gold_answer": "Yes, the parameters, resources, and outputs from the example ARM template can be merged into an existing template. The documentation suggests that if you already use an ARM (Azure Resource Manager) template, you can merge these elements from the example template provided into your existing one."}
{"hard_neg_docs": [{"page_content": "The example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\n---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"", "fmt_content": "Source: https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html\nContent:\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"", "metadata": {"source": "https://docs.databricks.com/security/keys/cmk-managed-disks-azure.html", "seq_num": 511, "description": "", "text": "---\ndescription: Learn how to configure your own keys (customer-managed keys) for Azure managed disk encryption.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure customer-managed keys for Azure managed disks\n\n<Databricks> compute workloads in the data plane store temporary data on Azure managed disks. By default, data stored on managed disks is encrypted at rest using server-side encryption with Microsoft-managed keys. This article describes how to configure a customer-managed key for your <Databricks> workspace to use for managed disk encryption.\n\n.. important:: Customer-managed keys for managed disk storage apply to data disks, but do _not_ apply to operating system (OS) disks.\n\n  Customer-managed keys for managed disk storage do _not_ apply to serverless compute resources such as [serverless SQL warehouses](/sql/admin/serverless.md) and [Model Serving](/machine-learning/model-serving/index.md). The disks used for serverless compute resources are short-lived and tied to the lifecycle of the serverless workload. When compute resources are stopped or scaled down, the VMs and their storage are destroyed.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- Your <Databricks> workspace must be on the [Premium plan](https://www.databricks.com/product/azure-pricing).\n\n- Your Azure Key Vault must be on the [Premium tier](https://azure.microsoft.com/pricing/details/key-vault/) and have [purge protection](https://learn.microsoft.com/azure/key-vault/general/soft-delete-overview#purge-protection) enabled.\n\n- Your Azure Key Vault and <Databricks> workspace must be in the same region and in the same Azure Active Directory (AAD) tenant. They can be in different subscriptions.\n\n- If you want to enable auto-rotation, only software and HSM RSA keys of sizes 2048-bit, 3072-bit, and 4096-bit are supported.\n\n- This feature is unsupported for workspaces with [FedRAMP](https://www.gsa.gov/technology/government-it-initiatives/fedramp) compliance. Contact your Databricks representative for any questions about this. <!-- I've pinged Filippo and PaulK to confirm -->\n\n<a id=\"key\"></a>\n\n## Step 1: Create or retrieve a key\n\nThis section describes how to create a key using the Azure CLI or retrieve an existing key stored in Azure Key Vault.\n\nFor more ways to create a key, see the [Azure keys documentation](https://learn.microsoft.com/azure/key-vault/keys/).\n\n<a id=\"create-key\"></a>\n\n### Create a key\n\nTo create a key using the Azure CLI, do the following:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key create --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following values from the output:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n<a id=\"existing-key\"></a>\n\n### Retrieve a key from Azure Key Vault\n\nTo retrieve a key from Azure Key Vault:\n\n#. Run the following command:\n\n  ```bash\n  az keyvault key show --name <key name> --vault-name <key-vault-name>\n  ```\n\n#. Make note of the following details for your existing key:\n\n  - Key vault name: The name of your Key Vault\n\n  - Key name: The name of your key\n\n  - Key version: The version of your key\n\n  - Key vault resource group: The resource group of your Key Vault\n\n  The full key ID has the form `https://<key-vault-name>.vault.azure.net/keys/<key-name>/<key-version>`.\n\n#. Confirm that your existing key is enabled before proceeding by running `az keyvault key show --name <key name>` again. The output displays `\"enabled\": true`.\n\n<a id=\"create-or-update-workspace\"></a>\n\n## Step 2: Create or update a workspace\n\nTo create or update a workspace with a customer-managed key for managed disks, choose one of the following deployment strategies:\n\n- [_](#create-with-ui)\n\n- [_](#create-with-template)\n\n\n<a id=\"create-with-ui\"></a>\n\n## Step 2a: Use the Azure portal without using a template\n\nThis section describes how to use UI blades in the Azure portal to create or update a workspace with customer-managed keys for managed disks without using a template. \n\n#. Start to create or update a workspace:\n\n   **To create a new workspace:**\n   \n   a. Go to the [Azure Portal homepage](https://azure.microsoft.com/get-started/azure-portal) and click **Create a resource** in the top-left corner of the page.\n\n   #. Within the search bar, type `Azure Databricks` and click **Azure Databricks**.\n\n   #. Select **Create** from within the <Databricks> widget.\n\n   #. Enter values in the form fields on the tabs **Basics** and **Networking**.\n\n   #. In the **Encryption** tab, select the **Use your own key** checkbox in the **Managed Disks** section.\n\n   **To update an existing workspace:**\n\n   a. Go to the [Azure portal's home page for Azure Databricks](https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.Databricks%2Fworkspaces)\n\n   #. Navigate to your existing <Databricks> workspace.\n\n   #. Open the **Encryption** tab from the left-side panel.\n\n   #. Under the **Customer-managed keys** section, enable **Managed Disks**.\n\n#. Set the encryption fields.\n\n   ![Show fields in the Managed Disks section of the Azure Databricks blade](/_static/images/security/keys/azure-cmk-managed-disks-ui.png)\n\n   - In the **Key Identifier** field, paste the Key Identifier of your Azure Key Vault key.\n   \n   - In the **Subscription** dropdown, enter the subscription name of your Azure Key Vault key.\n   \n   - To enable auto-rotation of your key, enable **Enable Auto Rotation of Key**.\n\n#. Complete the remaining tabs and click **Review + Create** (for new workspace) or **Save** (for updating a workspace).\n\n#. After your workspace deploys, navigate to your new <Databricks> workspace. \n\n#. From the **Overview** tab of your Azure Databricks workspace, click **Managed Resource Group**.\n\n#. In the **Overview** tab of the managed resource group, look for the object of type **Disk Encryption Set** that was created in this resource group. Copy the name of the Disk Encryption Set.\n\n#. In Azure portal, go to the Azure Key Vault that was used to configure the key that you are using for this feature.\n\n#. Open the **Access policies** tab from the left-side panel. Once the tab is open, click **Create** at the top of the page.\n\n#. In the Permissions tab under the **Key permissions** section, enable **Get**, **Unwrap Key**, and **Wrap key**. \n\n#. Click **Next**.\n\n#. On the **Principal** tab, enter the name of the Disk Encryption Set within the managed resource group of your <Databricks> workspace in the search bar. Select the result and click **Next**. \n\n   ![Enter the name of disk encryption set and select the result](/_static/images/security/keys/azure-cmk-access-policy-ui.png)\n\n#. Click the **Review + create** tab and click **Create**.\n\n<a id=\"create-with-template\"></a>\n\n## Step 2b: Use an ARM template \n\nYou can explore [Azure quickstart templates](https://azure.microsoft.com/resources/templates/) in the Azure documentation. For a list of ARM template deployment options, see the [ARM template documentation](https://learn.microsoft.com/azure/azure-resource-manager/templates/).\n\nWhen you create a workspace, a disk encryption set resource is also created within the managed resource group of your workspace. It has a system-assigned managed identity that is used to access your Key Vault. Before <Databricks> compute can use this key to encrypt your data, you must retrieve the principal ID of the disk encryption set, then grant the identity the `GET`, `WRAP`, and `UNWRAP` key permissions to your Key Vault.\n\nDatabricks recommends that you create or update the workspace and grant Key Vault permissions in the same template deployment. You must create or update the workspace before you grant Key Vault permissions, with one exception. If you update an existing managed disk customer-managed key workspace to use a new key in a new Key Vault, you must grant the existing disk encryption set permission to access the new Key Vault, and then update the workspace with the new key configuration.\n\nThe example template in this section does both of the following:\n\n- Creates or updates a workspace to add managed disk customer-managed key settings\n- Grants the disk encryption set access to your Key Vault\n\n.. important:: Updating a workspace to add a customer-managed key for managed disks restarts all running compute resources (clusters, pools). Plan for downtime. Rotating your key for a workspace that already uses this feature doesn't restart compute resources.\n\nYou can use the following example ARM template, which does two things:\n\n- Create or update a workspace with a managed disk customer-managed key.\n\n- Create a key access policy. \n\nIf you already use an ARM template, you can merge the parameters, resources, and outputs from the example template into your existing template.\n\n```json\n{\n   \"$schema\": \"https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#\",\n   \"contentVersion\": \"1.0.0.0\",\n   \"parameters\": {\n       \"workspaceName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The name of the Azure Databricks workspace to create\"\n           }\n       },\n       \"pricingTier\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"premium\",\n           \"allowedValues\": [\n               \"premium\"\n           ],\n           \"metadata\": {\n               \"description\": \"The pricing tier of workspace\"\n           }\n       },\n       \"apiVersion\": {\n           \"type\": \"string\",\n           \"defaultValue\": \"2023-02-01\",\n           \"allowedValues\": [\n               \"2023-02-01\",\n               \"2022-04-01-preview\"\n           ],\n           \"metadata\": {\n               \"description\": \"The API version to use to create the workspace resources\"\n           }\n       },\n       \"keyVaultName\": {\n           \"type\": \"string\",\n           \"metadata\": {\n               \"description\": \"The Key Vault name used for CMK\"\n   ", "timestamp": 0, "next": ""}}], "question": "What is the purpose of creating a key access policy in Azure Databricks?", "gold_answer": "The purpose of creating a key access policy in Azure Databricks is not explicitly mentioned in the provided document. However, based on general practices, the purpose of creating a key access policy in Azure Databricks would typically be to define and control access to encryption keys stored in an Azure Key Vault. These keys can be used for Customer-Managed Key (CMK) features, such as encrypting the managed disks used by Azure Databricks workspaces. The key access policy would determine which principals (users, groups, applications, etc.) have permissions to perform operations such as encrypt, decrypt, wrapKey, unwrapKey, and others on the keys. This is a critical part of securing data and managing encryption in the cloud environment."}
{"hard_neg_docs": [{"page_content": "- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "You must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\n---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the purpose of development mode in DLT pipelines?", "gold_answer": "The purpose of development mode in DLT (Delta Live Tables) pipelines is to facilitate the development and testing of pipelines by providing a more convenient and efficient workflow. Development mode offers the following features:\n\n- It does not immediately terminate compute resources after a pipeline update succeeds or fails, allowing developers to reuse the same compute resources for multiple updates without the need to wait for a cluster to start.\n- It does not automatically retry on task failure, which helps developers to immediately detect and fix logical or syntactic errors in the pipeline.\n\nDatabricks recommends using development mode during the development and testing phases. It is advised to switch to production mode when deploying to a production environment to ensure optimal performance and resource management."}
{"hard_neg_docs": [{"page_content": "- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "You must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/testing.html\nContent:\n---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/testing.html", "seq_num": 24, "description": "", "text": "---\ndescription: This article describes patterns you can use to develop and test <DLT> pipelines.\n---\n\n# How to develop and test <DLT> pipelines\n\nThis article describes patterns you can use to develop and test <DLT> pipelines. Through the pipeline settings, <DLT> allows you to specify configurations to isolate pipelines in developing, testing, and production environments. The recommendations in this article are applicable for both SQL and Python code development.\n\n## Use development mode to run pipeline updates\n\n<DLT> provides a UI toggle to control whether your pipeline updates run in development or production mode. This mode controls how pipeline updates are processed, including:\n\n- Development mode does not immediately terminate compute resources after an update succeeds or fails. You can reuse the same compute resources to run multiple updates of the pipeline without waiting for a cluster to start.\n- Development mode does not automatically retry on task failure, allowing you to immediately detect and fix logical or syntactic errors in your pipeline.\n\nDatabricks recommends using development mode during development and testing and always switching to production mode when deploying to a production environment.\n\nSee [_](/delta-live-tables/updates.md#optimize-execution).\n\n## Specify a target schema during all development lifecycle phases\n\nAll datasets in a <DLT> pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. If a target schema is specified, the `LIVE` virtual schema points to the target schema. To review the results written out to each table during an update, you must specify a target schema.\n\nYou must specify a target schema that is unique to your environment. Each table in a given schema can only be updated by a single pipeline.\n\nBy creating separate pipelines for development, testing, and production with different targets, you can keep these environments isolated. Using the target schema parameter allows you to remove logic that uses string interpolation or other widgets or parameters to control data sources and targets.\n\nSee [_](/delta-live-tables/publish.md).\n\n## Use Databricks Repos to manage <DLT> pipelines\n\nDatabricks recommends using Repos during <DLT> pipeline development, testing, and deployment to production. Repos enables the following:\n\n- Keeping track of how code is changing over time.\n- Merging changes that are being made by multiple developers.\n- Software development practices such as code reviews.\n\nDatabricks recommends configuring a single Git repository for all code related to a pipeline.\n\nEach developer should have their own Databricks Repo configured for development. During development, the user configures their own pipeline from their Databricks Repo and tests new logic using development datasets and isolated schema and locations. As development work is completed, the user commits and pushes changes back to their branch in the central Git repository and opens a pull request against the testing or QA branch.\n\nThe resulting branch should be checked out in a Databricks Repo and a pipeline configured using test datasets and a development schema. Assuming logic runs as expected, a pull request or release branch should be prepared to push the changes to production.\n\nWhile Repos can be used to synchronize code across environments, pipeline settings need to be kept up to date either manually or using tools like Terraform.\n\nThis workflow is similar to using Repos for CI/CD in all Databricks jobs. See [_](/repos/ci-cd-techniques-with-repos.md).\n\n## Segment libraries for ingestion and transformation steps\n\nDatabricks recommends isolating queries that ingest data from transformation logic that enriches and validates data. You can then organize libraries used for ingesting data from development or testing data sources in a separate directory from production data ingestion logic, allowing you to easily configure pipelines for various environments. You can then use smaller datasets for testing, accelerating development. See [_](#sample-data).\n\nYou can also use parameters to control data sources for development, testing, and production. See [_](#parameters).\n\nBecause <DLT> pipelines use the `LIVE` virtual schema for managing all dataset relationships, by configuring development and testing pipelines with ingestion libraries that load sample data, you can substitute sample datasets using production table names to test code. The same transformation logic can be used in all environments.\n\n<a id=\"sample-data\"></a>\n\n## Create sample datasets for development and testing\n\nDatabricks recommends creating development and test datasets to test pipeline logic with both expected data and potential malformed or corrupt records. There are multiple ways to create datasets that can be useful for development and testing, including the following:\n\n- Select a subset of data from a production dataset.\n- Use anonymized or artificially generated data for sources containing PII.\n- Create test data with well-defined outcomes based on downstream transformation logic.\n- Anticipate potential data corruption, malformed records, and upstream data changes by creating records that break data schema expectations.\n\nFor example, if you have a notebook that defines a dataset using the following code:\n\n```sql\nCREATE OR REFRESH STREAMING LIVE TABLE input_data AS SELECT * FROM cloud_files(\"/production/data\", \"json\")\n```\n\nYou could create a sample dataset containing specific records using a query like the following:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS\nSELECT \"2021/09/04\" AS date, 22.4 as sensor_reading UNION ALL\nSELECT \"2021/09/05\" AS date, 21.5 as sensor_reading\n```\n\nThe following example demonstrates filtering published data to create a subset of the production data for development or testing:\n\n```sql\nCREATE OR REFRESH LIVE TABLE input_data AS SELECT * FROM prod.input_data WHERE date > current_date() - INTERVAL 1 DAY\n```\n\nTo use these different datasets, create multiple pipelines with the notebooks implementing the transformation logic. Each pipeline can read data from the `LIVE.input_data` dataset but is configured to include the notebook that creates the dataset specific to the environment.\n\n<a id=\"parameters\"></a>\n\n## Control data sources with parameters\n\nYou can reference parameters set during pipeline configuration from within your libraries. These parameters are set as key-value pairs in the **Compute > Advanced > Configurations** portion of the pipeline settings UI. This pattern allows you to specify different data sources in different configurations of the same pipeline.\n\nFor example, you can specify different paths in development, testing, and production configurations for a pipeline using the variable `data_source_path` and then reference it using the following code:\n\n.. code-language-tabs::\n\n    ```sql\n    CREATE STREAMING LIVE TABLE bronze\n    AS (\n        SELECT\n        *,\n        input_file_name() AS source_file_name\n        FROM cloud_files( '${data_source_path}', 'csv',\n                map(\"header\", \"true\"))\n    )\n    ```\n\n    ```python\n    import dlt\n    from pyspark.sql.functions import input_file_name\n\n    data_source_path = spark.conf.get(\"data_source_path\")\n\n    @dlt.table\n    def bronze():\n        return (spark.readStream\n            .format(\"cloudFiles\")\n            .option(\"cloudFiles.format\", \"csv\")\n            .option(\"header\", True)\n            .load(data_source_path )\n            .select(\"*\", input_file_name().alias(\"source_file_name\"))\n        )\n    ```\n\nThis pattern is especially useful if you need to test how ingestion logic might handle changes to schema or malformed data during initial ingestion. You can use the identical code throughout your entire pipeline in all environments while switching out datasets.\n\n<!-- commenting out for 1st pass; note that with UC, storage location will be generally tied to UC catalog/schema locations\n\n## Specify a storage location\n\nEspecially for users new to <DLT>, specifying a storage location for a new pipeline provides an important learning opportunity for understanding how data and metadata are persisted. When specifying a storage location for development or testing purposes, you should always choose a location that is fully isolated from production data.\n\nSpecifying a storage location during development also provides an opportunity to learn to configure credentials to connect to cloud object storage. While you might not have the proper cloud access permissions or admin credentials in <Databricks> to configure access to cloud object storage, learning the process your organization uses to manage access to these locations is crucial to avoiding delays in getting pipelines into production. Databricks recommends contacting a workspace adminstrator to learn more about how your team has chosen to configure data access.\n\nFor ephemeral tests that simply confirm that code executes, you can choose to use the default storage location. <DLT> assigns a location with a hashed directory name to ensure that no conflicts occur between pipelines. You can still access data at this location similarly to how you would access data in a named storage location, but you must capture the generated storage location from the pipeline to locate this directory.\n\nYou can explore the storage location to query metadata such as the Delta transaction log or <DLT> event log. In a long-living development environment, you can also use this directory to track how <DLT> manages data and metadata in this location through automated maintenance tasks and retention settings. -->\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I review the results written out to each table during a pipeline update in DLT?", "gold_answer": "To review the results written out to each table during a pipeline update in Delta Live Tables (DLT), you must specify a target schema. All datasets in a DLT pipeline reference the `LIVE` virtual schema, which is not accessible outside the pipeline. By specifying a target schema, the `LIVE` virtual schema will point to that target schema, allowing you to review the results."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}, {"page_content": "![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}], "question": "What are some workloads that are not compatible with autoscaling clusters?", "gold_answer": "Some workloads that are not compatible with autoscaling clusters include spark-submit jobs and some Python packages."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}, {"page_content": "![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.", "fmt_content": "Source: https://docs.databricks.com/clusters/cluster-config-best-practices.html\nContent:\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.", "metadata": {"source": "https://docs.databricks.com/clusters/cluster-config-best-practices.html", "seq_num": 1677, "description": "", "text": "---\ndescription: Learn best practices when creating and configuring <Databricks> clusters.\nkeywords: single-node\n---\n\n<a id=\"cluster-config-best-practices\"></a>\n\n# Best practices: Cluster configuration\n\n<Databricks> provides a number of options when you create and configure clusters to help you get the best performance at the lowest cost. This flexibility, however, can create challenges when you’re trying to determine optimal configurations for your workloads. Carefully considering how users will utilize clusters will help guide configuration options when you create new clusters or configure existing clusters. Some of the things to consider when determining configuration options are:\n\n- What type of user will be using the cluster? A data scientist may be running different job types with different requirements than a data engineer or data analyst.\n- What types of workloads will users run on the cluster? For example, batch extract, transform, and load (ETL) jobs will likely have different requirements than analytical workloads.\n- What level of service level agreement (SLA) do you need to meet?\n- What budget constraints do you have?\n\nThis article provides cluster configuration recommendations for different scenarios based on these considerations. This article also discusses specific features of <Databricks> clusters and the considerations to keep in mind for those features.\n\nYour configuration decisions will require a tradeoff between cost and performance. The primary cost of a cluster includes the Databricks Units (DBUs) consumed by the cluster and the cost of the underlying resources needed to run the cluster. What may not be obvious are the secondary costs such as the cost to your business of not meeting an SLA, decreased employee efficiency, or possible waste of resources because of poor controls.\n\n<a id=\"cluster-features-overview\"></a>\n\n## Cluster features\n\nBefore discussing more detailed cluster configuration scenarios, it's important to understand some features of <Databricks> clusters and how best to use those features.\n\n<a id=\"cluster-types\"></a>\n\n### All-purpose clusters and job clusters\n\nWhen you [create a cluster](/clusters/configure.md) you select a cluster type: an all-purpose cluster or a job cluster. All-purpose clusters can be shared by multiple users and are best for performing ad-hoc analysis, data exploration, or development. Once you've completed implementing your processing and are ready to operationalize your code, switch to running it on a job cluster. Job clusters terminate when your job ends, reducing resource usage and cost.\n\n\n<a id=\"cluster-modes\"></a>\n\n### Cluster mode\n\n.. azure-aws::\n\n  .. include:: /shared/clusters-preview-ui-terminology.md\n\n<Databricks> supports three [cluster modes](/clusters/configure.md#access-mode): Standard, High Concurrency, and Single Node. Most regular users use Standard or Single Node clusters.\n\n.. include:: /shared/clusters-noisolationshared-security-warning.md\n\n- Standard clusters are recommended for single users only. Standard clusters can run workloads developed in Python, SQL, R, and Scala.\n\n- Single Node clusters are intended for jobs that use small amounts of data or non-distributed workloads such as single-node machine learning libraries.\n\n- High Concurrency clusters are ideal for groups of users who need to share resources or run ad-hoc jobs. Administrators usually create High Concurrency clusters. Databricks recommends enabling autoscaling for High Concurrency clusters.\n\n.. aws::\n\n  <a id=\"instance-types-aws\"></a>\n\n  ### On-demand and spot instances\n\n  Amazon Web Services has two tiers of EC2 instances: on-demand and spot. For on-demand instances, you pay for compute capacity by the second with no long-term commitments. Spot instances allow you to use spare Amazon EC2 computing capacity and choose the maximum price you are willing to pay. Spot pricing changes in real-time based on the supply and demand on AWS compute capacity. If the current spot market price is above the max spot price, the spot instances are terminated. Since spot instances are often available at a discount compared to on-demand pricing you can significantly reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n  <Databricks> supports creating clusters using a combination of on-demand and spot instances with a custom spot price, allowing you to tailor your cluster according to your use cases. For example, this image illustrates a  configuration that specifies that the driver node and four worker nodes should be launched as on-demand instances and the remaining four workers should be launched as spot instances where the maximum spot price is 100% of the on-demand price.\n\n  ![Configure on-demand and spot instances](/_static/images/admin-cluster-management/demand-spot-composition.png)\n  ![Max spot price](/_static/images/admin-cluster-management/spot-price.png)\n\n\n  Databricks recommends launching the cluster so that the Spark driver is on an on-demand instance, which allows saving the state of the cluster even after losing spot instance nodes. If you choose to use all spot instances including the driver, any cached data or tables are deleted if you lose the driver instance due to changes in the spot market.\n\n  Another important setting is **Spot fall back to On-demand**. If you are running a hybrid cluster (that is, a mix of on-demand and spot instances), and if spot instance acquisition fails or you lose the spot instances, <Databricks> falls back to using on-demand instances and provides you with the desired capacity. Without this option you will lose the capacity supplied by the spot instances for the cluster, causing delay or failure of your workload. Databricks recommends setting the mix of on-demand and spot instances in your cluster based on the criticality of jobs, tolerance to delays and failures due to loss of instances, and cost sensitivity for each type of use case.\n\n  .. tip::\n\n    You can use the [Amazon Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/) to determine a suitable price for your instance type and region.\n\n.. azure::\n\n  <a id=\"instance-types-azure\"></a>\n\n  ### On-demand and spot instances\n\n  To save cost, <Databricks> supports creating clusters using a combination of on-demand and [spot instances](/clusters/configure.md#spot-instances). You can use spot instances to take advantage of unused capacity on Azure to reduce the cost of running your applications, grow your application's compute capacity, and increase throughput.\n\n<a id=\"cluster-autoscaling\"></a>\n\n### Autoscaling\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n[Autoscaling](/clusters/configure.md#cluster-size-and-autoscaling) allows clusters to resize automatically based on workloads. Autoscaling can benefit many use cases and scenarios from both a cost and performance perspective, but it can be challenging to understand when and how to use autoscaling. The following are some considerations for determining whether to use autoscaling and how to get the most benefit:\n\n- Autoscaling typically reduces costs compared to a fixed-size cluster.\n- Autoscaling workloads can run faster compared to an under-provisioned fixed-size cluster.\n- Some workloads are not compatible with autoscaling clusters, including spark-submit jobs and some Python packages.\n- With single-user all-purpose clusters, users may find autoscaling is slowing down their development or analysis when the minimum number of workers is set too low. This is because the commands or queries they're running are often several minutes apart, time in which the cluster is idle and may scale down to save on costs. When the next command is executed, the cluster manager will attempt to scale up, taking a few minutes while retrieving instances from the cloud provider. During this time, jobs might run with insufficient resources, slowing the time to retrieve results. While increasing the minimum number of workers helps, it also increases cost. This is another example where cost and performance need to be balanced.\n- If [Delta Caching](/optimizations/disk-cache.md) is being used, it's important to remember that any cached data on a node is lost if that node is terminated. If retaining cached data is important for your workload, consider using a fixed-size cluster.\n- If you have a job cluster running an ETL workload, you can sometimes size your cluster appropriately when tuning if you know your job is unlikely to change. However, autoscaling gives you flexibility if your data sizes increase. It's also worth noting that optimized autoscaling can reduce expense with long-running jobs if there are long periods when the cluster is underutilized or waiting on results from another process. Once again, though, your job may experience minor delays as the cluster attempts to scale up appropriately. If you have tight SLAs for a job, a fixed-sized cluster may be a better choice or consider using <a Databricks> [pool](#pools) to reduce cluster start times.\n\n.. aws::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n.. azure::\n\n  <Databricks> also supports [autoscaling local storage](/clusters/configure.md#autoscaling-local-storage-azure). With autoscaling local storage, <Databricks> monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run low on disk, <Databricks> automatically attaches a new managed volume to the worker before it runs out of disk space.\n\n<a id=\"instance-pools\"></a>\n\n### Pools\n\n[_](/clusters/pools.md) reduce cluster start and scale-up times by maintaining a set of available, ready-to-use instances. Databricks recommends taking advantage of pools to improve processing time while minimizing cost.\n\n<a id=\"dbr-versions\"></a>\n\n### <DBR> versions\n\nDatabricks recommends using the latest <DBR> version for all-purpose clusters. Using the most current version will ensure you have the latest optimizations and most up-to-date compatibility between your code and preloaded packages.\n\nFor job clusters running operational workloads, consider using the Long Term Support (LTS) <DBR> version. Using the LTS version will ensure you don't run into compatibility issues and can thoroughly test your workload before upgrading. If you have an advanced use case around machine learning, consider the specialized <DBR> version.\n\n<a id=\"policies\"></a>\n\n### Cluster policies\n\n<Databricks> [cluster policies](/administration-guide/clusters/policies.md) allow administrators to enforce controls over the creation and configuration of clusters. Databricks recommends using cluster policies to help apply the recommendations discussed in this guide. Learn more about cluster policies in the [cluster policies best practices guide](/administration-guide/clusters/policies-best-practices.md).\n\n<a id=\"auto-termination\"></a>\n\n### Automatic termination\n\nMany users won't think to terminate their clusters when they're finished using them. Fortunately, clusters are automatically terminated after a set period, with a default of 120 minutes.\n\nAdministrators can change this default setting when creating cluster policies. Decreasing this setting can lower cost by reducing the time that clusters are idle. It's important to remember that when a cluster is terminated all state is lost, including all variables, temp tables, caches, functions, objects, and so forth. All of this state will need to be restored when the cluster starts again. If a developer steps out for a 30-minute lunch break, it would be wasteful to spend that same amount of time to get a notebook back to the same state as before.\n\n.. important::\n\n  Idle clusters continue to accumulate DBU and cloud instance charges during the inactivity period before termination.\n\n<a id=\"garbage-collection-considerations\"></a>\n\n### Garbage collection\n\nWhile it may be less obvious than other considerations discussed in this article, paying attention to garbage collection can help optimize job performance on your clusters. Providing a large amount of RAM can help jobs perform more efficiently but can also lead to delays during garbage collection.\n\nTo minimize the impact of long garbage collection sweeps, avoid deploying clusters with large amounts of RAM configured for each instance. Having more RAM allocated to the executor will lead to longer garbage collection times. Instead, configure instances with smaller RAM sizes, and deploy more instances if you need more memory for your jobs. However, there are cases where fewer nodes with more RAM are recommended, for example, workloads that require a lot of shuffles, as discussed in [Cluster sizing considerations](#cluster-sizing-considerations).\n\n<a id=\"cluster-permissions\"></a>\n\n### Cluster access control\n\nYou can configure two types of cluster permissions:\n\n- The **Allow Cluster Creation** permission controls the ability of users to create clusters.\n- Cluster-level permissions control the ability to use and modify a specific cluster.\n\nTo learn more about configuring cluster permissions, see [cluster access control](/security/auth-authz/access-control/cluster-acl.md).\n\nYou can create a cluster if you", "timestamp": 0, "next": ""}}], "question": "How does autoscaling affect the performance of single-user all-purpose clusters?", "gold_answer": "Autoscaling in single-user all-purpose clusters can slow down development or analysis if the minimum number of workers is set too low. This occurs because commands or queries run by the user may be several minutes apart, which can lead to the cluster idling and scaling down to save costs. When the user runs the next command, the cluster needs to scale up, which takes time to acquire new instances from the cloud provider. During this period, jobs might run with insufficient resources, which slows down the time to retrieve results. While increasing the minimum number of workers can help mitigate this issue, it also leads to higher costs. Therefore, there's a need to balance cost and performance with autoscaling in single-user all-purpose clusters."}
{"hard_neg_docs": [{"page_content": "#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}, {"page_content": "a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\na. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\n---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}], "question": "Can a cluster be connected to Qlik Sense using <PC>?", "gold_answer": "No, a cluster cannot be connected to Qlik Sense using <PC>. The document states that <PC> only supports SQL warehouses for Qlik Sense, and to connect a cluster to Qlik Sense, you must connect to Qlik Sense manually."}
{"hard_neg_docs": [{"page_content": "#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}, {"page_content": "a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\na. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).", "fmt_content": "Source: https://docs.databricks.com/partners/bi/qlik-sense.html\nContent:\n---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).", "metadata": {"source": "https://docs.databricks.com/partners/bi/qlik-sense.html", "seq_num": 1744, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to Qlik Sense so you can make data-driven decisions and take action.\n---\n\n<!-- This article has been tested with the Qlik Sense Enterprise software as a service (SaaS) website instance at\n     https://qzytbaf0c3d9sad.eu.qlikcloud.com\n     To get access, contact Prasad Kona (prasad.kona@databricks.com) -->\n\n# Connect to Qlik Sense\n\nQlik Sense delivers best-in-class cloud analytics that help people of all skill levels to make data-driven decisions and take action.\n\nThis article describes how to use Qlik Sense with <a Databricks> cluster or a <DBSQL> warehouse (formerly <DBSQL> endpoint) to analyze data in <Delta>.\n\n.. azure-aws::\n\n  .. note:: For information about Qlik Replicate, a solution that helps you pull data from multiple data sources (Oracle, Microsoft SQL Server, SAP, mainframe, and more) into <Delta>, see [_](/partners/ingestion/qlik.md).\n\n<a id=\"partner-connect\"></a>\n\n## Connect to Qlik Sense using <PC>\n\n.. note:: <PC> only supports SQL warehouses for Qlik Sense. To connect a cluster to Qlik Sense, connect to Qlik Sense manually.\n\nTo connect to Qlik Sense using <PC>, do the following:\n\n#. [_](/partner-connect/bi.md).\n\n#. On the **Qlik Sense Databricks Connect** page, click the **Sign up here** link.\n\n#. Follow the on-screen instructions to create a Qlik account and start your free Qlik Sense trial, then return to the **Qlik Sense Databricks Connect** page.\n\n#. Enter your Qlik Sense tenant URL.\n\n#. Click the **Click here** link.\n\n   A new tab opens in your browser that displays the Qlik Sense API Access Key Help page.\n\n#. Follow the instructions to generate an API key for your Qlik Sense tenant, then return to the **Qlik Sense Databricks Connect**.\n\n#. Enter your Qlik Sense Tenant API key, then click **Submit**.\n\n   The Qlik Management Console displays.\n\n<a id=\"manual-connection\"></a>\n\n## Connect to Qlik Sense manually\n\nThis section describes how to connect to Qlik Sense manually.\n\n<a id=\"manual-requirements\"></a>\n\n### Requirements\n\nBefore you connect to Qlik Sense manually, you must have the following:\n\n.. include:: /shared/manual-partner-requirements.md\n\n<a id=\"manual-steps\"></a>\n\n### Steps to connect\n\nTo connect to Qlik Sense manually, do the following:\n\n#. Sign in to the Qlik Sense app or website for your organization.\n#. Do one of the following:\n\n   - If you have an existing app that you want to use, click the app's tile on the home page to open it.\n   - If you do not have an existing app, click **Add new > New analytics app**, and follow the on-screen directions to finish creating the app and to open it.\n\n#. With the app open, click **Prepare > Data manager**.\n#. Click **Add data > Files and other sources**.\n#. For **Connect to a new data source**, click **Databricks**.\n#. In the **Create new connection (Databricks)** dialog, enter the following information:\n\n   a. For **Host name**, enter the **Server Hostname** value.\n   #. For **Port**, enter the **Port** value.\n   #. For **Database name**, enter the name of the database that you want to use.\n   #. For **HTTP Path**, enter the **HTTP Path** value.\n   #. In **Credentials**, for **User name**, enter the word `token`.\n   #. For **Password**, enter the token.\n   #. For **SSL Options**, select the boxes **Enable SSL**, **Allow Self-signed Server Certificate**, **Allow Common Name Host Name Mismatch**, and **Use System Trust Store**.\n   #. For **Name**, enter a name for this connection, or leave the default name.\n   #. You can leave the rest of the settings in this dialog with their default settings.\n\n#. Click **Test connection**.\n#. After the connection succeeds, click **Create**.\n#. Follow the on-screen directions to add tables to your connection and to filter the tables' data.\n#. Click **Next**.\n#. Follow the on-screen directions to analyze your data with [sheets](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Sheets/create-sheets-for-structure.htm), [visualizations](https://help.qlik.com/en-us/cloud-services/Subsystems/Hub/Content/Sense_Hub/Visualizations/visualizations.htm), and other [data analytics and visualization resources](https://help.qlik.com/en-US/cloud-services/Subsystems/Hub/Content/Sense_Hub/Introduction/creating-analytics-and-visualizing-data.htm).\n\n<a id=\"next-steps\"></a>\n\n## Next steps\n\nTo continue using Qlik Sense, see the following resources:\n\n- [Qlik Sense](https://www.qlik.com/products/qlik-sense)\n- [Get started with Qlik Sense](https://help.qlik.com/onboarding)\n- [Qlik Sense demos](https://demos.qlik.com/qliksense)\n- [Qlik help videos](https://www.youtube.com/channel/UCFxZPr8pHfZS0n3jxx74rpA)\n- [Qlik Sense on Windows help](https://help.qlik.com/sense)\n- [Qlik Sense help for administrators](https://help.qlik.com/sense-admin)\n- [Qlik Sense for developers help](https://help.qlik.com/sense-developer)\n- [Qlik support services and resources](https://www.qlik.com/services/support)\n- [Contact Qlik](https://www.qlik.com/contact)\n- [Support](https://support.qlik.com/)\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md", "timestamp": 0, "next": ""}}], "question": "What is Qlik Sense and how can it be used with Databricks?", "gold_answer": "Qlik Sense is a cloud analytics platform that helps people of all skill levels make data-driven decisions and take action. It can be used with a Databricks workspace by connecting to a Databricks cluster or a DBSQL warehouse (formerly known as a DBSQL endpoint) to analyze data in Delta. This connection allows users to leverage the analytics and visualization capabilities of Qlik Sense to work with the data stored in Databricks. The article mentioned provides instructions on how to connect Qlik Sense with a Databricks cluster or warehouse for data analysis."}
{"hard_neg_docs": [{"page_content": "An execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}, {"page_content": "Data scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}], "question": "What are some steps involved in the production stage of the MLOps workflow?", "gold_answer": "Some steps involved in the production stage of the MLOps workflow include:\n\n1. Computing fresh feature values.\n2. Training and testing new model versions.\n3. Publishing predictions to downstream tables or applications.\n4. Monitoring the entire process to avoid performance degradation and instability.\n\nAdditionally, it is mentioned that ML engineers own the production environment and are responsible for deploying ML pipelines, while data scientists should have visibility to test results, logs, model artifacts, and production pipeline status to help identify and diagnose problems in production."}
{"hard_neg_docs": [{"page_content": "An execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}, {"page_content": "Data scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "To reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)", "fmt_content": "Source: https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html\nContent:\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)", "metadata": {"source": "https://docs.databricks.com/machine-learning/mlops/mlops-workflow.html", "seq_num": 215, "description": "", "text": "---\ndescription: Learn the recommended Databricks MLOps workflow to optimize performance and efficiency of your machine learning production systems.\n---\n\n# MLOps workflow on <Databricks>\n\nThis article describes how you can use MLOps on the Databricks Lakehouse platform to optimize the performance and long-term efficiency of your machine learning (ML) systems. It includes general recommendations for an MLOps architecture and describes a generalized workflow using the Databricks Lakehouse platform that you can use as a model for your ML development-to-production process.\n\n## What is MLOps?\nMLOps is a set of processes and automated steps to manage code, data, and models.  It combines DevOps, DataOps, and ModelOps.\n\n![MLOps lakehouse](/_static/images/machine-learning/mlops-lakehouse.png)\n\nML assets such as code, data, and models are developed in stages that progress from early development stages that do not have tight access limitations and are not rigorously tested, through an intermediate testing stage, to a final production stage that is tightly controlled. The Databricks Lakehouse platform lets you manage these assets on a single platform with unified access control. You can develop data applications and ML applications on the same platform, reducing the risks and delays associated with moving data around.\n\n## General recommendations for MLOps\n\nThis section includes some general recommendations for MLOps on Databricks with links for more information.\n\n### Create a separate environment for each stage\n\nAn execution environment is the place where models and data are created or consumed by code. Each execution environment consists of compute instances, their runtimes and libraries, and automated jobs.\n\nDatabricks recommends creating separate environments for the different stages of ML code and model development with clearly defined transitions between stages. The workflow described in this article follows this process, using the common names for the stages:\n- [Development](#development-stage)\n- [Staging](#staging-stage)\n- [Production](#production-stage)\n\nOther configurations can also be used to meet the specific needs of your organization.\n\n### Access control and versioning\n\nAccess control and versioning are key components of any software operations process. Databricks recommends the following:\n\n- **Use Git for version control.** Pipelines and code should be stored in Git for version control. Moving ML logic between stages can then be interpreted as moving code from the development branch, to the staging branch, to the release branch. Use [Databricks Repos](/repos/index.md) to integrate with your Git provider and sync notebooks and source code with Databricks workspaces. Databricks also provides additional tools for Git integration and version control; see [_](/dev-tools/index.md).\n- **Store data in a Lakehouse architecture using Delta tables.** Data should be stored in a [Lakehouse architecture](/lakehouse/index.md) in your cloud account. Both raw data and feature tables should be stored as [Delta tables](/delta/index.md) with access controls to determine who can read and modify them.\n- **Manage models and model development with MLflow.** You can use [MLflow](/mlflow/index.md) to track the model development process and save code snapshots, model parameters, metrics, and other metadata. Use the [Model Registry](/mlflow/model-registry.md) to manage model versioning and deployment status. The Model Registry provides [webhooks](/mlflow/model-registry-webhooks.md) and an API so you can integrate with CD systems, and also handles [access control for models](/security/auth-authz/access-control/workspace-acl.md#configure-mlflow-model-permissions).\n\n### Deploy code, not models\n\nIn most situations, Databricks recommends that during the ML development process, you promote _code_, rather than  _models_, from one environment to the next. Moving project assets this way ensures that all code in the ML development process goes through the same code review and integration testing processes. It also ensures that the production version of the model is trained on production code. For a more detailed discussion of the options and trade-offs, see [_](/machine-learning/mlops/deployment-patterns.md).\n\n.. toctree::\n   :maxdepth: 1\n   :hidden:\n\n   deployment-patterns\n\n<!-- Remove for now per discussion on PR 6878\n.. azure-aws::\n\n  ### Consider Databricks MLOps project templates\n  ### Automate MLOps using Databricks MLOps project templates (suggested change from PR 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automates the creation of a production-grade ML project on <Databricks>.\n\n  (input from PF 6878)\n  [Databricks MLOps project templates](/machine-learning/mlops/mlops-templates.md) automate the creation of a production-grade ML project on <Databricks>. If you:\n    - Are new to ML or early in your ML journey: consider adopting the templates wholesale to quickly onboard to and benefit from a production-grade project stack\n    - Have existing ML projects in production: consider adopting individual components of the templates into your existing production ML stack.\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     mlops-templates\n\n-->\n\n## Recommended MLOps workflow\n\nThe following sections describe a typical MLOps workflow, covering each of the three stages: development, staging, and production.\n\nThis section uses the terms \"data scientist\" and \"ML engineer\" as archetypal personas; specific roles and responsibilities in the MLOps workflow will vary between teams and organizations.\n\n<a id=\"development-stage\"></a>\n\n## Development stage\n\nThe focus of the development stage is experimentation. Data scientists develop features and models and run experiments to optimize model performance. The output of the development process is ML pipeline code that can include feature computation, model training, inference, and monitoring.\n\n![MLOps development stage diagram](/_static/images/machine-learning/mlops-dev-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Data sources\n\nData scientists working in the dev environment often have read-only access to production data. In some cases to meet data governance requirements, the dev environment may have access only to a mirror or redacted version of production data. Data scientists also have read-write access to a separate dev storage environment to develop and experiment with new features and other data tables.\n\n### 2. Exploratory data analysis (EDA)\n\nData scientists explore and analyze data in an interactive, iterative process using notebooks, visualizations, and [Databricks SQL](/sql/index.md).\n\nThis ad hoc process is generally not part of a pipeline which will be deployed in other execution environments.\n\n### 3. Code\n\nAll of the code for the ML system is stored in a code repository. Data scientists create new or updated pipelines in a development branch of the Git project. The code can be developed inside or outside of <Databricks> and synced with the <Databricks> workspace using [Databricks Repos](/repos/index.md).\n\n### 4. Update feature tables\n\nThe model development pipeline reads from both raw data tables and existing feature tables, and writes to tables in the [Feature Store](/machine-learning/feature-store/index.md). This pipeline includes 2 tasks:\n\n- **Data preparation.** Check for data quality issues.\n\n- **Create or update feature tables.** Data scientists develop or update code to create features. These pipelines can read from the Feature Store and other Lakehouse tables and write to feature tables in the dev storage environment. Data scientists then use these dev feature tables to create prototype models. When the code is promoted to production, these changes update the production feature tables.\n\n  Feature pipelines can be managed separately from other ML pipelines, especially if they are owned by different teams.\n\n### 5. Train model\n\nData scientists develop model training and other pipelines either on read-only production data or on non-production data. The pipelines can use feature tables in either the dev or prod environments.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** The model training process reads features from the feature store and silver- or gold-level Lakehouse tables, and it logs model parameters, metrics, and artifacts to the MLflow tracking server.\n\n  When training and hyperparameter tuning are complete, the data scientist saves the final model artifact to the tracking server. This records a link between the model, its input data, and the code used to generate it.\n\n  When this training pipeline is run in staging or production, ML engineers (or their CI/CD code) can load the model by using the model URI (or path) and then push the model to the Model Registry for management and testing.\n\n- **Evaluation.** Evaluate model quality by testing on held-out data. The results of these tests are logged to the MLflow tracking server.\n\n  If your organization’s governance requirements include additional information about the model, you can save it using [MLflow tracking](/mlflow/tracking.md). Typical artifacts are plain text descriptions and model interpretations like those produced by SHAP or LIME.\n\n### 6. Commit code\n\nAfter developing code for feature engineering, training, inference, and other pipelines, the data scientist or ML engineer commits the dev branch changes into source control.\n\n<a id=\"staging-stage\"></a>\n\n## Staging stage\n\nThe focus of this stage is testing the ML pipeline code to ensure it is ready for production. All of the ML pipeline code is tested in this stage, including code for model training as well as feature engineering pipelines, inference code, and so on.\n\nML engineers create a CI pipeline to implement the unit and integration tests run in this stage. The output of the staging process is a release branch that triggers the CI/CD system to start the production stage.\n\n![MLOps staging stage diagram](/_static/images/machine-learning/mlops-staging-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\nThe staging environment can have its own storage area for testing feature tables and ML pipelines.  This storage is generally temporary and only retained until testing is complete. The development environment may also require access to this data storage for debugging purposes.\n\n### 1. Merge request\n\nThe deployment process begins when an ML engineer creates a merge request to the staging branch (usually the “main” branch) in source control. The merge request triggers a continuous integration (CI) process.\n\n### 2. Unit tests\n\nThe CI process automatically builds the source code and triggers unit tests. If the tests fail, the merge request is rejected. Unit tests do not interact with data or other services.\n\n### 3. Integration tests (CI)\n\nThe CI process then runs the integration tests. Integration tests run all pipelines (including feature engineering, model training, inference, and monitoring) to ensure that they function correctly together. The staging environment should match the production environment as closely as is reasonable.\n\nTo reduce the time required to run integration tests, model training steps can trade off between fidelity of testing and speed. For example, you might use small subsets of data or run fewer training iterations. Depending on the intended use of the model, you may choose to do full-scale load testing at this point.\n\nAfter the integration tests pass on the staging branch, you can promote the code to production.\n\n### 4. Merge to staging branch\n\nIf the tests pass, the code can be merged to the staging branch. If tests fail, the CI/CD system should notify users and post results on the merge (pull) request.\n\nYou can schedule periodic integration tests on the staging branch. This is a good idea if the branch is updated frequently by different users.\n\n### 5. Create a release branch\n\nWhen the code is ready to deploy to production, the ML engineer creates a release branch, which triggers the CI/CD system to update production jobs.\n\n<a id=\"production-stage\"></a>\n\n## Production stage\n\nML engineers own the production environment, where ML pipelines are deployed. These pipelines compute fresh feature values, train and test new model versions, publish predictions to downstream tables or applications, and monitor the entire process to avoid performance degradation and instability.\n\nData scientists typically do not have write or compute access in the production environment. However, it is important that they have visibility to test results, logs, model artifacts, and production pipeline status, to allow them to identify and diagnose problems in production.\n\n![MLOps production stage diagram](/_static/images/machine-learning/mlops-prod-diagram.png)\n\nThe numbered steps correspond to the numbers shown in the diagram.\n\n### 1. Update feature tables\n\nAs new production data becomes available, this pipeline ingests it and updates the production feature store tables. This pipeline can be run as a batch or streaming job and can be scheduled, triggered, or run continuously.\n\n### 2. Train model\n\nTrain the production version of the model on the full production data and register it with the MLflow Model Registry. This pipeline can be triggered by code changes or by automated retraining jobs.\n\nThis pipeline includes 2 tasks:\n\n- **Training and tuning.** As in the development stage, autologging saves a record of the training process to the MLflow tracking server.  This includes model metrics, parameters, tags, and the model itself.\n\n  During development,", "timestamp": 0, "next": ""}}], "question": "Who typically owns the production environment in the MLOps workflow?", "gold_answer": "In the MLOps workflow, ML engineers typically own the production environment, where ML pipelines are deployed."}
{"hard_neg_docs": [{"page_content": "\"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n\"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I retrieve information about routine privileges using SQL?", "gold_answer": "To retrieve information about routine privileges using SQL, you can execute the following SQL query:\n\n```sql\nSELECT specific_catalog, specific_schema, specific_name, grantee \nFROM information_schema.routine_privileges;\n```\n\nThis query will return details from the `information_schema.routine_privileges` relation, including the `specific_catalog`, `specific_schema`, `specific_name`, and the `grantee` for each routine privilege."}
{"hard_neg_docs": [{"page_content": "\"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n\"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html\nContent:\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/information-schema/routine_privileges.html", "seq_num": 1292, "description": "", "text": "---\ndescription: Learn about the INFORMATION_SCHEMA.ROUTINE_PRIVILEGES relation in <DBSQL> and <DBR>.\n---\n\n# ROUTINE_PRIVILEGES\n\n.. azure-aws::\n  .. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR> 11.1 and above ![check marked yes](/_static/images/icons/check.png) Unity Catalog only\n\nINFORMATION_SCHEMA.ROUTINE_PRIVILEGES lists [principals](../sql-ref-principal.md) that have [privileges](../sql-ref-privileges.md) on a [routine](routines.md) in a [catalog](catalogs.md).\n\nThe rows returned are limited to the routines the user is privileged to interact with.\n\n## Definition\n\nThe `ROUTINE_PRIVILEGES` relation contains the following columns:\n\n.. csv-table::\n   :header: \"Name\", \"Data type\", \"Nullable\", \"Standard\", \"Description\"\n\n   \"`GRANTOR`\", \"`STRING`\", \"No\", \"Yes\", \"Principal who granted the privilege.\"\n   \"`GRANTEE`\", \"`STRING`\", \"No\", \"Yes\", \"Principal to which the privilege is granted.\"\n   \"`SPECIFIC_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"[Catalog](catalogs.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"[Database](schemata.md) of routine on which the privilege is granted.\"\n   \"`SPECIFIC_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Schema unique (specific) name of [routine](routines.md) on which the privilege is granted.\"\n   \"`ROUTINE_CATALOG`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_CATALOG`.\"\n   \"`ROUTNE_SCHEMA`\", \"`STRING`\", \"No\", \"Yes\", \"Matches `SPECIFIC_SCHEMA`.\"\n   \"`ROUTINE_NAME`\", \"`STRING`\", \"No\", \"Yes\", \"Name of routine on which the privilege is granted.\"\n   \"`PRIVILEGE_TYPE`\", \"`STRING`\", \"No\", \"Yes\", \"Privilege being granted.\"\n   \"`IS_GRANTABLE`\", \"`STRING`\", \"No\", \"Yes\", \"Always `NO`. Reserved for future use.\"\n   \"`INHERITED_FROM`\", \"`STRING`\", \"No\", \"No\", \"The ancestor relation that the privilege is inherited from.\"\n\n## Constraints\n\nThe following constraints apply to the `ROUTINE_PRIVILEGES` relation:\n\n.. csv-table::\n   :header: \"Class\", \"Name\", \"Column List\", \"Description\"\n\n   \"Primary key\", \"`ROUTINE_PRIVS_PK`\", \"`GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`\", \"Unique identifier for the granted privilege.\"\n   \"Foreign key\", \"`ROUTINE_PRIVS_ROUTINE_FK`\", \"`SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`\", \"References [ROUTINES](routines.md)\"\n\n## Examples\n\n```sql\n> SELECT specific_catalog, specific_schema, specific_name, grantee \n    FROM information_schema.routine_privileges;\n```\n\n## Related\n\n- [Information schema](../sql-ref-information-schema.md)\n- [INFORMATION_SCHEMA.ROUTINE](routines.md)\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the primary key for the `ROUTINE_PRIVILEGES` relation?", "gold_answer": "The primary key for the `ROUTINE_PRIVILEGES` relation is `ROUTINE_PRIVS_PK`, which consists of the columns `GRANTOR`, `GRANTEE`, `SPECIFIC_CATALOG`, `SPECIFIC_SCHEMA`, `SPECIFIC_NAME`, `PRIVILEGE_TYPE`. This serves as a unique identifier for the granted privilege."}
{"hard_neg_docs": [{"page_content": "```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What are some characteristics of streaming jobs that may benefit from asynchronous state checkpointing?", "gold_answer": "The document does not provide the specific characteristics of streaming jobs that may benefit from asynchronous state checkpointing. It only introduces the concept and mentions that there are certain streaming job characteristics that may benefit from it, but the actual list of characteristics is not included in the provided excerpt. To know the characteristics, you would need to access the full content of the document at the source provided."}
{"hard_neg_docs": [{"page_content": "```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/async-checkpointing.html\nContent:\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.", "metadata": {"source": "https://docs.databricks.com/structured-streaming/async-checkpointing.html", "seq_num": 1379, "description": "", "text": "---\ndescription: For stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees.\n---\n\n# Asynchronous state checkpointing for <SS>\n\n.. note::\n\n   Available in <DBR> 10.3 and above.\n\nFor stateful streaming queries bottlenecked on state updates, enabling asynchronous state checkpointing can reduce end-to-end latencies without sacrificing any fault-tolerance guarantees, but with a minor cost of higher restart delays.\n\n<SS> uses synchronous checkpointing by default. Every micro-batch ensures that all the state updates in that batch are backed up in cloud storage (called “checkpoint location”) before starting the next batch. If a stateful streaming query fails, all micro-batches except the last micro-batch are checkpointed. On restart, only the last batch needs to be re-run. Fast recovery with synchronous checkpointing comes at the cost of higher latency for each micro-batch.\n\n![Streaming State Checkpointing Modes](/_static/images/spark/structured-streaming/streaming-state-checkpointing-modes.png)\n\nAsynchronous state checkpointing attempts to perform the checkpointing asynchronously so that the micro-batch execution doesn’t have to wait for the checkpoint to complete. In other words, the next micro-batch can start as soon as the computation of the previous micro-batch has been completed. Internally, however, the offset metadata (also saved in the checkpoint location) tracks whether the state checkpointing has been completed for a micro-batch. On query restart, more than one micro-batch may need to be re-executed - the last micro-batch whose computation was incomplete, as well as the one micro-batch before it whose state checkpointing was incomplete. And you get the same fault-tolerance guarantees (that is, exactly-once guarantees with an idempotent sink) as that of synchronous checkpointing.\n\n## Identifying <SS> workloads that benefit from asynchronous checkpointing\n\nThe following are streaming job characteristics that may benefit from asynchronous state checkpointing.\n\n- Job has one or more stateful operations (e.g., aggregation, `flatMapGroupsWithState`, `mapGroupsWithState`, stream-stream joins)\n- State checkpoint latency is one of the major contributors to overall batch execution latency. This information can be found in the [StreamingQueryProgress](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#monitoring-streaming-queries) events. These events are found in log4j logs on Spark driver as well. Here is an example of streaming query progress and how to find the state checkpoint impact on the overall batch execution latency.\n\n  - ```json\n     {\n       \"id\" : \"2e3495a2-de2c-4a6a-9a8e-f6d4c4796f19\",\n       \"runId\" : \"e36e9d7e-d2b1-4a43-b0b3-e875e767e1fe\",\n       \"...\",\n       \"batchId\" : 0,\n       \"durationMs\" : {\n         \"...\",\n         \"triggerExecution\" : 547730,\n         \"...\"\n       },\n       \"stateOperators\" : [ {\n         \"...\",\n         \"commitTimeMs\" : 3186626,\n         \"numShufflePartitions\" : 64,\n         \"...\"\n       }]\n     }\n\n    ```\n\n  - State checkpoint latency analysis of above query progress event\n    - Batch duration (`durationMs.triggerDuration`) is around 547 secs.\n    - State store commit latency (`stateOperations[0].commitTimeMs`) is around 3,186 secs. Commit latency is aggregated across tasks containing a state store. In this case there are 64 such tasks (`stateOperators[0].numShufflePartitions`).\n    - Each task containing state operator took an average of 50 sec (3,186/64) for checkpoint. This is an extra latency that is contributed to the batch duration. Assuming all 64 tasks are running concurrently, checkpoint step contributed around 9% (50 secs / 547 secs) of the batch duration. The percentage gets even higher when the max concurrent tasks is less than 64.\n\n## Enabling asynchronous state checkpointing\n\nSet following configuration in streaming job. Async checkpointing needs a state store implementation that supports async commits. Currently only the [RocksDB based state store](/structured-streaming/rocksdb-state-store.md) implementation supports it.\n\n```scala\n\nspark.conf.set(\n  \"spark.databricks.streaming.statefulOperator.asyncCheckpoint.enabled\",\n  \"true\"\n)\n\nspark.conf.set(\n  \"spark.sql.streaming.stateStore.providerClass\",\n  \"com.databricks.sql.streaming.state.RocksDBStateStoreProvider\"\n)\n\n```\n\n## Limitations and requirements for asynchronous checkpointing\n\n.. include:: /shared/warnings/no-ss-autoscale.md\n\n- Any failure in an asynchronous checkpoint at any one or more stores fails the query. In synchronous checkpointing mode, the checkpoint is executed as part of the task and Spark retries the task multiple times before failing the query. This mechanism is not present with asynchronous state checkpointing. However, using the Databricks [job retries](/workflows/jobs/jobs.md#retries), such failures can be automatically retried.\n\n- Asynchronous checkpointing works best when the state store locations are not changed between micro-batch executions. Cluster resizing, in combination with asynchronous state checkpointing, may not work well because the state stores instance might get re-distributed as nodes are added or deleted as part of the cluster resizing event.\n\n- Asynchronous state checkpointing is supported only in the RocksDB state store provider implementation. The default in-memory state store implementation does not support it.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the purpose of asynchronous state checkpointing in structured streaming?", "gold_answer": "The purpose of asynchronous state checkpointing in structured streaming is to perform checkpointing asynchronously, which allows the micro-batch execution to proceed without having to wait for the checkpoint to complete. This means that the next micro-batch can start immediately after the computation of the previous micro-batch has finished, rather than waiting for the checkpointing process to finish. This approach aims to improve the performance of streaming jobs by reducing the latency between micro-batches. Despite the asynchronous nature, it maintains the same fault-tolerance guarantees as synchronous checkpointing, providing exactly-once guarantees with an idempotent sink, while also tracking the completion of state checkpointing for each micro-batch in the offset metadata."}
{"hard_neg_docs": [{"page_content": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I update a subset of fields for a job using the Jobs API in Databricks?", "gold_answer": "You can update a subset of fields for a job using the Jobs API in Databricks by utilizing the \"Update\" endpoint. This endpoint allows you to specify only the fields which should be added, changed, or deleted without overwriting all job settings. This is particularly useful for bulk updates and is described as faster, easier, and safer when only some job settings need to be changed. Details on how to use this endpoint can be found in the Jobs API documentation, which is referenced with a placeholder link in the provided document. Unfortunately, the exact link to the documentation is represented by an underscore (_) in the text and is not directly accessible from the text provided. To proceed, you would need to navigate to the actual Jobs API documentation on the Databricks website to find the specific instructions for using the Update endpoint."}
{"hard_neg_docs": [{"page_content": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/december.html\nContent:\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/december.html", "seq_num": 1936, "description": "", "text": "---\ndescription: December 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# December 2020\n\nThese features and <Databricks> platform improvements were released in December 2020.\n\n.. note::\n\n    Releases are staged. Your <Databricks> account may not be updated until a week or more after the initial release date.\n\n## <DBR> 7.5 GA\n\n**December 16, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now generally available.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Existing Databricks accounts migrate to E2 platform today\n\n  **December 16, 2020**\n\n  Today we begin the migration of existing Databricks accounts to the [E2 version of the platform](/getting-started/overview.md#e2-architecture), bringing new security features and the ability to create and manage multiple workspaces with ease. This first phase includes all invoiced accounts. Free Trial and \"pay-as-you-go\" accounts that are billed monthly by credit card will be migrated at a later date. Databricks is sending notifications to owners of all accounts being migrated. This migration makes no change to accounts that already have workspaces on the E2 platform.\n\n  Accounts that migrate today will use the [new account console](/administration-guide/account-settings/index.md) to create and manage workspaces and view usage, and can use the [Account API](/administration-guide/workspace/create-workspace-api.md) to create and manage workspaces. Your _existing workspaces_ will not be converted to the E2 platform, but any _new workspace_ you create will be on E2.\n\n  For more information about managing accounts on E2, see [_](/administration-guide/account-settings/index.md).\n\n## Jobs API now supports updating existing jobs\n\n**December 14, 2020**\n\nYou can now use the Jobs API [_](/dev-tools/api/2.0/jobs.md#update) endpoint to update a subset of fields for a job. The Update endpoint supplements the existing [_](/dev-tools/api/2.0/jobs.md#reset) endpoint by allowing you to specify only the fields which should be added, changed, or deleted rather than overwriting all job settings. Using the Update endpoint is faster, easier, and safer when only some job settings need to be changed, particularly for bulk updates.\n\n.. azure-aws::\n  ## New global init script framework is GA\n\n  **December 14, 2020**\n\n  The new global init script framework, which was released as a Public Preview in July, is now generally available. The new framework brings significant improvements over legacy global init scripts:\n\n  *  Init scripts are more secure, requiring admin permissions to create, view, and delete.\n  *  Script-related launch failures are logged.\n  *  You can set the execution order of multiple init scripts.\n  *  Init scripts can reference cluster-related environment variables.\n  *  Init scripts can be created and managed using the admin console or the new Global Init Scripts REST API.\n\n  Databricks recommends that you [migrate existing legacy global init scripts to the new framework](/clusters/init-scripts.md#migrate-legacy-scripts) to take advantage of these improvements.\n\n  For details, see [_](/clusters/init-scripts.md#global-init-script).\n\n.. aws::\n\n  ## New account console enables customers on the E2 platform to create and manage multiple workspaces (Public Preview)\n\n  **December 8, 2020**\n\n  We are excited to announce the Public Preview of the new [account console](/administration-guide/account-settings/index.md) for accounts on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). The new account console gives you visibility across all of your workspaces in a single pane of glass, enabling you to:\n\n  - Create and manage the lifecycle of multiple workspaces.\n  - View your organization's spend on Databricks across all workspaces.\n\n  ![Workspace form](/_static/images/account-settings/workspace-form.png)\n\n  The new account console is available for customers who are on the next generation of Databricks’ Enterprise Cloud Platform (E2). In the coming weeks we will also upgrade existing non-E2 Databricks accounts to enable you to take advantage of multiple workspaces for your teams and the account console to manage them. Stay tuned for more information about that migration.\n\n  For details, see [_](/administration-guide/account-settings/index.md).\n\n## <DBR> 7.5 (Beta)\n\n**December 3, 2020**\n\n<DBR> 7.5, <DBR> 7.5 ML, and <DBR> 7.5 for Genomics are now available as Beta releases.\n\nFor information, see the full release notes at [_](/release-notes/runtime/7.5.md), [_](/release-notes/runtime/7.5ml.md), and [_](/release-notes/runtime/7.5genomics.md).\n\n.. aws::\n\n  ## Auto-AZ: automatic selection of availability zone (AZ) when you launch clusters\n\n  **December 2-8, 2020: Version 3.34**\n\n  For workspaces on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you can now enable automatic selection of availability zone (AZ) when your users launch clusters (including job clusters). After you [enable Auto-AZ using the Clusters API](/dev-tools/api/latest/clusters.md#clusterawsattributes), <Databricks> chooses an AZ automatically based on available IPs in the workspace subnets, and retries in other AZs in case insufficient capacity errors are returned by AWS.\n\n  See [_](/clusters/configure.md#availability-zones).\n\n## Jobs API end_time field now uses epoch time\n\n**December 2-8, 2020: Version 3.34**\n\nThe Jobs API returns new data for the [Runs Get](/dev-tools/api/2.0/jobs.md#runs-get) and [Runs List](/dev-tools/api/2.0/jobs.md#runs-list) endpoints. The new `end_time` field returns the epoch time (milliseconds since 1/1/1970 UTC) when the run ended, for example `1605266150681`.\n\n## Find DBFS files using new visual browser\n\n**December 2-8, 2020: Version 3.34**\n\nYou can now browse and search for DBFS objects  in the <Databricks> workspace UI using the new [DBFS file browser](/dbfs/file-browser.md).\n\n![Browse DBFS](/_static/images/dbfs/browse.png)\n\nYou can also upload files to DBFS using the [new upload dialog](/dbfs/file-browser.md) in the browser.\n\nThis feature is disabled by default. An admin user must enable it using the Admin Console. See [_](/administration-guide/workspace/settings/dbfs-browser.md).\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects are now enabled by default on new workspaces\n\n**December 1-8, 2020: Version 3.34**\n\nVisibility controls, which were introduced in September, ensure that users can view only the notebooks, folders, clusters, and jobs that they have been given access to through workspace, cluster, or jobs access control. With this release, visibility controls are enabled by default for new workspaces. If your workspace was created before this release, an admin must enable them. See:\n\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Improved display of nested runs in MLflow\n\n**December 2-8, 2020: Version 3.34**\n\nWe improved the display of nested runs in the MLflow runs table. Child runs now appear grouped beneath the root run.\n\n## Admins can now lock user accounts (Public Preview)\n\n**December 2-8, 2020: Version 3.34**\n\nAdministrators can now set a user's status to inactive. An inactive user can’t access the workspace, but their permissions and associations with objects such as running clusters or access to notebooks remain unchanged. You can manually deactivate users via the SCIM API or automatically deactivate users after a set period of inactivity. See [_](/dev-tools/api/latest/scim/scim-users.md#activate-and-deactivate-user-by-id).\n\n## Updated NVIDIA driver\n\n**December 2-8, 2020: Version 3.34**\n\nGPU-enabled clusters now use NVIDIA driver version 450.80.02.\n\n.. aws::\n\n  ## Use your own keys to secure notebooks (Public Preview)\n\n  **December 1, 2020**\n\n  By default, notebooks and secrets are encrypted in the <Databricks> [control plane](/getting-started/overview.md) using a key unique to the control plane but not the workspace. Now you can specify your own encryption key to encrypt notebook and secret data in the control plane, a feature known as [customer-managed keys for notebooks](/security/keys/customer-managed-keys-managed-services-aws.md).\n\n  This feature is now in Public Preview. It requires that your account be on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture) or on a custom plan that has been enabled by <Databricks> for this feature. Contact your <Databricks> representative if you want to use the feature but are not on an E2 workspace.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are the benefits of using the new global init script framework in Databricks?", "gold_answer": "The benefits of using the new global init script framework in Databricks include:\n\n- Enhanced security, as the creation, viewing, and deletion of init scripts require admin permissions.\n- Improved reliability, with script-related launch failures being logged.\n- The ability to set the execution order of multiple init scripts.\n- Init scripts can reference cluster-related environment variables.\n- Init scripts can be created and managed using the admin console or the Global Init Scripts REST API.\n\nDatabricks recommends migrating existing legacy global init scripts to the new framework to take advantage of these improvements."}
{"hard_neg_docs": [{"page_content": ".. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}, {"page_content": "```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "There are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}], "question": "What are the four formats for input JSON when creating serving endpoints?", "gold_answer": "The document provided lists two of the four formats for input JSON when creating serving endpoints in Databricks:\n\n- `dataframe_split`: This format is a JSON-serialized Pandas DataFrame in the `split` orientation.\n\n- `dataframe_records`: This format is a JSON-serialized Pandas DataFrame in the `records` orientation.\n\nUnfortunately, the document supplied does not include the other two formats. To obtain information on the remaining formats, you would need to refer to the complete documentation."}
{"hard_neg_docs": [{"page_content": ".. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}, {"page_content": "```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "There are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,", "fmt_content": "Source: https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html\nContent:\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,", "metadata": {"source": "https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html", "seq_num": 220, "description": "", "text": "---\ndescription: Learn how to create and manage your MLflow models as REST API endpoints with <Databricks> Model Serving for model deployment and model inference.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# Create and manage model serving endpoints\n\nThis article describes how to create and manage model serving endpoints that utilize <Databricks> [Model Serving](/machine-learning/model-serving/index.md).\n\n<a id=\"requirement\"></a>\n\n## Requirements\n\n- Model Serving is only available for Python-based MLflow models registered in the MLflow Model Registry. You must declare all model dependencies in the conda environment or requirements file.\n\n  - If you don't have a registered model, see the [notebook examples](#example-notebooks) for pre-packaged models you can use to get up and running with Model Serving endpoints.\n\n- Your workspace must be in a [supported region](/machine-learning/model-serving/index.md#regions).\n\n- If you use custom libraries or libraries from a private mirror server with your model, see [_](/machine-learning/model-serving/private-libraries-model-serving.md) before you create the model endpoint.\n\n.. important:: If you rely on Anaconda, review the [terms of service](#anaconda-notice) notice for additional information.\n\n## Access control\n\nTo understand access control options for model serving endpoints and best practice guidance for endpoint management, see [_](/security/auth-authz/access-control/serving-endpoint-acl.md).\n\n## Create model serving endpoints\n\nYou can create Model Serving endpoints with the <DML> API or the <DML> UI. An endpoint can serve any registered Python MLflow model in the [Model Registry](/mlflow/model-registry.md).\n\n### API workflow\n\nYou can create an endpoint with the following:\n\n```bash\n\nPOST /api/2.0/serving-endpoints\n\n{\n  \"name\": \"feed-ads\",\n  \"config\": {\n   \"served_models\": [{\n     \"model_name\": \"ads1\",\n     \"model_version\": \"1\",\n     \"workload_size\": \"Small\",\n     \"scale_to_zero_enabled\": true,\n   }]\n }\n}\n\n```\n\nThe following is an example response. The endpoint’s `config_update` state is `IN_PROGRESS` and the served model is in a `CREATING` state. The `pending_config` field shows the details of the update that is in progress.\n\n```json\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"customer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666829055000,\n  \"state\": {\n    \"ready\": \"NOT_READY\",\n    \"config_update\": \"IN_PROGRESS\"\n  },\n  \"pending_config\": {\n    \"start_time\": 1666718879000,\n    \"served_models\": [{\n      \"name\": \"ads1-1\",\n      \"model_name\": \"ads1\",\n      \"model_version\": \"1\",\n      \"workload_size\": \"Small\",\n      \"scale_to_zero_enabled\": true,\n      \"state\": {\n        \"deployment\": \"DEPLOYMENT_CREATING\",\n        \"deployment_state_message\": \"Creating\"\n      },\n      \"creator\": \"customer@example.com\",\n      \"creation_timestamp\": 1666829055000\n    }],\n    \"config_version\": 1,\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n<a id=\"model-serving-ui\"></a>\n\n### UI workflow\n\nYou can create an endpoint for model serving with the **Serving** UI.\n\n#. Click **Serving** in the sidebar to display the Serving UI.\n#. Click **Create serving endpoint**.\n#. In the **Serving endpoint name** field provide a name for your endpoint.\n#. In the **Edit configuration** section select which model and model version you want to serve.\n#. Select what size compute to use.\n#. Specify if the endpoint should scale to zero when not in use, and the percentage of traffic to route to a served model.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready.\n\nYou can also access the **Serving** UI to create an endpoint from the [registered model page](/mlflow/model-registry.md#registered-model-page) in the <DML> UI.\n\n#. Select the model you want to serve.\n#. Click the **Use model for inference** button.\n#. Select the **Real-time** tab.\n#. Select the model version and provide an endpoint name.\n#. Select the compute size for your endpoint, and specify if your endpoint should scale to zero when not in use.\n#. Click **Create serving endpoint**. The **Serving endpoints** page appears with **Serving endpoint state** shown as Not Ready. After a few minutes, **Serving endpoint state** changes to Ready.\n\n## Modify the compute configuration of an endpoint\n\nAfter enabling a model endpoint, you can set the compute configuration as desired with the API or the UI. This configuration is particularly helpful if you need additional resources for your model. Workload size and compute configuration play a key role in what resources are allocated for serving your model.\n\nUntil the new configuration is ready, the old configuration keeps serving prediction traffic. While there is an update in progress, another update cannot be made.\n\nYou can also configure your endpoint to serve multiple models. See [_](/machine-learning/model-serving/serve-multiple-models-to-serving-endpoint.md).\n\n### API workflow\n\n```bash\nPUT /api/2.0/serving-endpoints/{name}/config\n\n{\n  \"served_models\": [{\n    \"model_name\": \"ads1\",\n    \"model_version\": \"2\",\n    \"workload_size\": \"Small\",\n    \"scale_to_zero_enabled\": true,\n  }]\n}\n```\n\nThe following is a response example:\n\n```json\n\n{\n  \"name\": \"feed-ads\",\n  \"creator\": \"cuastomer@example.com\",\n  \"creation_timestamp\": 1666829055000,\n  \"last_updated_timestamp\": 1666946600000,\n  \"state\": {\n    \"ready\": true,\n    \"update_state\": \"IN_PROGRESS\"\n  },\n  \"config\": {\n    \"served_models\": [\n      {\n        \"name\": \"ads1-1\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"1\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_READY\",\n          \"deployment_state_message\": \"\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666887851000\n      }\n    ],\n    \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-1\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    },\n    \"config_version\": 2\n  },\n  \"pending_update\": {\n    \"start_time\": 1666946600000,\n    \"served_models\": [\n      {\n        \"name\": \"ads1-2\",\n        \"model_name\": \"ads1\",\n        \"model_version\": \"2\",\n        \"workload_size\": \"Small\",\n        \"scale_to_zero_enabled\": true,\n        \"state\": {\n          \"deployment\": \"DEPLOYMENT_CREATING\",\n          \"deployment_state_message\": \"Created\"\n        },\n        \"creator\": \"customer@example.com\",\n        \"creation_timestamp\": 1666946600000\n      }\n    ],\n     \"traffic_config\": {\n      \"routes\": [\n        {\n          \"served_model_name\": \"ads1-2\",\n          \"traffic_percentage\": 100\n        }\n      ]\n    }\n    \"config_version\": 3\n  },\n  \"id\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n  \"permission_level\": \"CAN_MANAGE\"\n}\n\n```\n\n### UI workflow\n\nAfter you enable a model endpoint, select **Edit configuration** to modify the compute configuration of your endpoint.\n\nYou can do the following:\n- Choose from a few workload sizes, and autoscaling is automatically configured within the workload size.\n- Specify if your endpoint should scale down to zero when not in use.\n- Modify the percent of traffic to route to your served model.\n\n<a id=\"score\"></a>\n\n## Scoring a model endpoint\n\nTo score a deployed model, you can send a REST API request to the model URL or use the UI.\n\nYou can call a model by calling the API and score using this URI:\n\n`POST /serving-endpoints/{endpoint-name}/invocations`\n\n### Request format\n\nRequests should be sent by constructing a JSON with one of the following keys and a JSON object corresponding to the input format.\n\nThere are four formats for the input JSON depending on your use case:\n\n- `dataframe_split` is JSON-serialized Pandas Dataframe in the `split` orientation.\n\n  ```json\n  {\n    \"dataframe_split\": [{\n      \"index\": [0, 1],\n      \"columns\": [\"sepal length (cm)\", \"sepal width (cm)\", \"petal length (cm)\", \"petal width (cm)\"],\n      \"data\": [[5.1, 3.5, 1.4, 0.2], [4.9, 3.0, 1.4, 0.2]]\n    }]\n  }\n  ```\n\n- `dataframe_records` is JSON-serialized Pandas Dataframe in the `records` orientation.\n\n   .. note:: This format does not guarantee the preservation of column ordering, and the `split` format is preferred over the `records` format.\n\n   ```json\n   {\n     \"dataframe_records\": [\n     {\n        \"sepal length (cm)\": 5.1,\n        \"sepal width (cm)\": 3.5,\n        \"petal length (cm)\": 1.4,\n    ", "timestamp": 0, "next": ""}}], "question": "Which format is preferred over the `records` format for JSON-serialized Pandas Dataframe?", "gold_answer": "The `split` format is preferred over the `records` format for JSON-serialized Pandas Dataframe."}
{"hard_neg_docs": [{"page_content": "- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the ISO 8601 week-numbering year for a given datetime?", "gold_answer": "The ISO 8601 week-numbering year for a given datetime is the year that the datetime falls in according to the ISO 8601 week-numbering system. For example, if the date is 2005-01-02, it is part of the 53rd week of the year 2004, so the ISO 8601 week-numbering year result would be 2004. This week-numbering year may differ from the calendar year for dates that fall in the first few days of January or the last few days of December."}
{"hard_neg_docs": [{"page_content": "- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)", "fmt_content": "Source: https://docs.databricks.com/sql/language-manual/functions/extract.html\nContent:\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)", "metadata": {"source": "https://docs.databricks.com/sql/language-manual/functions/extract.html", "seq_num": 1237, "description": "", "text": "---\ndescription: Learn the syntax of the extract function of the SQL language in <DBSQL> and <DBR>.\n---\n\n# `extract` function\n\n**Applies to:** ![check marked yes](/_static/images/icons/check.png) <DBSQL> ![check marked yes](/_static/images/icons/check.png) <DBR>\n\nReturns `field` of `source`.\n\n## Syntax\n\n```\nextract(field FROM source)\n```\n\n## Arguments\n\n- `field`: A keyword that selects which part of `source` should be extracted.\n- `source`: A DATE, TIMESTAMP, or INTERVAL expression.\n\n## Returns\nIf `field` is `SECOND`, a `DECIMAL(8, 6)`.\nIn all other cases, an INTEGER.\n\nSupported values of `field` when `source` is DATE or TIMESTAMP are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the year field\n- \"YEAROFWEEK\" - the ISO 8601 week-numbering year that the datetime falls in. For example, 2005-01-02 is part of the 53rd week of year 2004, so the result is 2004\n- \"QUARTER\", (\"QTR\") - the quarter (1 - 4) of the year that the datetime falls in\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the month field (1 - 12)\n- \"WEEK\", (\"W\", \"WEEKS\") - the number of the ISO 8601 week-of-week-based-year. A week is considered to start on a Monday and week 1 is the first week with >3 days. In the ISO week-numbering system, it is possible for early-January dates to be part of the 52nd or 53rd week of the previous year, and for late-December dates to be part of the first week of the next year. For example, 2005-01-02 is part of the 53rd week of year 2004, while 2012-12-31 is part of the first week of 2013\n- \"DAY\", (\"D\", \"DAYS\") - the day of the month field (1 - 31)\n- \"DAYOFWEEK\",(\"DOW\") - the day of the week for datetime as Sunday(1) to Saturday(7)\n- \"DAYOFWEEK_ISO\",(\"DOW_ISO\") - ISO 8601 based day of the week for datetime as Monday(1) to Sunday(7)\n- \"DOY\" - the day of the year (1 - 365/366)\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - The hour field (0 - 23)\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - the minutes field (0 - 59)\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - the seconds field, including fractional parts\n\nSupported values of `field` when `source` is INTERVAL are:\n\n- \"YEAR\", (\"Y\", \"YEARS\", \"YR\", \"YRS\") - the total months / 12\n- \"MONTH\", (\"MON\", \"MONS\", \"MONTHS\") - the total months % 12\n- \"DAY\", (\"D\", \"DAYS\") - the days part of interval\n- \"HOUR\", (\"H\", \"HOURS\", \"HR\", \"HRS\") - how many hours the microseconds contains\n- \"MINUTE\", (\"M\", \"MIN\", \"MINS\", \"MINUTES\") - how many minutes left after taking hours from microseconds\n- \"SECOND\", (\"S\", \"SEC\", \"SECONDS\", \"SECS\") - how many seconds with fractions left after taking hours and minutes from microseconds\n\n## Examples\n\n```sql\n> SELECT extract(YEAR FROM TIMESTAMP '2019-08-12 01:00:00.123456');\n 2019\n\n> SELECT extract(week FROM TIMESTAMP'2019-08-12 01:00:00.123456');\n 33\n\n> SELECT extract(DAY FROM DATE'2019-08-12');\n 12\n\n> SELECT extract(SECONDS FROM TIMESTAMP'2019-10-01 00:00:01.000001');\n 1.000001\n\n> SELECT extract(MONTHS FROM INTERVAL '2-11' YEAR TO MONTH);\n 11\n\n> SELECT extract(SECONDS FROM INTERVAL '5:00:30.001' HOUR TO SECOND);\n 30.001000\n```\n\n## Related functions\n\n- [_](date_part.md)\n- [_](dayofweek.md)\n- [_](dayofmonth.md)\n- [_](dayofyear.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I extract the quarter of the year from a datetime?", "gold_answer": "To extract the quarter of the year from a datetime, you can use the function `EXTRACT` with the keyword \"QUARTER\" or its abbreviation \"QTR\". For example, to extract the quarter from a datetime column `date_column`, you would use the SQL expression:\n\n```sql\nEXTRACT(QUARTER FROM date_column)\n```\n\nAlternatively, you could also use:\n\n```sql\nEXTRACT(QTR FROM date_column)\n```\n\nThis will return the quarter (1 - 4) of the year that the datetime falls in."}
{"hard_neg_docs": [{"page_content": "- If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\n- If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}, {"page_content": "To find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}], "question": "Can I deploy my Databricks workspace to a VNet in a different region?", "gold_answer": "No, you cannot deploy your Databricks workspace to a VNet in a different region. The VNet must reside in the same region as the Databricks workspace."}
{"hard_neg_docs": [{"page_content": "- If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\n- If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}, {"page_content": "To find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html\nContent:\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/azure/vnet-inject.html", "seq_num": 1523, "description": "", "text": "---\ndescription: Learn how to deploy <Databricks> in your Azure Virtual Network, also known as VNet injection.\n---\n\n.. aws-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Deploy <Databricks> in your Azure virtual network (VNet injection)\n\nThe default deployment of <Databricks> is a fully managed service on Azure: all data plane resources, including a [VNet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-overview) that all clusters will be associated with, are deployed to a locked resource group. If you require network customization, however, you can deploy <Databricks> data plane resources in your own virtual network (sometimes called _VNet injection_), enabling you to:\n\n- Connect <Databricks> to other Azure services (such as Azure Storage) in a more secure manner using [service endpoints](https://learn.microsoft.com/azure/virtual-network/virtual-network-service-endpoints-overview) or [private endpoints](https://learn.microsoft.com/azure/storage/common/storage-private-endpoints).\n\n- Connect to [on-premises data sources](/administration-guide/cloud-configurations/azure/on-prem-network.md) for use with <Databricks>, taking advantage of [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Connect <Databricks> to a [network virtual appliance](/administration-guide/cloud-configurations/azure/on-prem-network.md#route-via-firewall) to inspect all outbound traffic and take actions according to allow and deny rules, by using [user-defined routes](/administration-guide/cloud-configurations/azure/udr.md).\n\n- Configure <Databricks> to use [custom DNS](/administration-guide/cloud-configurations/azure/on-prem-network.md#vnet-custom-dns).\n\n- Configure [network security group (NSG) rules](https://learn.microsoft.com/azure/virtual-network/manage-network-security-group) to specify egress traffic restrictions.\n\n- Deploy <Databricks> clusters in your existing VNet.\n\nDeploying <Databricks> data plane resources to your own VNet also lets you take advantage of flexible CIDR ranges (anywhere between `/16`-`/24` for the VNet and up to `/26` for the subnets).\n\n.. important::\n    You cannot replace the VNet for an existing workspace. If your current workspace cannot accommodate the required number of active cluster nodes, we recommend that you create another workspace in a larger VNet. Follow these [detailed migration steps](https://learn.microsoft.com/azure/azure-databricks/howto-regional-disaster-recovery#detailed-migration-steps) to copy resources (notebooks, cluster configurations, jobs) from the old to new workspace.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n<a id=\"vnet-inject-reqs\"></a>\n\n## Virtual network requirements\n\nThe VNet that you deploy your <Databricks> workspace to must meet the following requirements:\n\n- **Region:** The VNet must reside in the same region as the <Databricks> workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the <Databricks> workspace.\n\n- **Address space:** A CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets: a container subnet and a host subnet. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n- **Subnets:** The VNet must include two subnets dedicated to your <Databricks> workspace: a container subnet (sometimes called the private subnet) and a host subnet (sometimes called the public subnet). However, for a workspace that uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both the container subnet and host subnet are _private_. It is _unsupported_ to share subnets across workspaces or deploy other Azure resources on the subnets that are used by your <Databricks> workspace. For guidance about maximum cluster nodes based on the size of your VNet and its subnets, see [_](#max-nodes).\n\n  - If you [_](#vnet-inject-portal), you can choose either to use existing subnets in the VNet or create two new subnets that you specify by names and IP ranges.\n\n  - If you use the [All-in-one ARM template](#arm-all-in-one) or the [VNet-only ARM template](#arm-vnet-only), the templates create the subnets for you. In both cases, the subnets are delegated to the `Microsoft.Databricks/workspaces` resource provider before workspace deployment, which allows <Databricks> to create [_](#nsg). <Databricks> will give advance notice if we need to add or update the scope of <a Databricks>-managed NSG rule.\n\n  - If you use the [Workspace ARM template](#arm-workspace) or a custom ARM template, it is up to you to ensure that your two subnets for the workspace use the _same_ network security group and are properly delegated. For delegation instructions, see [_](/administration-guide/cloud-configurations/azure/vnet-inject-upgrade.md#vnet-inject-upgrade) or [Add or remove a subnet delegation](https://learn.microsoft.com/azure/virtual-network/manage-subnet-delegation).\n\n  .. important::\n    There is a one-to-one relationship between these subnets and <a Databricks> workspace. You cannot share multiple workspaces across a single subnet. It is _unsupported_ to share subnets across workspaces or to deploy other Azure resources on the subnets that are used by your <Databricks> workspace.\n\nFor more information about templates to configure your VNet and deploy your workspace, see [Azure-Databricks-supplied Azure Resource Manager templates](#vnet-inject-advanced).\n\n<a id=\"max-nodes\"></a>\n\n### Address space and maximum cluster nodes\n\nA workspace with a smaller virtual network can run out of IP addresses (network space) more quickly than a workspace with a larger virtual network. Use a CIDR block between `/16` and `/24` for the VNet and a CIDR block up to `/26` for the two subnets (the container subnet and the host subnet).\n\nThe CIDR range for your VNet address space affects the maximum number of cluster nodes that your workspace can use:\n\n- <A Databricks> workspace requires two subnets in the VNet: a container subnet (also known as private subnet) and a host subnet (also known as public subnet). If the workspace uses [secure cluster connectivity](/security/network/secure-cluster-connectivity.md), both container and host subnets are private.\n\n- Azure reserves [five IPs in each subnet](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets).\n\n- Within each subnet, <Databricks> requires one IP address per cluster node. In total, there are two IP for each cluster node: one IP address for the host in the host subnet and one IP address for the container in the container subnet.\n\n- You may not want to use all the address space of your VNet. For example, you might want to create multiple workspaces in one VNet. Because you cannot share subnets across workspaces, you may want subnets that do not use the total VNet address space.\n\n- You must allocate address space for two new subnets that are within the VNet's address space and don't overlap address space of current or future subnets in that VNet.\n\nThe following table shows maximum subnet size based on network size. This table assumes no additional subnets exist that take up address space. Use smaller subnets if you have pre-existing subnets or if you want to reserve address space for other subnets:\n\n.. list-table::\n  :header-rows: 1\n\n  * - VNet address space (CIDR)\n    - Maximum <Databricks> subnet size (CIDR) assuming no other subnets\n  * - `/16`\n    - `/17`\n  * - `/17`\n    - `/18`\n  * - `/18`\n    - `/19`\n  * - `/20`\n    - `/21`\n  * - `/21`\n    - `/22`\n  * - `/22`\n    - `/23`\n  * - `/23`\n    - `/24`\n  * - `/24`\n    - `/25`\n\nTo find the maximum cluster nodes based on the subnet size, use the following table. The IP addresses per subnet column includes the [five Azure-reserved IP addresses](https://learn.microsoft.com/azure/virtual-network/virtual-networks-faq#are-there-any-restrictions-on-using-ip-addresses-within-these-subnets). The rightmost column indicates the number of cluster nodes that can simultaneously run in a workspace that is provisioned with subnets of that size.\n\n.. list-table::\n  :header-rows: 1\n\n  * - Subnet size (CIDR)\n    - IP addresses per subnet\n    - Maximum <Databricks> cluster nodes\n  * - `/17`\n    - 32768\n    - 32763\n  * - `/18`\n    - 16384\n    - 16379\n  * - `/19`\n    - 8192\n    - 8187\n  * - `/20`\n    - 4096\n    - 4091\n  * - `/21`\n    - 2048\n    - 2043\n  * - `/22`\n    - 1024\n    - 1019\n  * - `/23`\n    - 512\n    - 507\n  * - `/24`\n    - 256\n    - 251\n  * - `/25`\n    - 128\n    - 123\n  * - `/26`\n    - 64\n    - 59\n\n<a id=\"vnet-inject-portal\"></a>\n\n## Create <a Databricks> workspace using Azure portal\n\nThis section describes how to create <a Databricks> workspace in the Azure portal and deploy it in your own existing VNet. <Databricks> updates the VNet with two new subnets if those do not exist yet, using CIDR ranges that you specify. The service also updates the subnets with a new network security group, configuring inbound and outbound rules, and finally deploys the workspace to the updated VNet. For more control over the configuration of the VNet, use Azure-Databricks-supplied Azure Resource Manager (ARM) templates instead of the portal UI. For example, use existing network security groups or create your own security rules. See  [_](#vnet-inject-advanced).\n\n.. important:: The user who creates the workspace must be assigned the [Network contributor role](https://learn.microsoft.com/azure/role-based-access-control/built-in-roles#network-contributor) or a [custom role](https://learn.microsoft.com/azure/role-based-access-control/custom-roles) that's assigned the `Microsoft.Network/virtualNetworks/subnets/join/action` action.\n\nYou must [configure a VNet](https://learn.microsoft.com/azure/virtual-network/) to which you will deploy the <Databricks> workspace. You can use an existing VNet or create a new one, but the VNet must be in the same region and same subscription as the <Databricks> workspace that you plan to create. The VNet must be sized with a CIDR range between /16 and /24. For more requirements, see [_](#vnet-inject-reqs).\n\nYou can either use existing subnets or specify names and IP ranges for new subnets when you configure your workspace.\n\n1. In the Azure portal, select **+ Create a resource > Analytics > Azure Databricks** or search for Azure Databricks and click **Create** or **+ Add** to launch the <Databricks> Service dialog", "timestamp": 0, "next": ""}}], "question": "What are the requirements for the virtual network when deploying a Databricks workspace?", "gold_answer": "The requirements for the virtual network when deploying a Databricks workspace are as follows:\n\n- **Region:** The virtual network (VNet) must be in the same region as the Databricks workspace.\n\n- **Subscription:** The VNet must be in the same subscription as the Databricks workspace.\n\n- **Address space:** The VNet should have a CIDR block between `/16` and `/24`, and it should include two subnets with a CIDR block up to `/26`: a container subnet and a host subnet.\n\n- **Subnets:** The VNet must have two subnets dedicated to the Databricks workspace: a container subnet and a host subnet. For workspaces that use secure cluster connectivity, both subnets are considered private. It is not supported to share these subnets with other workspaces or to deploy other Azure resources on them."}
{"hard_neg_docs": [{"page_content": "To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\nTo support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}, {"page_content": ".. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\n.. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\n---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}], "question": "Can I enable the compliance security profile at the account level for all workspaces?", "gold_answer": "Yes, you can enable the compliance security profile at the account level for all workspaces. If you apply the compliance security profile to your account, all existing and future workspaces in the account will use the security profile."}
{"hard_neg_docs": [{"page_content": "To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\nTo support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}, {"page_content": ".. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\n.. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements", "fmt_content": "Source: https://docs.databricks.com/security/privacy/security-profile.html\nContent:\n---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements", "metadata": {"source": "https://docs.databricks.com/security/privacy/security-profile.html", "seq_num": 520, "description": "", "text": "---\ndescription: Learn about Databricks the compliance security profile.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# Enable the compliance security profile\n\nIf <a Databricks> workspace has the _compliance security profile_ enabled, the workspace has additional features and controls. The profile enables additional monitoring, enforced instance types for inter-node encryption, a hardened compute image, and other features. For details, see [_](#features).\n\nThe compliance security profile includes controls that help meet certain security requirements in some compliance standards. However, you can choose to enable the compliance security profile for its enhanced security features without the need to conform to any compliance standard.\n\nEnabling the compliance security profile is required to use <Databricks> to process data that is regulated under the following compliance standards:\n\n- [PCI-DSS](pci.md)\n\n- [HIPAA](hipaa.md)\n\n- [FedRAMP Moderate](fedramp.md)\n\nChoose how you want to enable the compliance security profile:\n\n- **Account level**: You can choose to apply the compliance security profile to your account, in which case all existing and future workspaces in the account use the security profile.\n\n- **Workspace level**: You can specify which workspaces for which security profiles are enabled.\n\n<!--  FUTURE: FEDRAMP WOULD BE LISTED HERE TOO BUT IT IS GOING\n TO BE SECRET FOR LONGER SO OMITTED FOR NOW,\n SINCE WE ARE PREPARING TO UN-HIDE THIS PAGE\n-->\n\n## Requirements\n\n- Your Databricks account must include the Enhanced Security and Compliance add-on. For details, see the [pricing page](https://databricks.com/product/aws-pricing).\n\n- Your Databricks workspace is on the E2 version of the platform.\n\n- Your Databricks workspace is on the Enterprise tier.\n\n- [Single sign-on (SSO)](/administration-guide/account-settings-e2/single-sign-on/index.md) authentication is configured for the workspace.\n\n<a id=\"enable\"></a>\n\n## Enable the compliance security profile\n\n#. Prepare any existing workspaces that will use the security profile. See [_](#prepare).\n\n#. Contact your Databricks representative and request adding the compliance security profile at the account level or just for some workspaces.\n\n   If you want to enable it just for some workspaces, send the list of workspace IDs for the workspaces that you would like to use for the profile. Get a workspace ID from the URL when you are using the workspace. Look for `o=` in the URL. The number after `o=` is the <Databricks> workspace ID. For example, if the URL is `https://<databricks-instance>/?o=6280049833385130`, the workspace ID is `6280049833385130`.\n\n#. Wait for confirmation that the profile is now enabled.\n\n#. If any clusters or SQL warehouses were running, restart them. If you have many clusters running and only want to restart the ones that were started before enablement, you can use a script that <Databricks> provides that [determines for all clusters if the start time was before the enablement date](#cluster-start-date-script).\n\n   Setup is complete. Create or use <Databricks> compute resources as desired.\n\nRemember that if you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. Databricks recommends that admins regularly restart clusters before they run for 25 days and do so during a scheduled maintenance window. This reduces the risk of an auto-restart disrupting a scheduled job. You can use a script that <Databricks> provides that can determine how long your clusters have been running, and optionally restart them. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n<a id=\"prepare\"></a>\n\n## Prepare a workspace for the compliance security profile\n\nSome steps are necessary to prepare a workspace for the compliance security profile. If you have not yet enabled the security profile, do these steps **before** requesting to [enable the security profile](#enable).\n\nIf the security profile is already enabled at an account level and you create any new workspaces, you must do these steps after you create any new workspace.\n\n#. If you enable the [compliance security profile](/security/privacy/security-profile.md) for your account or your workspace, long-running clusters are automatically restarted after 25 days. If any clusters were running 25 days or longer when the compliance security profile is enabled, the clusters will be immediately restarted, which will cause any running jobs to fail. Instead, check for long-running clusters before you enable the security profile. This reduces the risk of an auto-restart disrupting a scheduled job.\n\n   .. important:: Check how long your clusters have been running and restart any that have been running longer than 20 days (not 25 days) to reduce the risk of clusters being auto-restarted after 25 days running when the security profile is enabled. See [_](/clusters/clusters-manage.md#cluster-update-restart).\n\n#. Configure [Single sign-on (SSO) authentication](/administration-guide/account-settings-e2/single-sign-on/index.md).\n\n#. Add required network ports.\n\n   - **For workspaces with PrivateLink back-end connectivity**: You must make a change to support FIPS encryption if the workspace uses a [PrivateLink](/administration-guide/cloud-configurations/aws/privatelink.md) back-end connection for private connectivity between the Classic data plane in your AWS account and the <Databricks> control plane in the Databricks account.\n\n     One of the [networking requirements for PrivateLink back-end connections](/administration-guide/cloud-configurations/aws/privatelink.md#requirements) is to create a separate security group for the endpoint that allows HTTPS/443 and TCP/6666 with bidirectional access (from and to) for both the workspace subnets and the endpoint subnet itself. This configuration allows access for both REST APIs (port 443) and secure cluster connectivity (6666). You can then use the security group for both purposes.\n\n     To support the upcoming changes for FIPS encryption, update your network security group to **additionally** allow bidirectional access to port 2443 for FIPS connections. The total set of ports to allow bidirectional access are 443, 2443, and 6666.\n\n   - **For workspaces with no PrivateLink back-end connectivity**: If the workspace does not use a PrivateLink back-end connection for private connectivity but the workspace is configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints.\n\n     To support the upcoming changes for FIPS encryption, update your network security group (or firewall) to allow outbound access from the data plane to the control plane on port 2443 for FIPS connections. This is in addition to outgoing port 443 access that you are required to allow already. For related information about related security group and firewall configuration for customer-managed VPCs, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#security-groups) and [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). <!--  Wed, Jun 22, 2022 nuance here... we don't REQUIRE customer-managed VPC to use sec profile,.... and HISTORICALLY we only documented limiting egress for customer-managed VPC, but recently there started to be some interest in having us more generally support it, which means removing references to \"limiting egress\" being a feature of customer-managed VPCs on AWS specifically -- we do NOT make that claim on GCP - some PM discussions were started on maybe going that direction for PM/field/docs spin for AWS    -->\n\n#. If any workspace is in the US East Region, the US West Region, or Canada (Central) Region, and it's configured to restrict outbound network access, you need to allow traffic to additional endpoints to support FIPS endpoints. Remember that if you use those regions and do not restrict outgoing access now, if you restrict outgoing access in the future, you will need to revisit this step.\n\n   For the S3 service only, you must ensure that your Classic data plane network in your AWS account allows outgoing traffic to the AWS endpoints for the cloud services for S3 and also the FIPS variant of the S3 service with the prefix `s3-fips`. This applies to the S3 service but not to STS and Kinesis endpoints.\n\n   - For S3, allow outgoing traffic to the endpoint `s3.<region>.amazonaws.com` and `s3-fips.<region>.amazonaws.com`. For example `s3.us-east-1.amazonaws.com` and `s3-fips.us-east-1.amazonaws.com`.\n\n   - For STS, allow outgoing traffic to the endpoint `sts.<region>.amazonaws.com`.\n\n   - For Kinesis, allow outgoing traffic to the endpoint `kinesis.<region>.amazonaws.com`.\n\n#. For every workspace that uses the profile, run the following tests to verify that the changes were correctly applied:\n\n   a. Launch <a Databricks> cluster with 1 driver and 1 worker, any DBR version, and any instance type.\n\n   #. Create a notebook attached to the cluster. Use this cluster for the following tests.\n\n   #. In the notebook, validate DBFS connectivity by running:\n\n      ```bash\n      %fs ls /\n      %sh ls /dbfs\n      ```\n\n      Confirm that a file listing appears without errors.\n\n   #. In the notebook, confirm access to the control plane instance for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the Webapp endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <webapp-domain-name> 443\n      ```\n\n      For example, for VPC region `us-west-2`:\n\n      ```bash\n      %sh nc -zv oregon.cloud.databricks.com 443\n      ```\n\n      Confirm the result says it succeeded.\n\n   #. In the notebook, confirm access to the SCC relay for your region. Get the address from the table [this section](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#allow-required-addresses) and look for the SCC relay endpoint for your VPC region.\n\n      ```bash\n      %sh nc -zv <scc-relay-domain-name> 2443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv tunnel.cloud.databricks.com 2443\n      ```\n\n      Confirm that the results says it succeeded.\n\n   #. In the notebook, confirm access to the S3, STS, and Kinesis FIPS endpoints for your region.\n\n      .. note:: For this step, FIPS endpoints for Canada apply only to the S3 service. AWS does not yet provide FIPS endpoints for STS and Kinesis.\n\n      ```bash\n      %sh nc -zv <bucket-name>.s3-fips.<region>.amazonaws.com 443\n      %sh nc -zv sts.<region>.amazonaws.com 443\n      %sh nc -zv kinesis.<region>.amazonaws.com 443\n      ```\n\n      For example, for VPC region `us-west-1`:\n\n      ```bash\n      %sh nc -zv acme-company-bucket.s3-fips.us-west-1.amazonaws.com 443\n      %sh nc -zv sts.us-west-1.amazonaws.com 443\n      %sh nc -zv kinesis.us-west-1.amazonaws.com 443\n      ```\n\n      Confirm the results for all three commands indicate success.\n\n   #. In the same notebook, validate that the cluster Spark config points to the desired endpoints. For example:\n\n      ```", "timestamp": 0, "next": ""}}], "question": "What are some additional features and controls enabled when the compliance security profile is enabled in a Databricks workspace?", "gold_answer": "When the compliance security profile is enabled in a Databricks workspace, it includes additional features and controls such as:\n\n- Additional monitoring\n- Enforced instance types for inter-node encryption\n- A hardened compute image\n- Other unspecified features that help meet certain security requirements in some compliance standards"}
