{"hard_neg_docs": [{"page_content": "## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I share data securely with any Databricks user using Databricks-to-Databricks (DS)?", "gold_answer": "You can share data securely with any Databricks user using the Databricks-to-Databricks (DS) protocol as long as the user has access to a workspace enabled for Unity Catalog. If you are on Databricks on Google Cloud Platform (GCP), note that DS is in Private Preview, and you would need to reach out to your Databricks contact to try it. For more detailed information on how to use DS to share data and an overview of the workflow, you should reference the Databricks documentation, particularly the sections that discuss data sharing using the DS protocol. If you need to share data with users who don't have access to a workspace enabled for Unity Catalog, or if you are a data recipient, you should consult the additional links provided in the documentation."}
{"hard_neg_docs": [{"page_content": "## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow", "fmt_content": "Source: https://docs.databricks.com/data-sharing/share-data-databricks.html\nContent:\n---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow", "metadata": {"source": "https://docs.databricks.com/data-sharing/share-data-databricks.html", "seq_num": 1566, "description": "", "text": "---\ndescription: Learn how to share data securely with any Databricks user, regardless of account or cloud host, using Databricks-to-Databricks <DS> and <UC>.\n---\n\n# Share data using the <DS> Databricks-to-Databricks protocol\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <DS> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article gives an overview of how to use Databricks-to-Databricks <DS> to share data securely with any Databricks user, regardless of account or cloud host, as long as that user has access to a workspace enabled for [Unity Catalog](/data-governance/unity-catalog/index.md).\n\n.. note:: If you want to learn how to share data with users who don't have access to a Databricks workspace that is enabled for <UC>, see [_](/data-sharing/share-data-open.md). If you are a data recipient (a user or group of users with whom Databricks data is being shared), see [_](/data-sharing/recipient.md).\n\nFor an introduction to <DS> and a comparison of Databricks-to-Databricks sharing with open sharing, see [_](/data-sharing/index.md).\n\n## Databricks-to-Databricks <DS> workflow\n\nThis section provides a high-level overview of the Databricks-to-Databricks sharing workflow, with links to detailed documentation for each step.\n\nIn the Databricks-to-Databricks <DS> model:\n\n#. A data _recipient_ gives a data _provider_ the unique _sharing identifier_ for the Databricks <UC> metastore that is attached to the Databricks workspace that the recipient (which represents a user or group of users) will use to access the data that the data provider is sharing.\n\n   For details, see [_](create-recipient.md#request-uuid).\n\n#. The data provider creates a _share_ in the provider's <UC> metastore. This named object contains a collection of tables registered in the metastore.\n\n   For details, see [_](create-share.md).\n\n#. The data provider creates a recipient object in the provider's <UC> metastore. This named object represents the user or group of users who will access the data included in the share, along with the sharing identifier of the <UC> metastore that is attached to the workspace that the user or group of users will use to access the share. The sharing identifier is the key identifier that enables the secure connection.\n\n   For details, see [_](create-recipient.md#create-recipient-db-to-db).\n\n#. The data provider grants the recipient access to the share.\n\n   For details, see [_](grant-access.md).\n\n#. The share becomes available in the recipient's Databricks workspace, and users can access it using Data Explorer, the Databricks CLI, or SQL commands in <a Databricks> notebook or the <DBSQL> query editor.\n\n   To access the tables in a share, a metastore admin or [privileged user](read-data-databricks.md#access-data) must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in <UC>, with the important distinction being that users can be granted only read access on objects in catalogs that are created from <DS> shares.\n\n   For details, see [_](read-data-databricks.md).\n\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of Databricks-to-Databricks (DS) protocol?", "gold_answer": "The purpose of the Databricks-to-Databricks (DS) protocol is to allow for the secure sharing of data with any Databricks user, regardless of their account or cloud host, as long as they have access to a workspace that is enabled for Unity Catalog."}
{"hard_neg_docs": [{"page_content": ".. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}, {"page_content": "* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the \"useAzureMSI\" option in the Spark - Synapse Analytics connector?", "gold_answer": "The purpose of the \"useAzureMSI\" option in the Spark - Synapse Analytics connector is to be used as a means of authentication in place of \"forward_spark_azure_storage_credentials\" when Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account. It is part of the setup for VNet + Service Endpoints in the Spark - Synapse Analytics connector."}
{"hard_neg_docs": [{"page_content": ".. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}, {"page_content": "* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.", "fmt_content": "Source: https://docs.databricks.com/migration-notes/dbr5.5-12.2.html\nContent:\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.", "metadata": {"source": "https://docs.databricks.com/migration-notes/dbr5.5-12.2.html", "seq_num": 1357, "description": "", "text": "---\ndescription: Release notes from <DBR> 5.5 to 12.2.\norphan: 1\nrobots: noindex,  nofollow\n---\n\n# Migration support: release notes <DBR> 5.5 to 12.2\n\n.. include:: /shared/replacements.md\n\n.. important:: This experimental article was generated programmatically to help customers find pertinent information from release notes while upgrading <DBR> versions. See [_](/release-notes/runtime/index.md) for official release notes.\n\n\n## <DBR> 5.5 features\n\n## <Delta> on <Databricks> Auto Optimize (5.5)\n\nToday, when you write data into cloud storage, you need to compact files for optimal I/O performance. You have to worry about the right file size, how often to compact the files, how big a cluster to use, and so on. To address this class of problems, we are excited to announce the general availability of Auto Optimize with <Delta> on <Databricks>. During every write to Delta tables, we automatically figure out the right file sizes and compact files so that you don't have to worry about optimizing the storage layout. During writes, if the `auto-optimize` option is `true`, <Databricks> automatically determines if an optimize is required and optimizes the small files. For details, see [_](/optimizations/auto-optimize.md).\n\n## <Delta> on <Databricks> improved min, max, and count aggregation query performance (5.5)\n\nThe performance of min, max, and count aggregation queries for <Delta> on <Databricks> has been significantly improved by reducing the amount of data that's read. These queries are now run using statistics and partition values in the metadata, rather than scanning the data.\n\n.. aws::\n    ## Presto and Athena support for Delta tables on AWS S3 (Public Preview) (5.5)\n\n    You can now query Delta tables from external tools such as Presto and Athena. Presto and Athena support reading from external tables when the list of data files to process is read from a manifest file, which is a text file containing the list of data files to read for querying a table. When an external table is defined in the Hive metastore using manifest files, Presto and Athena use the list of files in the manifest rather than finding the files by directory listing.\n\n    ## Glue Catalog as Databricks metastore (5.5)\n\n    Glue Catalog support is generally available. This feature lets you configure <DBR> to use the AWS Glue Data Catalog as its metastore, which can serve as a drop-in replacement for an  [external Hive metastore](/data/metastores/external-hive-metastore.md#external-hive-metastore). It also enables multiple <Databricks> workspaces to share the same metastore. For more information and limitations, see [_](/data/metastores/aws-glue-metastore.md).\n\n    .. admonition::  Preview\n        :class: preview\n\n        [Credential passthrough](https://databricks.com/blog/2019/03/26/introducing-databricks-aws-iam-credential-passthrough.html) for Glue Catalog is in [Private Preview](/release-notes/release-types.md#runtime-releases). Contact support if you'd like to try this out.\n\n## Faster model inference pipelines with improved binary file data source and scalar iterator pandas UDF (Public Preview) (5.5)\n\nMachine learning tasks, especially in the image and video domain, often have to operate on a large number of files. In <DBR> 5.4, we already made available the binary file data source to help ETL arbitrary files such as images, into Spark tables. In <DBR> 5.5, we have added an option, `recursiveFileLookup`, to load files recursively from nested input directories. See [_](/external-data/binary.md).\n\nThe binary file data source enables you to run model inference tasks in parallel from Spark tables using a scalar pandas UDF. However, you might have to initialize the model for every record batch, which introduces overhead. In <DBR> 5.5, we backported a new pandas UDF type called \"scalar iterator\" from <AS> master. With it you can initialize a model only once and apply the model to many input batches, which can result in a 2-3x speedup for models like ResNet50. See [_](/udf/pandas.md#series-to-scalar-udf).\n\n## Secrets API in R notebooks (5.5)\n\nThe [Secrets API](/dev-tools/api/latest/secrets.md) lets you inject secrets into notebooks without hardcoding them. This API is now available in R notebooks in addition to existing support for Python and Scala notebooks. You can use the `dbutils.secrets.get` function to obtain secrets. Secrets are redacted before printing to a notebook cell.\n\n## Improvements (5.5)\n\n* Support for running <Delta> SQL operations in Python `foreachBatch`: We have fixed the known limitation of not being able to write to Delta tables from inside `foreachBatch` of a Structured Streaming query defined in Python. This is useful in a common Python streaming workloads; for example, [Writing streaming aggregates in update mode using MERGE and foreachBatch](/structured-streaming/delta-lake.md#merge-in-streaming).\n* Performance of Delta tables stored in Azure Data Lake Gen2: The check for the latest version of a Delta table on ADLS Gen2 now only checks the end of the transaction log, rather than listing all available versions. This optimization makes `UPDATE` a constant time operation, significantly improving the latency.\n* Scalability of Optimize `ZORDER BY`: Z-Ordering on very large Delta tables now uses smaller units of work that are governed by advanced admission control. This feature improves the stability of this operation without sacrificing cluster utilization.\n* Improved performance of DML commands on tables with large number of columns: We now perform better column pruning when scanning for matching data in `UPDATE`, `DELETE`, and `MERGE` commands.\n* Support for VNet + Service Endpoints setup in Spark - Synapse Analytics connector: We added ADL Gen2 paths to an allow list as temp data locations (`.option(\"tempDir\", \"abfss://...\"`)) and added a new option called `useAzureMSI`, to be used instead of `forward_spark_azure_storage_credentials` in case your [Synapse Analytics is configured to authenticate via Managed Identities to your V2 storage account](/external-data/synapse-analytics.md).\n* Automatic invalidation for disk caching: Disk caching now automatically detects files that have been modified or overwritten after being cached. Any stale entries are automatically invalidated and evicted from the cache. See [_](/optimizations/disk-cache.md).\n* Upgraded Python library wheel from 0.33.3 to 0.33.4.\n* Upgraded R library nlme from 3.1-139 to 3.1-140.\n\n## Bug fixes (5.5)\n\n.. aws::\n  * Fixed a bug that the S3-SQS/ABS-AQS connector may hit a corrupt Delta table error when a file gets removed and re-added.\n\n* Fixed cancellation for R commands that do not run Spark jobs. Previously, R commands that do not run Spark jobs could be cancelled, but the notebooks’ state would be lost; commands can now be cancelled without losing notebook state.\n* Dropping or moving a managed table now invalidates the cached <Delta> log.\n* Fixed a bug that writing <Delta> checkpoint may fail due to `FileAlreadyExistsException`.\n* Scala REPL now sets the proper `-target:jvm-1.8` flag to support calling Java methods that use Java 8 features.\n\n\n\n## <DBR> 6.0 features\n\n\n\n## Python environment (6.0)\n\n<DBR> 6.0 includes major changes to Python and the way Python environments are configured, including upgrading Python to [3.7.3](https://docs.python.org/release/3.7.3/), refining the list of installed Python packages, and upgrading those installed packages to newer versions. \n\nIn addition, as was previously [announced](/release-notes/product/2019/july.md), <DBR> 6.0 does not support Python 2.\n\nMajor changes include:\n\n* Upgraded Python from 3.5.2 to 3.7.3. Some old versions of Python packages may not be forward compatible with Python 3.7 because they depend on old versions of Cython that are not compatible with Python 3.7. Installing such a package can trigger errors similar to `‘PyThreadState’ {‘struct _ts’} has no member named ‘exc_type’` (see [GitHub issue 1978](https://github.com/cython/cython/issues/1978) for details). Install Python 3.7 compatible versions of Python packages instead.\n* Major package upgrades:\n  * boto3 to 1.9.162\n  * ipython to 7.4.0\n  * matplotlib to 3.0.3\n  * numpy to 1.16.2\n  * pandas to 0.24.2\n  * pyarrow to 0.13.0\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are newly included: asn1crypto, backcall, jedi, kiwisolver, parso, and PySocks.\n* Compared with [_](/release-notes/runtime/5.5.md), the following Python packages are not installed: ansi2html, brewer2mpl, colorama, configobj, enum34, et-xmlfile, freetype-py, funcsigs, fusepy, ggplot, html5lib, ipaddress, jdcal, Jinja2, llvmlite, lxml, MarkupSafe, mpld3, msgpack-python, ndg-httpsclient, numba, openpyxl, pathlib2, Pillow, ply, pyasn1, pypng, python-geohash, scour, simplejson, and singledispatch.\n* The `display` function on Python ggplot objects is no longer supported because the ggplot package is not compatible with newer version of pandas.\n* Setting `PYSPARK_PYTHON` to `/databricks/python2/bin/python` is not supported because <DBR> 6.0 does not support Python 2. A cluster with such setting can still start. However, Python notebooks and Python commands will not work, that is, Python command cells will fail with a \"Canceled\" error and a `Python shell failed to start` error will appear in the driver logs.\n* If `PYSPARK_PYTHON` points to a Python executable that is in an environment managed by [Virtualenv](https://virtualenv.pypa.io/en/latest/) , this environment will be activated for <Virtual Env Support>. You can use `python` and `pip` commands that are defined in the activated environment directly without having to specify the absolute locations of these commands. By default, `PYSPARK_PYTHON` is set to `/databricks/python3/bin/python`. Thus, by default, `python` points to `/databricks/python3/bin/python` and `pip` points to `/databricks/python3/bin/pip` for <Virtual Env Support>. If your `PYSPARK_PYTHON` points to a Python executable that is not in an environment managed by Virtualenv or if you are writing an init script to create the Python specified by `PYSPARK_PYTHON`, you will need to use absolute paths to access the correct `python` and `pip`. When [Python library isolation](/dev-tools/databricks-utils.md#dbutils-library) is enabled (it is enabled by default), the activated environment is still the environment that `PYSPARK_PYTHON` is associated with. We recommend that you use [_](/dev-tools/databricks-utils.md#dbutils-library) to modify the isolated environment associated with a Python notebook.\n\n.. aws::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md), SSH sessions, and notebooks\n\n.. azure::\n    .. <Virtual Env Support> replace:: [init scripts](/clusters/init-scripts.md) and notebooks\n\n## Scala and Java APIs for <Delta> DML commands (6.0)\n\nYou can now modify data in Delta tables using programmatic APIs for delete, update, and merge. These APIs mirror the syntax and semantics of their corresponding SQL commands and are great for many workloads, for example, slowly changing dimension (SCD) operations, merging change data for replication, and upserts from streaming queries.\n\nFor details,", "timestamp": 0, "next": ""}}], "question": "How does disk caching handle modified or overwritten files?", "gold_answer": "Disk caching in Databricks now automatically detects files that have been modified or overwritten after being cached. When such changes are detected, any stale entries are automatically invalidated and evicted from the cache."}
{"hard_neg_docs": [{"page_content": "![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are the steps to connect to Workbench/J?", "gold_answer": "The steps to connect to Workbench/J are:\n\n1. Launch SQL Workbench/J.\n2. Select **File > Connect window**.\n3. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   - In the Name field, type `Spark JDBC`.\n   - In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   - Verify that the **Classname** field is populated.\n4. Click the **Create a new connection profile** icon (depicted as <Connection Profile Icon> in the text).\n   - Type a name for the profile.\n   - In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   - In the **URL** field, enter the URL you constructed in Step 1.\n   - In the **Username** field, enter `token`.\n   - In the **Password** field, enter your personal access token from Step 1.\n5. Click **Test** to test the connection."}
{"hard_neg_docs": [{"page_content": "![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.", "fmt_content": "Source: https://docs.databricks.com/partners/bi/workbenchj.html\nContent:\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.", "metadata": {"source": "https://docs.databricks.com/partners/bi/workbenchj.html", "seq_num": 1745, "description": "", "text": "---\ndescription: Learn how to connect your <Databricks> workspace to SQL Workbench/J so you can run MySQL queries against Databricks data.\n---\n\n<a id=\"workbenchj\"></a>\n\n# Connect to SQL Workbench/J\n\nThis article describes how to use [SQL Workbench/J](https://www.sql-workbench.eu/) with <Databricks>.\n\n.. include:: /shared/dev-tools/unsupported-third-party.md\n\n.. <product-name> replace:: SQL Workbench/J\n.. <contact-url> replace:: use the [SQL Workbench/J support forum](https://groups.google.com/g/sql-workbench) in Google Groups.\n\n<a id=\"requirements\"></a>\n\n## Requirements\n\n- [SQL Workbench/J](https://www.sql-workbench.eu/downloads.html).\n\n.. include:: /shared/manual-partner-requirements.md\n\n## Steps to connect to Workbench/J\n\nTo connect to Workbench/J, do the following:\n\n#. Launch SQL Workbench/J.\n#. Select **File > Connect window**.\n#. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n   a. In the Name field, type `Spark JDBC`.\n   #. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   #. Verify that the **Classname** field is populated.\n\n      ![Spark JDBC driver](/_static/images/third-party-integrations/workbenchj/workbenchj-driver.png)\n\n   #. Click **OK**.\n#. Click the **Create a new connection profile** <Connection Profile Icon> icon.\n   a. Type a name for the profile.\n   #. In the Driver field, select **com.simba.spark.jdbc.Driver**.\n   #. In the **URL** field, enter the URL you constructed in Step 1.\n   #. In the **Username** field, enter `token`.\n   #. In the **Password** field, enter your personal access token from Step 1.\n\n      ![Connection profile](/_static/images/third-party-integrations/workbenchj/workbenchj-profile.png)\n\n   #. Click **Test**.\n\n      ![Connection test](/_static/images/third-party-integrations/workbenchj/workbenchj-test.png)\n\n\n   #. Click **OK** twice.\n\n.. <Connection Profile Icon> ![Connection Profile Icon](/_static/images/third-party-integrations/workbenchj/workbenchj-new-connection-profile.png)\n\n<a id=\"additional-resources\"></a>\n\n## Additional resources\n\n[Support](https://groups.google.com/g/sql-workbench)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I configure the Spark JDBC driver in Workbench/J?", "gold_answer": "To configure the Spark JDBC driver in Workbench/J, follow these steps:\n\n1. Launch SQL Workbench/J.\n2. Select **File > Connect window**.\n3. In the **Select Connection Profile** dialog, click **Manage Drivers**.\n4. In the dialog that appears, do the following:\n   a. In the Name field, type `Spark JDBC`.\n   b. In the Library field, click the **Select the JAR file(s)** icon. Browse to the directory where you downloaded the Simba Spark JDBC driver JAR.\n   c. Verify that the **Classname** field is populated with the correct driver class name.\n   d. Click **OK**.\n5. Click the **Create a new connection profile** icon (which may look like a plug or a small database icon with a plus sign, depending on the version of Workbench/J).\n6. In the new connection profile setup, do the following:\n   a. Type a name for the profile.\n   b. In the Driver field, select **com.simba.spark.jdbc.Driver** (which should be available after completing step 4).\n   c. In the **URL** field, enter the JDBC URL you constructed previously.\n   d. In the **Username** field,"}
{"hard_neg_docs": [{"page_content": "For a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}, {"page_content": "## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Your network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}], "question": "What network troubleshooting steps can be performed to troubleshoot connectivity issues with Databricks control plane?", "gold_answer": "To troubleshoot connectivity issues with the Databricks control plane, you can perform the following network troubleshooting steps:\n\n- Deploy an EC2 instance into one of the workspace subnets.\n- Use network troubleshooting commands like `nc` (netcat), `ping`, `telnet`, `traceroute`, etc.\n- Check the Relay CNAME for the specific region as mentioned in the customer-managed VPC article.\n- Ensure there is a successful networking path to S3 for Artifact Storage.\n- Verify access to the web application, secure cluster connectivity relay, and S3 global and regional access using the `nc` command with the appropriate domain names and ports, as shown in the example for the AWS region `eu-west-1`."}
{"hard_neg_docs": [{"page_content": "For a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}, {"page_content": "## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Your network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443", "fmt_content": "Source: https://docs.databricks.com/administration-guide/workspace/troubleshooting.html\nContent:\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443", "metadata": {"source": "https://docs.databricks.com/administration-guide/workspace/troubleshooting.html", "seq_num": 1424, "description": "", "text": "---\ndescription: Learn about the most common <Databricks> E2 configuration errors during workspace creation and how to fix them.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Troubleshooting creating workspaces\n\n## Overview\n\nThe following sections describe configuration errors during workspace creation and how to fix the errors. Most issues apply to workspace creation using both the [account console](/administration-guide/workspace/create-workspace.md) or [Account API](/administration-guide/workspace/create-workspace-api.md), with exceptions as indicated.\n\n.. note:: This article describes the process for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture). All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n.. include:: /shared/serverless-data-plane-clarification.md\n\n## General errors\n\n### Maximum number of VPCs\n\nIf you get an error message that mentions the maximum number of VPCs, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of VPCs allowed in the region.  This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of VPC endpoints\n\nIf you get an error message that mentions a maximum number of VPC endpoints, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for the number of Gateway VPC Endpoints allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Maximum number of addresses\n\nIf you get an error message that mentions a maximum number of addresses, submit a [service limit increase request](https://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html) for VPC Elastic IP Addresses allowed in the region. This error typically happens only if you are using a Databricks-managed VPC, not a customer-managed VPC.\n\n### Not authorized to perform this operation\n\nIf you get an error that you are not authorized to perform this operation, check that your IAM role has all of the necessary policies, as defined in the [IAM role article](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n## Storage configuration error messages\n\n### Malformed request: Failed storage configuration validation checks\n\nIf you get an error message that mentions failed storage configuration validation checks, your S3 bucket permissions are not properly set up. Follow the steps in the article [_](/administration-guide/cloud-configurations/aws/aws-storage.md) to ensure that the S3 bucket permissions are correct.\n\n\n## Credentials configuration error messages\n\n### Malformed request: Failed credential configuration validation checks\n\nThe list of permissions checks in the error message indicate the likely cause of your problems.\n\n- If the credential configuration validation fails fewer than ten permission checks, it’s likely that your IAM policy is missing those specific permissions. Copy the correct policy from the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\n- If the credential configuration validation fails ten or more checks, it’s more likely that the trust relationship of the IAM role is incorrectly set up. Verify that the trust relationship of your customer role is set up properly according to instructions in the article [_](/administration-guide/cloud-configurations/aws/iam-role.md).\n\nIf both your policy and trust relationship appear to be correct, also check the following:\n\n- Confirm that you include the correct role ARN in the credentials object.\n\n- Confirm whether you have organization-level [service control policies](https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html) (SCPs) that deny the `AssumeRole` action or deny EC2/VPC access. If you are unsure, ask your AWS administrator about SCPs.\n\n## Network configuration\n\n### Subnet is already in use by another network\n\nA subnet in use error generally looks like the following:\n\n```text\nMALFORMED_REQUEST: Malformed parameters: subnet_id subnet-xxxxxxxx1 is already used by another Network, subnet_id subnet-xxxxxxxx2 is already used by another Network.\n```\n\nThis means that you have <a Databricks> network configuration that uses these same subnets. To resolve, do one of the following:\n\n- Delete the previous configuration. If you are using the Account API, use the [Delete network configuration API](https://docs.databricks.com/dev-tools/api/latest/account.html#operation/delete-network-config). You can also use the [account console](/administration-guide/account-settings/index.md#account-console) to delete the configuration.\n\n- If that previous configuration is not in use, you can use that previous configuration for your new workspace.\n\n- If that network configuration is already in use by a running workspace, create new subnets and a new network configuration for your new workspace.\n\nNote that if a previous attempt at workspace creation failed, the related configuration components are not automatically deleted.\n\n### No network configuration errors during setup, but errors appear during workspace creation\n\nA network configuration might show errors after attempting to deploy a workspace but it showed no error when you set it up. This is because <Databricks> performs only basic validations while creating the network object. For example, it checks for unique subnets, unique security groups, and missing fields.\n\nMost meaningful network configuration validation happens only after you attempt to create a new workspace with your new network configuration. If there were errors during workspace deployment, look closely at the network validation error message for details.\n\n### A new cluster does not respond or \"data plane network is misconfigured\" event log error\n\nAfter what seems like a successful workspace deployment, you might notice that your first test cluster does not respond. After approximately 20-30 minutes, check your [cluster event log](/clusters/clusters-manage.md#event-log). You might see a error message similar to:\n\n```text\nThe data plane network is misconfigured. Please verify that the network for your data plane is configured correctly. Error message: Node daemon ping timeout in 600000 ms ...\n```\n\nThis message indicates that routing or the firewall is incorrect. <Databricks> requested EC2 instances for a new cluster, but encountered a long time delay waiting for the EC2 instance to bootstrap and connect to the control plane. The cluster manager terminates the instances and reports this error.\n\nYour network configuration must allow cluster node instances to successfully connect to the <Databricks> [control plane](/getting-started/overview.md). For a faster troubleshooting technique than using a cluster, you can deploy an EC2 instance into one of the workspace subnets and do typical network troubleshooting steps like `nc`, `ping`, `telnet`, `traceroute`, etc. The Relay CNAME for each region is mentioned [in the customer-managed VPC article](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#firewall). For the Artifact Storage, ensure that there’s a successful networking path to S3.\n\nFor access domains and IPs by region, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses). For regional endpoints, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#regional-endpoints). The following example uses the AWS region `eu-west-1`:\n\n```bash\n# Verify access to the web application\nnc -zv ireland.cloud.databricks.com 443\n\n# Verify access to the secure cluster connectivity relay\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n\n# Verify S3 global and regional access\nnc -zv s3.amazonaws.com 443\nnc -zv s3.eu-west-1.amazonaws.com 443\n\n# Verify STS global and regional access\nnc -zv sts.amazonaws.com 443\nnc -zv sts.eu-west-1.amazonaws.com 443\n\n# Verify regional Kinesis access\nnc -zv kinesis.eu-west-1.amazonaws.com 443\n```\n\nIf these all return correctly, the networking could be configured correctly but there could be another issue if you are using a firewall. The firewall may have deep packet inspection, SSL inspection, or something else that causes <Databricks> commands to fail. Using an EC2 instance in the <Databricks> subnet, try the following:\n\n```bash\ncurl  -X GET -H 'Authorization: Bearer <token>' \\\nhttps://<workspace-name>.cloud.databricks.com/api/2.0/clusters/spark-versions\n```\n\nReplace `<token>` with your own personal access token and use the correct URL for your workspace. See [_](/dev-tools/api/latest/authentication.md).\n\nIf this request fails, try the `-k` option with your request to remove SSL verification. If this works with the `-k` option, then the firewall is causing an issue with SSL certificates.\n\nLook at the SSL certificates using the following and replace the domain name with the [control plane web application domain](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md#required-data-plane-addresses) for your region:\n\n```bash\nopenssl s_client -showcerts -connect oregon.cloud.databricks.com:443\n```\n\nThis command shows the return code and the Databricks certificates. If it returns an error, it’s a sign that your firewall is misconfigured and must be fixed.\n\nNote that [SSL issues are not a networking layer issue](https://security.stackexchange.com/questions/19681/where-does-ssl-encryption-take-place). Viewing traffic at the firewall will not show these SSL issues. Looking at source and destination requests will work as expected.\n\n### A workspace seems to work but its network configuration has status `WARNED`\n\nMake sure that you can start a cluster, run a data job, and that you don’t have `DBFS_DOWN` or `METASTORE_DOWN` showing in your [_](/clusters/clusters-manage.md#cluster-event-logs). If there are no such errors in the cluster event log, the `WARNED` status is not necessarily a problem.\n\nFor a new workspace, there are a number of things that <Databricks> attempts to check. If you do not do a simple routing like Workspace subnets → NAT Gateway → Internet Gateway, then <Databricks> cannot verify that your network is correct. In such cases, <Databricks> displays a warning on the network configuration.\n\n#### Check for subnet route table errors\n\nIn your cluster events log, you may see errors like:\n\n```text\nsubnet: Route Table with ID rtb-xxxxxxxx used for subnet with ID subnet-yyyyyyyyy is missing default route to direct all traffic to the NAT gateway nat-zzzzzzzzzzz.\n```\n\nThis error could actually indicate a problem if you are trying to deploy a simple <Databricks> workspace configuration.\n\nIf you do your own egress setup, like routing through a firewall (optionally via a Transit Gateway in a hub-spoke fashion), this error will not necessarily be meaningful.\n\nAnother potential reason for this error is you register a NAT subnet as <a Databricks> subnet for clusters. Remove the NAT subnet from the list of <Databricks> workspace subnets and re-create the workspace.\n\n#### Do not add your NAT subnet to the list of subnets in a network configuration\n\nDo not add your NAT subnet to the list of <Databricks> workspace subnets. The NAT subnet is for the NAT gateway and is not intended as a subnet for deployment of <Databricks> cluster nodes. When creating a network configuration, only list the two subnets to use for <Databricks> nodes.\n\n### Problems using your metastore or cluster event log includes `METASTORE_DOWN` events\n\nIf your workspace seems to be up and you can set up clusters, but you have `METASTORE_DOWN` events in your [cluster event logs](/clusters/clusters-manage.md#view-cluster-logs), or if your [metastore](/data/metastores/index.md) does not seem to work, confirm if you use a Web Application Firewall (WAF) like Squid proxy. Cluster members must connect to several", "timestamp": 0, "next": ""}}], "question": "How can I verify access to the web application and secure cluster connectivity relay in a specific AWS region?", "gold_answer": "To verify access to the web application and secure cluster connectivity relay in a specific AWS region, you can use the `nc` (netcat) command to test connectivity to the respective endpoints and ports. Here's how you can do it for the AWS region `eu-west-1`:\n\nFor the web application, you can test connectivity to the Databricks web application using the following command:\n```bash\nnc -zv ireland.cloud.databricks.com 443\n```\n\nFor the secure cluster connectivity relay, you can test connectivity to the Databricks relay service using the following command:\n```bash\nnc -zv tunnel.eu-west-1.cloud.databricks.com 443\n```\n\nThese commands check if your network can establish a connection to the specified hosts on port 443, which is the standard port for HTTPS traffic. If the connection is successful, it means the network configuration allows access to these Databricks services in the specified AWS region."}
{"hard_neg_docs": [{"page_content": "## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}, {"page_content": "### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}], "question": "What is the Privilege Model version 1.0 in <UC>?", "gold_answer": "The Privilege Model version 1.0 in <UC> (Unity Catalog) refers to the updated set of privileges and inheritance model that governs how permissions are managed for securable objects within Databricks on the Unity Catalog. If a user created their Unity Catalog metastore during the public preview before August 25, 2022, they have the option to upgrade to this Privilege Model version 1.0. Privileges in this model can be granted by a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. The model dictates the rules and capabilities for managing access to various resources in the Unity Catalog."}
{"hard_neg_docs": [{"page_content": "## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}, {"page_content": "### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).", "fmt_content": "Source: https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html\nContent:\n---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).", "metadata": {"source": "https://docs.databricks.com/data-governance/unity-catalog/manage-privileges/privileges.html", "seq_num": 1553, "description": "", "text": "---\ndescription: Learn about securable objects, privilege inheritance, and managing privilege types in <UC>.\n---\n\n# <UC> privileges and securable objects\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\nThis article describes the <UC> privilege model. To learn about how this model differs from the Hive metastore, see [_](/data-governance/unity-catalog/hive-metastore.md).\n\n.. azure-aws::\n\n  .. note::\n    This article refers to the <UC> privileges and inheritance model in Privilege Model version 1.0. If you created your <UC> metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [_](/data-governance/unity-catalog/manage-privileges/upgrade-privilege-model.md)\n\n## Manage privileges\n\nPrivileges can be granted by either a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object. You can manage privileges for metastore objects by using SQL commands, [_](/dev-tools/cli/unity-catalog-cli.md), or in Data Explorer. To learn how to use Data Explorer to manage privileges see, [_](/data/manage-permissions.md).\n\nTo manage privileges in SQL, you use [GRANT](/sql/language-manual/security-grant.md) and [REVOKE](/sql/language-manual/security-revoke.md) statements in a notebook or the <DBSQL> query editor, using the syntax:\n\n```sql\nGRANT privilege_type ON securable_object TO principal\n```\n\nWhere:\n\n- `privilege_type` is a [Unity Catalog privilege type](#privilege-types)\n- `securable_object` is a [securable object in the Unity Catalog](#securable-objects)\n- `principal` is a user, service principal (represented by its applicationId value), or group. You must enclose users, service principals, and group names with [special characters](/sql/language-manual/sql-ref-identifiers.md#delimited-identifiers) in backticks (`` ` ` ``). See [_](/sql/language-manual/sql-ref-principal.md).\n\nFor example, the following command grants a group named _finance-team_ access to create tables in schema named _default_ with the parent catalog named _main_:\n\n```sql\nGRANT CREATE TABLE ON SCHEMA main.default TO `finance-team`;\n```\n\nFor more information about granting privileges using SQL commands, see [_](/sql/language-manual/sql-ref-privileges.md).\n\nYou can also manage privileges by using the [Databricks Terraform](/dev-tools/terraform/index.md) provider and [databricks_grants](https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grants#storage-location-grants).\n\n<a id=\"securable-objects\"></a>\n\n## Securable objects in <UC>\n\nA securable object is an object defined in the <UC> metastore on which privileges can be granted to a principal. Securable objects in <UC> are hierarchical.\n\n![Unity Catalog object hierarchy](/_static/images/unity-catalog/object-hierarchy.png)\n\nThe securable objects are:\n\n- **METASTORE**: The top-level container for metadata. Each <UC> metastore exposes a three-level namespace (`catalog`.`schema`.`table`) that organizes your data.\n- **CATALOG**: The first layer of the object hierarchy, used to organize your data assets.\n- **SCHEMA**: Also known as databases, schemas are the second layer of the object hierarchy and contain tables and views.\n- **TABLE**: The lowest level in the object hierarchy, tables can be  _external_ (stored in external locations in your cloud storage of choice) or _managed_ tables (stored in a storage container in your cloud storage that you create expressly for <Databricks>).\n- **VIEW**: A read-only object created from one or more tables that is contained within a schema.\n- **EXTERNAL LOCATION**: An object that contains a reference to a storage credential and a cloud storage path that is contained within a <UC> metastore.\n- **STORAGE CREDENTIAL**: An object that encapsulates a long-term cloud credential that provides access to cloud storage that is contained within a <UC> metastore.\n- **FUNCTION**: A user-defined function that is contained within a schema.\n- **SHARE**: A logical grouping for the tables you intend to share using <DS>. A share is contained within a <UC> metastore.\n- **RECIPIENT**: An object that identifies an organization or group of users that can have data shared with them using <DS>. These objects are contained within a <UC> metastore.\n- **PROVIDER**: An object that represents an organization that has made data available for sharing using <DS>. These objects are contained within a <UC> metastore.\n\n<a id=\"inheritance\"></a>\n\n## Inheritance model\n\nSecurable objects in <UC> are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a <UC> metastore are not inherited.\n\nFor example, the following command grants the `SELECT` privilege on all tables and views in any schema in the catalog _main_ to the group _finance_:\n\n```sql\nGRANT SELECT ON CATALOG main TO finance;\n```\n\nSimilarly, you can perform the grants on a schema for a smaller scope of access:\n\n```sql\nGRANT SELECT ON SCHEMA main.default TO finance;\n```\n\nThe inheritance model provides an easy way to set up default access rules for your data. For example the following commands enable the machine learning team to create tables within a schema and read each other's tables:\n\n```sql\nCREATE CATALOG ml;\nCREATE SCHEMA ml.team_sandbox;\nGRANT USE_CATALOG ON CATALOG ml TO ml_team;\nGRANT USE_SCHEMA ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT CREATE TABLE ON SCHEMA ml.team_sandbox TO ml_team;\nGRANT SELECT ON SCHEMA ml.team_sandbox TO ml_team;\n```\n\n<a id=\"privilege-types\"></a>\n\n## Privilege types in <UC>\n\nThe following table summarizes which privilege types apply to each securable object in the catalog:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Securable\n    - Privileges\n  * - Metastore\n    - `CREATE CATALOG`, `CREATE EXTERNAL LOCATION`, `CREATE RECIPIENT`, `CREATE SHARE`, `CREATE PROVIDER`\n  * - Catalog\n    - `ALL PRIVILEGES`, `CREATE SCHEMA`, `USE CATALOG`\n\n      The following privilege types apply to securable objects within a catalog. You can grant these privileges at the catalog level to apply them to the pertinent current and future objects within the catalog.\n\n      `CREATE FUNCTION`, `CREATE TABLE`, `EXECUTE`, `MODIFY`, `SELECT`, `USE SCHEMA`\n  * - Schema\n    - `ALL PRIVILEGES`, `CREATE FUNCTION`, `CREATE TABLE`, `USE SCHEMA`\n\n      The following privilege types apply to securable objects within a schema. You can grant these privileges at the schema level to apply them to the pertinent current and future objects within the schema.\n\n      `EXECUTE`, `MODIFY`, `SELECT`\n  * - Table\n    - `ALL PRIVILEGES`, `SELECT`, `MODIFY`\n  * - View\n    - `ALL PRIVILEGES`, `SELECT`\n  * - External location\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`, `CREATE MANAGED STORAGE`\n  * - Storage credential\n    - `ALL PRIVILEGES`, `CREATE EXTERNAL LOCATION`, `CREATE EXTERNAL TABLE`, `READ FILES`, `WRITE FILES`\n  * - Function\n    - `ALL PRIVILEGES`, `EXECUTE`\n  * - Share\n    - `SELECT` (Can be granted to `RECIPIENT`)\n  * - Recipient\n    - None\n  * - Provider\n    - None\n\nWhen you manage privileges on a metastore, you do not include the metastore name in a SQL command. <UC> grants or revokes the privilege on the metastore attached to your workspace. For example, the following command grants a group named _engineering_ the ability to create a catalog in the metastore attached to the workspace:\n\n```sql\nGRANT CREATE CATALOG ON METASTORE TO engineering\n```\n\n### ALL PRIVILEGES\n\n**Applicable object types: `CATALOG`, `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`, `SCHEMA`, `FUNCTION`, `TABLE`, `VIEW`**\n\nUsed to grant or revoke all privileges applicable to the securable and its child objects without explicitly specifying them. This expands to all available privileges at the time permissions checks are made.\n\n.. note ::\n  This privilege is powerful when applied at higher levels in the hierarchy. For example, GRANT ALL PRIVILEGES ON CATALOG main TO `analysts` would give the analyst team all privileges on every object (schemas, tables, views, functions) in the catalog.\n\n### CREATE CATALOG\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a catalog in a <UC> metastore.\n\n### CREATE EXTERNAL LOCATION\n\n**Applicable object types: <UC> metastore, `STORAGE CREDENTIAL`**\n\nWhen applied to a storage credential, allows a user to create an external location using the storage credential. This privilege can also be granted to a user on the metastore to allow them to create an external location.\n\n### CREATE EXTERNAL TABLE\n\n**Applicable object types: `EXTERNAL LOCATION`, `STORAGE CREDENTIAL`**\n\nAllows a user to create external tables directly in your cloud tenant using an external location or storage credential. Databricks recommends granting this privilege on an external location rather than storage credential (since it's scoped to a path, it allows more control over where users can create external tables in your cloud tenant).\n\n### CREATE FUNCTION\n\n**Applicable object types: `SCHEMA`**\n\nAllows a user to create a function in the schema. Since privileges are inherited, `CREATE FUNCTION` can also be granted on a catalog, which allows a user to create a function in any existing or future schema in the catalog.\n\nThe user must also have the `USE CATALOG` on its parent catalog and `USE SCHEMA` on its parent schema.\n\n### CREATE MANAGED STORAGE\n\n**Applicable object types: `EXTERNAL LOCATION`**\n\nAllows a user to specify a location for storing managed tables at the catalog or schema level, overriding the default root storage for the metastore.\n\n### CREATE PROVIDER\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> provider object in the metastore. A provider identifies an organization or group of users that have shared data using <DS>. Provider creation is performed by a user in the recipient's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE RECIPIENT\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a <DS> recipient object in the metastore. A recipient identifies an organization or group of users that can have data shared with them using <DS>. Recipient creation is performed by a user in the provider's Databricks account. See [_](/data-sharing/index.md).\n\n### CREATE SCHEMA\n\n**Applicable object types: `CATALOG`**\n\nAllows a user to create a schema. The user must also have the `USE CATALOG` privilege on the catalog.\n\n### CREATE SHARE\n\n**Applicable object types: <UC> metastore**\n\nAllows a user to create a share in the metast", "timestamp": 0, "next": ""}}], "question": "How can I manage privileges for metastore objects in <UC>?", "gold_answer": "You can manage privileges for metastore objects in <UC> (Unity Catalog) by using SQL commands, the Unity Catalog CLI, or in Data Explorer. To use Data Explorer to manage privileges, you can follow the guidance provided in the respective documentation. If you are a metastore admin, the owner of an object, or the owner of the catalog or schema that contains the object, you can grant privileges."}
{"hard_neg_docs": [{"page_content": "Copy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}, {"page_content": "In the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}], "question": "Can service principals be automatically provisioned to Databricks using AAD?", "gold_answer": "No, service principals cannot be automatically provisioned to Databricks using AAD (Azure Active Directory). The document specifies that AAD does not support the automatic provisioning of service principals to Databricks. However, you can manually add service principals to your Databricks workspace as indicated by the provided link in the document."}
{"hard_neg_docs": [{"page_content": "Copy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}, {"page_content": "In the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).", "fmt_content": "Source: https://docs.databricks.com/administration-guide/users-groups/scim/aad.html\nContent:\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).", "metadata": {"source": "https://docs.databricks.com/administration-guide/users-groups/scim/aad.html", "seq_num": 1455, "description": "", "text": "---\ndescription: Learn how to provision users to <Databricks> using Microsoft <AAD>.\n---\n\n# Configure SCIM provisioning using Microsoft <AAD>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    This article refers to identity federation, which is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nThis article describes how to set up provisioning to <Databricks> using <AAD>.\n\nYou can set set up provisioning to <Databricks> using <AAD> (Azure AD) at the <Databricks> account level or at the <Databricks> workspace level.\n\nDatabricks recommends that you provision <identities> to the account level and manage the assignment of users and groups to workspaces within <Databricks>. Your workspaces must be [enabled for identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), in order to manage the assignment of users to workspaces. If you have any workspaces that are not enabled for identity federation, you should continue to provision <identities> directly to those workspaces.\n\n.. azure::\n  .. note::\n      The way provisioning is configured is entirely separate from configuring authentication and conditional access for <Databricks> workspaces or accounts. Authentication for <Databricks> is handled automatically by <AAD>, using the OpenID Connect protocol flow. You can configure [conditional access](/administration-guide/access-control/conditional-access.md), which lets you create rules to require multi-factor authentication or restrict logins to local networks, at the service level.\n\n.. gcp::\n\n  <Databricks> users authenticate to the account and workspace with their Google Cloud Identity account (or GSuite account), see [_](/administration-guide/users-groups/single-sign-on/index.md). If you provision a user to the account or workspace using <AAD>, that user must also have a Google Cloud Identity account in order to authenticate.\n\n<a id=\"prereq\"></a>\n\n## Requirements\n\n- Your <Databricks> account must have the <Security plan>.\n- You must be a global administrator for the Azure Active Directory account.\n- Your Azure Active Directory account must be a Premium edition account to provision groups. Provisioning users is available for any Azure Active Directory edition.\n- To provision users to your <Databricks> account, you must be a <a Databricks> account admin.\n- To provision users to <a Databricks> workspace, you must be a <a Databricks> workspace admin.\n\n<a id=\"account\"></a>\n\n## Provision identities to your <Databricks> account using Azure Active Directory (Azure AD)\n\nYou can sync account-level users and groups from your Azure Active Directory (Azure AD) tenant to <Databricks> using a SCIM provisioning connector.\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\n.. important:: If you already have SCIM connectors that sync users and groups directly to your workspaces **and those workspaces are enabled for identity federation**, you should disable those SCIM connectors when the account-level SCIM connector is enabled. If you have workspaces that are not enabled for identity federation, you should continue to use any SCIM connectors you have configured for those workspaces, running in parallel with the account-level SCIM connector.\n\n### Step 1: Configure <Databricks>\n\n#. As <a Databricks> account admin, log in to the <Databricks> <accountConsoleURL>.\n#. Click <User Settings Icon> **Settings**.\n#. Click **User Provisioning**.\n#. Click **Enable user provisioning**.\n\nCopy the SCIM token and the Account SCIM URL. You will use these to configure your Azure AD application.\n\n### Step 2: Configure the enterprise application\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\nFollow these steps to enable Azure AD to sync users and groups to your <Databricks> account. This configuration is separate from any configurations you have created to sync users and groups to workspaces.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n#. Click **+ New Application** above the application list. Under **Add** from the gallery, search for and select **Azure Databricks SCIM Provisioning Connector**.\n#. Enter a **Name** for the application and click **Add**.\n#. Under the **Manage** menu, click **Provisioning**.\n#. Set **Provisioning Mode** to Automatic.\n#. Set the **SCIM API endpoint URL** to the Account SCIM URL that you copied earlier.\n#. Set **Secret Token** to the <Databricks> SCIM token that you generated earlier.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Click **Save**.\n\n### Step 3: Assign users and groups to the application\n\nUsers and groups assigned to the SCIM application will be provisioned to the <Databricks> account. If you have existing <Databricks> workspaces, Databricks recommends that you add all existing users and groups in those workspaces to the SCIM application.\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> account following [_](/administration-guide/users-groups/service-principals.md#add-sp-account).\n\n#. Go to **Manage > Provisioning**.\n#. Under **Settings**, set **Scope** to **Sync only assigned users and groups**. Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n#. To start synchronizing Azure Active Directory users and groups to <Databricks>, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Go to **Manage > Users and groups**.\n#. Add some users and groups. Click **Add** user, select the users and groups, and click the **Assign** button.\n#. Wait a few minutes and check that the users and groups exist in your <Databricks> account.\n\nUsers and groups that you add and assign will automatically be provisioned to the <Databricks> account when Azure Active Directory schedules the next sync.\n\n<a id=\"workspace\"></a>\n\n## Provision identities to your <Databricks> workspace using Azure Active Directory (Azure AD)\n\n.. include:: /shared/preview.md\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    Identity federation is in [Private Preview](/release-notes/release-types.md). Contact your <Databricks> representative for access.\n\nIf you have any workspaces not enabled for [identity federation](/administration-guide/users-groups/index.md#enable-identity-federation), you should provision <identities> directly to those workspaces. This section describes how to do this.\n\nIn the following examples, replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment.\n\n<a id=\"create-enterprise-application\"></a>\n\n### Step 1: Create the enterprise application and connect it to the <Databricks> SCIM API\n\nTo set up provisioning directly to <Databricks> workspaces using <AAD>, you create an enterprise application for each <Databricks> workspace.\n\nThese instructions tell you how to create an enterprise application in the Azure portal and use that application for provisioning. If you have an existing enterprise application, you can modify it to [automate SCIM provisioning using Microsoft Graph](#automate-scim-provisioning-using-microsoft-graph). This removes the need for a separate provisioning application in the Azure Portal.\n\n#. As a workspace admin, log in to your <Databricks> workspace.\n\n#. Generate a [personal access token](/dev-tools/api/latest/authentication.md#token-management) and copy it. You provide this token to <AAD> in a subsequent step.\n\n   .. important::\n       Generate this token as <a Databricks> workspace admin who _is not_ managed by the <AAD> enterprise application. If the <Databricks> admin user who owns the personal access token is deprovisioned using <AAD>, the SCIM provisioning application will be disabled.\n\n#. In your Azure portal, go to **Azure Active Directory > Enterprise Applications**.\n\n#. Click **+ New Application** above the application list. Under **Add from the gallery**, search for and select **Azure Databricks SCIM Provisioning Connector**.\n\n#. Enter a **Name** for the application and click **Add**. Use a name that will help administrators find it, like `<workspace-name>-provisioning`.\n\n#. Under the **Manage** menu, click **Provisioning**.\n\n#. Set **Provisioning Mode** to **Automatic**.\n\n#. Enter the SCIM API endpoint URL. Append `/api/2.0/preview/scim` to your workspace URL:\n\n   ```\n   https://<databricks-instance>/api/2.0/preview/scim\n   ```\n\n   Replace `<databricks-instance>` with the <Workspace URL> of your <Databricks> deployment. See [_](/workspace/workspace-details.md).\n\n#. Set **Secret Token** to the <Databricks> personal access token that you generated in step 1.\n#. Click **Test Connection** and wait for the message that confirms that the credentials are authorized to enable provisioning.\n#. Optionally, enter a notification email to receive notifications of critical errors with SCIM provisioning.\n#. Click **Save**.\n\n### Step 2: Assign users and groups to the application\n\n.. azure::\n   .. note::\n      <AAD> does not support the automatic provisioning of service principals to <Databricks>. You can add service principals your <Databricks> workspace following [_](/administration-guide/users-groups/service-principals.md#add-sp-workspace).\n\n#. Go to **Manage > Provisioning**.\n#. Under Settings, set **Scope** to **Sync only assigned users and groups**.\n\n   Databricks recommends this option, which syncs only users and groups assigned to the enterprise application.\n\n   .. note::\n       <AAD> does not support the automatic provisioning of nested groups to <Databricks>. <AAD> can only read and provision users that are immediate members of the explicitly assigned group. As a workaround, explicitly assign (or otherwise scope in) the groups that contain the users who need to be provisioned. For more information, see [this FAQ](https://learn.microsoft.com/azure/active-directory/manage-apps/user-provisioning#does-automatic-user-provisioning-to-saas-apps-work-with-nested-groups-in-azure-ad).\n\n#. To start synchronizing <AAD> users and groups to the <Databricks> workspace, click the **Provisioning Status** toggle.\n#. Click **Save**.\n#. Test your provisioning setup:\n   a. In your Azure Databricks SCIM Provisioning Connector, go to **Manage > Users and groups**.\n   #. Add some users and groups. Click **Add user**, select the users and groups, and click the **Assign** button.\n   #. Wait a few minutes and check that the users and groups exist in your <Databricks> workspace.\n\nIn the future, users and groups that you add and assign are automatically provisioned when <AAD> schedules the next sync.\n\n.. important::\n    Do not assign the <Databricks> workspace admin whose personal access token was used to configure the **Azure Databricks SCIM Provisioning Connector** application.\n\n<a id=\"automate-scim-provisioning-using-microsoft-graph\"></a>\n\n### (Optional) Automate SCIM provisioning using Microsoft Graph\n\n[Microsoft Graph](https://learn.microsoft.com/graph/auth/auth-concepts) includes authentication and authorization libraries that you can integrate into your application", "timestamp": 0, "next": ""}}], "question": "What is the SCIM API endpoint URL for Databricks provisioned application?", "gold_answer": "The SCIM API endpoint URL for a Databricks provisioned application is:\n\n```\nhttps://<databricks-instance>/api/2.0/preview/scim\n```\n\nYou should replace `<databricks-instance>` with the Workspace URL of your Databricks deployment."}
{"hard_neg_docs": [{"page_content": "We have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}, {"page_content": "## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "question": "How can I access the model registry from multiple workspaces in Databricks?", "gold_answer": "To access the model registry from multiple workspaces in Databricks, you can use the functionality that Databricks now supports for sharing models across workspaces. With this feature, you can register models, track model runs, and load models across workspaces, allowing multiple teams to share access to models and organizations to use multiple workspaces to handle different stages of development.\n\nTo use this functionality, ensure that you have MLflow Python client version 1.11.0 or above. For more detailed instructions and information on how to set up and manage model sharing across workspaces, you should refer to the Databricks documentation on managing the model lifecycle in multiple workspaces, which is hinted at with a placeholder link in the provided text. You may need to visit the Databricks documentation or administration guide directly to find the specific guide on sharing models across workspaces."}
{"hard_neg_docs": [{"page_content": "We have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}, {"page_content": "## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "question": "What is the default behavior for generating personal access tokens in workspaces created after version 3.28?", "gold_answer": "The default behavior for generating personal access tokens in workspaces created after the release of Databricks platform version 3.28 is that users no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before version 3.28 will maintain the permissions that were already in place."}
{"hard_neg_docs": [{"page_content": "In such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\n---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "The default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the default value for the `shardsPerTask` configuration parameter in Databricks when using Kinesis?", "gold_answer": "The default value for the `shardsPerTask` configuration parameter in Databricks when using Kinesis is 5."}
{"hard_neg_docs": [{"page_content": "In such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\n---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "The default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.", "fmt_content": "Source: https://docs.databricks.com/structured-streaming/kinesis-best-practices.html\nContent:\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.", "metadata": {"source": "https://docs.databricks.com/structured-streaming/kinesis-best-practices.html", "seq_num": 1394, "description": "", "text": "---\nlint: false\ndescription: This article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n<!--  @GCP-kinesis I think is not supported, confirm   -->\n\n# Best practices: <SS> with Kinesis\n\nThis article describes best practices when using Kinesis as a streaming source with <Delta> and <AS> Structured Streaming.\n\n[Amazon Kinesis Data Streams (KDS)](https://aws.amazon.com/kinesis/data-streams/) is a massively scalable and durable real-time data streaming service. KDS continuously captures gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.\n\nKDS is a popular choice for streaming data services on AWS, due to its ease of use and serverless setup. A Kinesis Data Stream consists of individual throughput units, known as shards, and is billed based on shard-hours as well as PUT payload units. Each shard has an estimated ingestion capacity of 1000 records/sec, or 1MB/sec, and an output rate of 2MB/sec.\n\nAfter collecting data in KDS, you can use the deep integration of <AS> Structured Streaming with <Delta> for applications such as log analytics, clickstream analytics, and real time metrics. You can continuously process the data and store it into Delta tables. The following diagram depicts a typical architecture for these use cases:\n\n![Kinesis Delta architecture diagram](/_static/images/spark/structured-streaming/kinesis-delta-architecture.png)\n\n## Databricks Kinesis Structured Streaming source\n\n<DBR> includes an out-of-the-box [Kinesis source](/structured-streaming/kinesis.md). This is a proprietary connector for KDS that is not available in open source. This connector is not based on the Kinesis Client Library (KCL). The Kinesis source architecture is shown in the diagram:\n\n![Kinesis source architecture diagram](/_static/images/spark/structured-streaming/kinesis-source-architecture.png)\n\n## Key technical considerations and best practices\n\nThis section includes best practices and troubleshooting information for using Kinesis with <Delta>.\n\n### Optimize prefetching\n\nThe Kinesis source runs Spark jobs in a background thread to prefetch Kinesis data periodically and cache it in the memory of the Spark executors. The streaming query processes the cached data after each prefetch step completes and makes the data available for processing. The prefetch step significantly affects the observed end-to-end latency and throughput. You can control performance using the options described in this section.\n\nThe default setting for the `shardsPerTask` configuration parameter is 5. At scale, however, this could require a very large number of CPU cores, so a setting of 10 might be a good starting point. Then, depending on the complexity of the streaming workload and data volume, you can adjust this value based on the cluster's Ganglia metrics (CPU, memory, network, and so on). For example, a CPU-bound cluster may require a smaller value with more cores to compensate.\n\nTo optimize for minimal query latency and maximum resource usage, use the following calculation:\n\n  `total number of CPU cores in the cluster (across all executors)` >= `total number of Kinesis shards` / `shardsPerTask`.\n\nThe parameters used to determine the amount of data read per prefetch from Kinesis are described in this table.\n\n| Option   | Value  | Default  | Description  |\n|-----------------|---------------|------|------|\n| `maxRecordsPerFetch`    | Integer | 10000 | Number of records to fetch per `getRecords` API call. |\n|  `shardFetchInterval` | Duration string (2m = 2 minutes) |  1s | How long to wait before updating the list of shards (this is how the system knows that a stream has been resized). |\n|  `minFetchPeriod` | Duration string | 400ms | How long to wait between consecutive fetch attempts. This setting helps avoid Kinesis throttling. 200ms is the minimum, because the Kinesis service limit is 5 fetches/sec. |\n|  `maxFetchRate` | Decimal | 1.0 | Maximum rate of prefetch data per shard in MB/sec. This rate-limits fetches and avoids Kinesis throttling. The maximum rate allowed by Kinesis is 2.0 MB/sec. |\n|  `maxFetchDuration` |  Duration string| 10s |  How long to buffer prefetched new data before making it available for processing|\n|  `fetchBufferSize` | Byte string | 20gb | How much data to buffer for the next trigger. This is a soft limit due to its use as a stopping condition, so more data may be buffered. |\n|  `shardsPerTask` | Integer | 5 |  How many shards to prefetch in parallel per task.|\n\n.. important:: `minFetchPeriod` can create multiple GetRecords API calls to the Kinesis shard until it hits ReadProvisionedThroughputExceeded. If an exception occurs, it's not indicative of an issue as the connector maximizes the utilization of the Kinesis shard. \n\n### Avoid slowdowns caused by too many rate limit errors\n\nThe connector reduces the amount of data read from Kinesis by half each time it encounters a rate limiting error and records this event in the log with a message: `\"Hit rate limit. Sleeping for 5 seconds.\"`\n\nIt is common to see these errors as a stream is being caught up, but after it is, you should no longer see these errors. If you do, you might need to tune either from the Kinesis side (by increasing capacity) or adjust the [prefetching options](#optimize-prefetching).\n\n### Too much data causing writes to disk\n\nIf you have a sudden spike in your Kinesis streams, the assigned buffer capacity might fill up and the buffer not be emptied fast enough for new data to be added.\n\nIn such cases, Spark spills blocks from the buffer to disk and slows down processing, which affects stream performance. This event appears in the log with a message like this:\n\n```bash\n./log4j.txt:879546:20/03/02 17:15:04 INFO BlockManagerInfo: Updated kinesis_49290928_1_ef24cc00-abda-4acd-bb73-cb135aed175c on disk on 10.0.208.13:43458 (current size: 88.4 MB, original size: 0.0 B)\n```\n\nTo address this problem, try increasing the cluster memory capacity (either add more nodes or increase the memory per node), or adjust the configuration parameter `fetchBufferSize`.\n\n### Enable S3 VPC endpoints\n\nTo ensure that all S3 traffic is routed on the AWS network, Databricks recommends that you enable S3 VPC endpoints.\n\n### Hanging S3 write tasks\n\nHanging tasks can cause long streaming batch durations, which can result in streams having trouble keeping up with the input. In this case, Databricks recommends enabling Spark speculation. To ensure that tasks are not terminated too aggressively, tune the quantile and multiplier for this setting carefully. A good starting point is to set `spark.speculation.multiplier` to `3` and `spark.speculation.quantile` to `0.95`.\n\n### Latency issues while managing state with RocksDB due to slow S3 writes\n\nA common scenario when maintaining stateful operations in your streaming query is large garbage collection pauses, which in turn introduce latency and cause extended batch execution times. This typically happens while maintaining millions of keys in state. In these cases, instead of maintaining state in JVM memory, consider using RocksDB as a state store in native memory or on disk. State changes are propagated automatically to the structured streaming checkpoint. However, you may observe latency when RocksDB writes these checkpoints to S3 due to potential S3 throttling. Try reducing `spark.sql.shuffle.partitions` (default 200) to minimize the number of files written. You can also try tuning multi-part upload thresholds (`spark.hadoop.fs.s3a.multipart.size`, default 1048576000 bytes) to reduce the number of concurrent S3 writes.\n\n### Monitor streaming applications\n\nTo monitor streaming applications, Databricks recommends using Spark's Streaming Query Listener implementation.\n\nObservable metrics are named arbitrary aggregate functions that can be defined on a query (DataFrame). As soon as the execution of a DataFrame reaches a completion point (that is, finishes batch query or reaches streaming epoch) a named event is emitted that contains the metrics for the data processed since the last completion point.\n\nYou can observe these metrics by attaching a listener to the Spark session. The listener depends on the execution mode:\n\n- **Batch mode**: Use `QueryExecutionListener`.\n\n  `QueryExecutionListener` is called when the query completes. Access the metrics using the `QueryExecution.observedMetrics` map.\n\n\n- **Streaming, or micro-batch**: Use `StreamingQueryListener`.\n\n  `StreamingQueryListener` is called when the streaming query completes an epoch. Access the metrics using the `StreamingQueryProgress.observedMetrics` map. <Databricks> does not support continuous execution streaming.\n\n\nFor example:\n\n```scala\n// Observe row count (rc) and error row count (erc) in the streaming Dataset\nval observed_ds = ds.observe(\"my_event\", count(lit(1)).as(\"rc\"), count($\"error\").as(\"erc\"))\nobserved_ds.writeStream.format(\"...\").start()\n\n// Monitor the metrics using a listener\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n    event.progress.observedMetrics.get(\"my_event\").foreach { row =>\n      // Trigger if the number of errors exceeds 5 percent\n      val num_rows = row.getAs[Long](\"rc\")\n      val num_error_rows = row.getAs[Long](\"erc\")\n      val ratio = num_error_rows.toDouble / num_rows\n      if (ratio > 0.05) {\n        // Trigger alert\n      }\n    }\n  }\n})\n```\n\nYou can also monitor metrics through the UI. If you are using <DBR> 7.0 or above, use the Streaming tab in the [Spark UI](/clusters/debugging-spark-ui.md).\n\n### Deleting and recreating a stream\n\nIf you delete and then recreate a stream, you must use a new checkpoint location and directory.\n\n### Resharding\n\nStructured Streaming supports resharding. In such a case, increasing the number of shards is sufficient. You do not need to switch streams or create temporary streams to divert traffic.\n\n## Learn more\n\n[_](/structured-streaming/kinesis.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I optimize query latency and resource usage when using Kinesis in Databricks?", "gold_answer": "To optimize query latency and resource usage when using Kinesis with Databricks, you should follow these steps:\n\n1. Consider starting with a setting of 10 for the `shardsPerTask` configuration parameter. This is a good starting point, especially at scale, because the default setting of 5 might require too many CPU cores.\n\n2. Adjust the `shardsPerTask` value based on the complexity of your streaming workload and the data volume. Monitor the cluster's Ganglia metrics, like CPU, memory, network, etc., to make informed adjustments.\n\n3. Ensure that the total number of CPU cores in your cluster (across all executors) is greater than or equal to the total number of Kinesis shards divided by the `shardsPerTask` value. This calculation helps to balance the load across your available resources, aiming for minimal query latency and maximum resource utilization.\n\nFor example, if you have 100 Kinesis shards and your `shardsPerTask` is set to 10, you should have at least 10 CPU cores in your cluster to handle the data processing effectively."}
{"hard_neg_docs": [{"page_content": ".. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I troubleshoot a job that fails with an ArrayIndexOutOfBoundsException in the joint genotyping pipeline?", "gold_answer": "To troubleshoot a job that fails with an `ArrayIndexOutOfBoundsException` in the joint genotyping pipeline, you should try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`. This error typically indicates that an input record has an incorrect number of genotype probabilities, and adjusting these settings can help identify or bypass the issue."}
{"hard_neg_docs": [{"page_content": ".. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info", "fmt_content": "Source: https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html\nContent:\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info", "metadata": {"source": "https://docs.databricks.com/archive/genomics/joint-genotyping/joint-genotyping-pipeline.html", "seq_num": 96, "description": "", "text": "---\ndescription: Learn about using the joint genotyping pipeline contained in Databricks Runtime for Genomics.\nrobots: noindex\n---\n\n.. gcp::\n\n  ---\n  orphan: 1\n  ---\n\n\n# Joint genotyping pipeline\n\n.. include:: /shared/warnings/archive-genomics.md\n\nThe <Databricks> joint genotyping pipeline is a [GATK best practices][gatk best practices] compliant pipeline for joint genotyping\nusing [GenotypeGVCFs][genotypegvcfs].\n\n[gatk best practices]: https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-\n\n[genotypegvcfs]: https://gatk.broadinstitute.org/hc/en-us/articles/360041417092-GenotypeGVCFs\n\n## Walkthrough\n\nThe pipeline typically consists of the following steps:\n\n1. Ingest variants into <Delta>.\n#. Joint-call the cohort with GenotypeGVCFs.\n\nDuring variant ingest, single-sample gVCFs are processed in batches and the rows are stored in <Delta> to provide fault\ntolerance, fast querying, and incremental joint genotyping. In the joint genotyping step, the gVCF rows are ingested\nfrom <Delta>, split into bins, and distributed to partitions. For each variant site, the relevant gVCF rows per sample are\nidentified and used for regenotyping.\n\n## Setup\n\nThe pipeline is run as <a Databricks> job. Most likely <a Databricks> solutions architect will work with\nyou to set up the initial job. The necessary details are:\n\n.. aws::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-aws.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use storage-optimized instances. We recommend **i3.8xlarge**.\n    * To reduce costs, use all spot workers with the **Spot fall back to On-demand** option selected.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n    * Enable autoscaling local storage to ensure that the cluster doesn't run out of disk space\n\n.. azure::\n\n    .. literalinclude:: /_static/examples/hls/jg-policy-azure.json\n      :language: json\n\n    * The cluster configuration should use Databricks Runtime for Genomics.\n    * The task should be the joint genotyping pipeline notebook found at the bottom of this page.\n    * For best performance, use the storage-optimized VMs. We recommend **Standard_L32s_v2**.\n    * To reduce costs, enable autoscaling with a minimum of 1 worker and a maximum of 10-50 depending\n      on latency requirements.\n\n\n## Reference genomes\n\nYou must configure the reference genome using [environment variables](/clusters/configure.md#env-var).\nTo use GRCh37, set the environment variable:\n\n```ini\nrefGenomeId=grch37\n```\n\nTo use GRCh38, change `grch37` to `grch38`.\n\nTo use a custom reference genome, see instructions in [_](/archive/genomics/secondary/dnaseq-pipeline.md#custom-reference-genomes).\n\n## Parameters\n\nThe pipeline accepts parameters that control its behavior. The most important and\ncommonly changed parameters are documented here. To view all available parameters and their usage\ninformation, run the first cell of the pipeline notebook. New parameters are added regularly.\nAfter importing the notebook and setting it as a job task, you can set these parameters for\n[all runs](/workflows/jobs/jobs.md#create-a-job) or [per-run](/workflows/jobs/jobs.md#run-a-job-with-different-parameters).\n\n.. list-table::\n    :header-rows: 1\n\n    * - Parameter\n      - Default\n      - Description\n    * - manifest\n      - n/a\n      - The [manifest](#manifest-format) describing the input.\n    * - output\n      - n/a\n      - The path where pipeline [output](#output) is written.\n    * - replayMode\n      - skip\n      - One of:\n\n        * `skip`: stages are skipped if output already exists.\n        * `overwrite`: existing output is deleted.\n    * - exportVCF\n      - false\n      - If true, the pipeline writes results in VCF as well as <Delta>.\n    * - targetedRegions\n      - n/a\n      - Path to files containing regions to call. If omitted, calls all regions.\n    * - gvcfDeltaOutput\n      - n/a\n      - If specified, gVCFs are ingested to a Delta table before genotyping. You should\n        specify this parameter only if you expect to joint call the same gVCFs many times.\n    * - performValidation\n      - false\n      - If `true`, the system verifies that each record contains the necessary information for\n        joint genotyping. In particular, it checks that the correct number of genotype\n        probabilities are present.\n    * - validationStringency\n      - STRICT\n      - How to handle malformed records, both during loading and validation.\n\n        * `STRICT`: fail the job\n        * `LENIENT`: log a warning and drop the record\n        * `SILENT`: drop the record without a warning\n\n<a id=\"manifest-format\"></a>\n\n.. tip::\n  To perform joint calling from an existing Delta table, set `gvcfDeltaOutput` to the table path and `replayMode` to `skip`. You can also provide the `manifest`, which will be used to define the VCF schema and samples; these will be inferred from the Delta table otherwise. We ignore the `targetedRegions` and `performValidation` parameters in this setup.\n\n## Output\n\nThe regenotyped variants are all written out to Delta tables inside the provided output directory. In addition, if\nyou configured the pipeline to export VCFs, they'll appear under the output directory as well.\n\n```\noutput\n|---genotypes\n    |---Delta files\n|---genotypes.vcf\n    |---VCF files\n```\n\n## Manifest format\n\nThe manifest is a file or blob describing where to find the input single-sample GVCF files, with each file path on a new\nrow. For example:\n\n```text\nHG00096.g.vcf.bgz\nHG00097.g.vcf.bgz\n```\n\n.. tip::\n    If the provided manifest is a file, each row may be an absolute path or a path relative to the manifest file. If the\n    provided manifest is a blob, the row field must be an absolute path. You can include globs `(*)` to match many files.\n\n## Troubleshooting\n\n**Job fails with an `ArrayIndexOutOfBoundsException`**\n\nThis error usually indicates that an input record has an incorrect number of genotype probabilities. Try setting the `performValidation` option to `true` and the `validationStringency` option to `LENIENT` or `SILENT`.\n\n## Additional usage info\n\nThe joint genotyping pipeline shares many operational details with the other <Databricks> pipelines. For\nmore detailed usage information, such as output format structure, tips for running programmatically,\nand steps for setting up custom reference genomes, see [_](/archive/genomics/secondary/dnaseq-pipeline.md).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Where are the regenotyped variants written to in the joint genotyping pipeline?", "gold_answer": "The regenotyped variants in the joint genotyping pipeline are written out to Delta tables inside the provided output directory."}
{"hard_neg_docs": [{"page_content": "### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}, {"page_content": "- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "The following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}], "question": "What are some security benefits of using Private Service Connect (PSC) with Databricks?", "gold_answer": "Some security benefits of using Private Service Connect (PSC) with Databricks include mitigating the risks of:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n- Data access from a client on the internet or an unauthorized network using the Databricks API."}
{"hard_neg_docs": [{"page_content": "### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}, {"page_content": "- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "The following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1", "fmt_content": "Source: https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html\nContent:\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1", "metadata": {"source": "https://docs.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html", "seq_num": 1499, "description": "", "text": "---\ndescription: Learn how to secure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC>.\n---\n\n.. azure-aws::\n  ---\n  orphan: 1\n  ---\n\n\n# Enable Private Service Connect for your workspace\n\n.. include:: /shared/preview.md\n\nSecure a workspace with private connectivity and mitigate data exfiltration risks by enabling Google <PSC> (PSC) on the workspace. This article describes how to perform some steps using the account console.\n\n\n## Two <PSC> options\n\nThere are two ways that you can use private connectivity so that you don't expose the traffic to the public network. This article discusses how to configure either one or both <PSC> connection types:\n\n- **Front-end <PSC> (user to workspace)**: Allows users to connect to the <Databricks> web application, REST API, and Databricks Connect API over a [Virtual Private Cloud (VPC) endpoint](https://cloud.google.com/vpc/docs/private-service-connect)  endpoint.\n\n- **Back-end <PSC> (data plane to control plane)**: Connects Databricks compute resources in a customer-managed Virtual Private Cloud (VPC) (the data plane) to the <Databricks> workspace core services (the control plane). Clusters connect to the control plane for two destinations: REST APIs (such as the Secrets API) and the [secure cluster connectivity](/security/network/secure-cluster-connectivity.md) relay. This <PSC> connection type involves two different VPC interface endpoints because of the two different destination services. For information about the data and control planes, see [_](/getting-started/overview.md).\n\nYou can implement both front-end and back-end <PSC> or just one of them. If you implement <PSC> for both the front-end and back-end connections, you can optionally mandate private connectivity for the workspace, which means <Databricks> rejects any connections over the public network. If you decline to implement any one of these connection types, you cannot enforce this requirement.\n\nTo enable <PSC>, you must create <Databricks> configuration objects and add new fields to existing configuration objects.\n\n.. important:: In this release, you can create a new workspace with <PSC> connectivity using a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) that you set up. You cannot add <PSC> connectivity to an existing workspace. You cannot enable <PSC> on a workspace that uses a Databricks-managed VPC.\n\nThe following diagram is an overview of the <PSC> network flow and architecture with <Databricks>.\n\n![Diagram: Private Service Connect network flow and architecture](/_static/images/administration-guide/cloud-configurations/gcp/psc-network-flow-high-level.png)\n\nFor more detailed diagrams and more information about using a firewall, see [_](#ref-arch).\n\n\n### Security benefits\n\nUsing <PSC> helps mitigate the following data exfiltration risks:\n\n- Data access from a browser on the internet or an unauthorized network using the Databricks web application.\n\n- Data access from a client on the internet or an unauthorized network using the Databricks API.\n\n\n<a id=\"terminology\"></a>\n\n### Terminology\n\nThe following <GCP> terms are used in this guide to describe Databricks configuration:\n\n.. list-table::\n  :header-rows: 1\n\n  * - Google terminology\n    - Description\n  * - Private Service Connect (PSC)\n    - A <GCP> feature that provides private connectivity between VPC networks and <GCP> services.\n  * - Host project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project in which the VPCs are created. This applies both to the data plane VPC (for back-end <PSC>) and the transit VPC (for front-end <PSC>)\n  * - Service project\n    - If you use [what Google calls Shared VPCs](https://cloud.google.com/vpc/docs/shared-vpc), which allow you to use a different <GCP> project for the VPC separate from the workspace's main project ID for compute resources, this is the project for the workspace compute resources.\n  * - <PSC> endpoint or VPC endpoint\n    - A private connection from a VPC network to services, for example services published by Databricks.\n\n\nThe following table describes important terminology.\n\n.. list-table::\n  :header-rows: 1\n\n  * - <Databricks> terminology\n    - Description\n  * - Databricks client\n    - Either a user on a browser accessing the Databricks WebApp or an application client accessing the Databricks APIs.\n  * - Transit VPC\n    - The VPC network hosting clients that access the Databricks workspace WebApp or APIs.\n  * - Front-end (User to Workspace) <PSC> endpoint\n    - The <PSC> Endpoint configured on the Transit VPC network that allows clients to privately connect to the Databricks web application and APIs.\n  * - Back-end (data plane to control plane) <PSC> endpoints\n    - The <PSC> Endpoints configured on the your customer-managed VPC network to allow private communication between the data plane and the Databricks control plane.\n  * - Data plane VPC\n    - The VPC network that hosts the compute resources of your Databricks workspace. You configure your customer-managed data plane VPC in your <GCP> Organization.\n  * - Private workspace\n    - Refers to a workspace where (1) The virtual machines of the data plane compute cluster do not have any public IP address. (2) The workspace endpoints on the Databricks control plane can only be accessed privately from authorized VPC networks or, optionally, from authorized IP addresses.\n\n\n<a id=\"requirements\"></a>\n\n## Requirements and limitations\n\nThe following requirements and limitations apply to the private preview release:\n\n- **Regions**: <PSC> connectivity is available in the following <GCP> regions: `asia-southeast1`, `australia-southeast1`, `europe-west1`, `europe-west2`, `europe-west3`, `us-central1`, `us-west1`, `us-west4`, `us-east1`, and `us-east4`.\n\n- **New workspaces only**: You can create a new workspace with <PSC> connectivity. You cannot add <PSC> connectivity to an existing workspace.\n\n- **Customer-managed VPC is required.**: You must use a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md). You need to create your VPC in <GCP> console or with another tool. Next, in the [Databricks account console](https://accounts.gcp.databricks.com), you  create a network configuration that references your VPC and sets additional fields that are specific to <PSC>.\n\n- **Self-service other than enabling your account for the preview.**: Databricks must enable your account for the feature. To enable <PSC> on one or more workspaces, contact your Databricks representative and request to enable it on your account. Provide the Google Cloud region and your [host project](#terminology) ID to reserve quota for <PSC> connections. After your account is enabled for <PSC>, use the [Databricks account console](https://accounts.gcp.databricks.com) to configure your <PSC> objects and create new workspaces.\n\n- **Quotas**: You can configure up to two <PSC> endpoints to the Databricks service for each VPC host project. You can deploy data plane compute for multiple Databricks workspaces on the same VPC network. In such a scenario, all those workspaces will share the same <PSC> endpoints. Please contact your account team if this limitation does not work for you.\n\n- **No cross-region connectivity**: <PSC> workspace components must be in the same region including:\n  - Transit VPC network and subnets\n  - Data plane VPC network and subnets\n  - Databricks workspace\n  - <PSC> endpoints\n  - <PSC> endpoint subnets\n  \n\n### Multiple options for network topology\n\nYou can deploy a private Databricks workspace with the following network configuration options:\n\n- **Host Databricks users (clients) and the Databricks data plane on the same network**: In this option, the transit VPC and data plane VPC refer to the same underlying VPC network. If you choose this topology, all access to any Databricks workspace from that VPC must go over the front-end <PSC> connection for that VPC. See [_](#requirements).\n\n- **Host Databricks users (clients) and the Databricks data plane on separate networks**: In this option, the user or application client can access different Databricks workspaces using different network paths. You can optionally allow a user on the transit VPC to access a private workspace over a <PSC> connection and also allow users on the public internet to the workspace.\n\n- **Host data plane for multiple Databricks workspaces on the same network**: In this option, the data plane VPC for multiple Databricks workspaces refer to the same underlying VPC network. All such workspaces must share the same back-end <PSC> endpoint. This deployment pattern can allow you to configure a smaller number of <PSC> endpoints while configuring a large number of workspaces.\n\nYou can share one transit VPC for multiple workspaces. However, each transit VPC must contain only workspaces that use front-end PSC, or only workspaces that do not use front-end PSC. Due to the way DNS resolution works on Google Cloud, you cannot use both types of workspaces with a single transit VPC.\n\n## Related security configuration\n\nYou can also configure the following network security features to keep your data and compute environment private:\n\n- Provide a [customer-managed VPC](/administration-guide/cloud-configurations/gcp/customer-managed-vpc.md) and use it for compute resources. Because you control the configuration, you can control the firewall configuration to isolate the workloads deployed to Databricks.\n\n- Configure <PSC> endpoint and IP range ACLs to only allow access from authorized networks.\n\n- Use VPC Service Controls (VPC SC) to protect your Cloud Storage (GCS) resources including the DBFS and data lake buckets.\n\n- Add the data plane VPC of your Databricks workspace to a VPC SC service perimeter to limit egress to resources of <GCP> storage services.\n\n<a id=\"ref-arch\"></a>\n\n## Reference architecture\n\nA Databricks workspace deployment includes the following network paths that you can secure:\n\n- Databricks client on your transit VPC to the Databricks control plane. This includes both the web application and REST API access.\n- Databricks data plane VPC network to the Databricks control plane service. This includes the secure cluster connectivity relay and the workspace connection for the REST API endpoints.\n- Databricks data plane to storage in a Databricks-managed project.\n- Databricks data plane VPC network to the GKE API server.\n- Databricks control plane to storage in your Projects including the DBFS bucket.\n\n<!-- ### Use case diagrams\n\nThe following diagram shows the key objects for the back-end <PSC> use case:\n\n![diagram back-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-backend.png)\n\nThe following diagram shows the key objects for the front-end <PSC> use case:\n\n![diagram front-end](/_static/images/administration-guide/cloud-configurations/gcp/psc-diagram-frontend.png)\n\n## Detailed architecture diagram -->\n\n<!--  THESE DIAGRAMS ARE TOO SIMILAR BUT I WILL LEAVE THEM IN FOR NOW AND REMOVE ONE ... AND MAYBE  REDO THE OTHER FROM SOURCE    -->\n\nIt's possible to have a no-firewall architecture to restrict outbound traffic, ideally using an external metastore. Outbound traffic to a public library repository is not possible by default, but you can bring your own locally mirrored package repo. The following diagram shows a network architecture for a full (front-end and back-end) <PSC> deployment with no firewalls:\n\n![Private Service Connect network architecture with no firewall](/_static/images/administration-guide/cloud-configurations/gcp/psc-net-arch-full-no-firewall.png", "timestamp": 0, "next": ""}}], "question": "What terminology is used in this guide to describe Databricks configuration on Google Cloud Platform (GCP)?", "gold_answer": "The document provided above mentions that certain GCP terms are used in the guide to describe Databricks configuration, but it does not list the specific terms. It only indicates that there is a section titled \"Terminology\" which likely contains the GCP terms used. To find out the exact terms, one would need to access the full guide and look at the content under the \"Terminology\" section."}
{"hard_neg_docs": [{"page_content": "#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I view network configurations and any validation errors in Databricks?", "gold_answer": "To view network configurations and any validation errors in Databricks, follow these steps:\n\n1. In the account console, click **Cloud resources**.\n2. Click **Network**. This will display all network configurations, with **VPC ID**, **VPC Status**, and **Created** date shown for each.\n3. Click the network configuration name to view more details, including subnet IDs, security group IDs. If there are network validation error messages, they will be displayed here.\n4. If a new workspace fails to deploy, it is important to revisit this page to view new network validation error messages, as some network validation errors are only detected when the configuration is used to create a new workspace."}
{"hard_neg_docs": [{"page_content": "#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink", "fmt_content": "Source: https://docs.databricks.com/administration-guide/account-settings-e2/networks.html\nContent:\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink", "metadata": {"source": "https://docs.databricks.com/administration-guide/account-settings-e2/networks.html", "seq_num": 1443, "description": "", "text": "---\ndescription: Learn how to manage network configurations for customer-managed VPCs using the <Databricks> account console.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Manage network configurations using the account console\n\n.. aws::\n\n  This article describes a process that is available only for [accounts on the E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture).  All new <Databricks> accounts and most existing accounts are now E2. If you are unsure which account type you have, contact your Databricks representative.\n\n  By default, <Databricks> creates a VPC in your AWS account for each workspace and creates new <DBR> clusters in those workspaces.  If you are on the [E2 version of the Databricks platform](/getting-started/overview.md#e2-architecture), you have the option to create workspaces in your own VPC, known as a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md). This article describes how to use the [account console](/administration-guide/account-settings/index.md#account-console) to create and manage _network configurations_ for your account when you want to use a customer-managed VPC. To learn how to create network configurations using the Account API, see [_](/administration-guide/workspace/create-workspace-api.md).\n\nUsing your own VPC enables you to configure it according to your organization's enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your own VPC.\n\nThe following related sections discuss updating existing network and configuration objects:\n- [_](/administration-guide/workspace/update-workspace.md).\n- [_](/administration-guide/cloud-configurations/aws/privatelink.md#update-related).\n\n## Create a network configuration\n\nTo create a network configuration for a [customer-managed VPC](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md), you must create the VPC and subnets to meet Databricks requirements and then reference that VPC---including network objects such as VPCs, subnets, and security groups---in a network configuration for your Databricks account.\n\n.. note:: These instructions show you how to create the network configuration from the **Cloud resources** page in the account console before you create a new workspace. You can also create the storage configuration in a similar way as part of the flow of creating a new workspace. See [_](/administration-guide/workspace/create-workspace.md).\n\n#. Set up your VPC, subnets, and security groups, using the instructions in [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n   Copy the IDs for each of these objects for use in the next step.\n\n   .. important:: You can share one customer-managed VPC with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you **cannot** reuse subnets or security groups with any other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks network configuration encapsulates this information, you cannot reuse a network configuration across workspaces.\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. From the vertical navigation on the page, click **Network configurations**.\n\n#. Click **Add network configuration**.\n\n#. In the **Network configuration name** field, enter a human-readable name for your new network configuration.\n\n#. In the **VPC ID** field, enter the VPC ID.\n\n#. In the **Subnet IDs** field, enter the IDs for at least two AWS subnets in the VPC. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. In the **Security Group IDs** field, enter the ID for at least one AWS security group. For network configuration requirements, see [_](/administration-guide/cloud-configurations/aws/customer-managed-vpc.md).\n\n#. (Optional) To support [AWS PrivateLink back-end connectivity](/administration-guide/cloud-configurations/aws/privatelink.md), you must select two VPC endpoint registrations from the  fields under the **Back-end private connectivity** heading.\n\n   ![Back-end private connectivity](/_static/images/administration-guide/cloud-configurations/aws/privatelink-vpc-back-end-picker.png)\n\n   a. If you have not yet created the two AWS VPC endpoints that are specific to your workspace region, you must do so now. See [_](/administration-guide/cloud-configurations/aws/privatelink.md#create-vpce). You can use the AWS Console or various automation tools.\n\n   b. For each field, either choose existing VPC endpoint registrations, or choose **Register a new VPC endpoint** to create one immediately that references the AWS VPC endpoints that you have already created. For guidance on fields, see [_](/administration-guide/account-settings-e2/vpc-endpoints.md).\n\n   <!--  the button label may change (in dispute Tue, Aug 9, 2022 )    -->\n\n#. Click **Add**.\n\n## View network configurations and any validation errors\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n   All network configurations are listed, with **VPC ID**, **VPC Status**, and **Created** date displayed for each.\n\n#. Click the network configuration name to view more details, including subnet IDs, security group IDs.\n\n   If there are network validation error messages, they will be shown here.\n\n   .. important:: Some network validation errors are detected only when the configuration is used to create a new workspace. If a new workspace fails to deploy, re-visit this page to view new network validation error messages.\n\n<a id=\"delete\"></a>\n\n## Delete a network configuration\n\nNetwork configurations cannot be edited after creation. If the configuration has incorrect data or if you no longer need it, delete the network configuration:\n\n#. In the [account console](/administration-guide/account-settings/index.md#account-console), click **Cloud resources**.\n\n#. Click **Network**.\n\n#. On the row for the configuration, click the kebab menu <Vertical Ellipsis> on the right, and select **Delete**.\n\n#. In the confirmation dialog, click **Confirm Delete**.\n\n.. <PL> replace:: PrivateLink\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Can network configurations be edited after they are created in Databricks?", "gold_answer": "No, network configurations cannot be edited after they are created in Databricks. If the configuration has incorrect data or if it is no longer needed, it must be deleted."}
{"hard_neg_docs": [{"page_content": "## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "question": "What storage accounts are used for the DBFS root in new Databricks workspaces?", "gold_answer": "New Databricks workspaces use ADLS2 (Azure Data Lake Storage Gen2) storage accounts for the DBFS (Databricks File System) root. Existing Databricks workspaces continue to use Blob storage for the DBFS root."}
{"hard_neg_docs": [{"page_content": "## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2023/march.html\nContent:\n## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2023/march.html", "seq_num": 1926, "description": "", "text": "---\ndescription: March 2023 release notes for new <Databricks> features and improvements.\n---\n\n# March 2023\n\nThese features and <Databricks> platform improvements were released in March 2023.\n\n.. note::\n\n  Releases are staged. Your <Databricks> account might not be updated until a week or more after the initial release date.\n\n.. azure::\n\n  ## New IP addresses for East Asia region\n\n  **March 15, 2023**\n\n  <Databricks> has added a new dedicated control plane for the `eastasia` region. If you host a <Databricks> workspace in your own VNet (also known as VNet injection) in the East Asia region, you likely manage network traffic for your workspace using custom routes (UDR). If so, you must update those routes using the additional IP addresses and domains listed at [_](/resources/supported-regions.md#ip-domain-azure).\n\n## Execute SQL cells in the notebook in parallel\n\n**March 15, 2023**\n\nYou can now run SQL cells in <Databricks> notebooks in parallel while attached to an interactive cluster. See [_](/notebooks/notebooks-code.md#sql-run-now).\n\n## Databricks Terraform provider updated to version 1.12.0\n\n**March 9, 2023**\n\n.. azure-aws::\n\n   Version 1.12.0 adds a `databricks_model_serving` resource, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n.. gcp::\n\n   Version 1.12.0 adds support for <UC> on <GCP>, deprecates the `schedule` block within the `databricks_sql_query` resource, and more. For more details, see the changelog for version [1.12.0](https://github.com/databricks/terraform-provider-databricks/blob/v1.12.0/CHANGELOG.md).\n\n## SQL admin console and workspace admin console combined\n\n**March 9, 2023**\n\nThe SQL admin console has been combined with the general [admin console](/administration-guide/workspace/index.md#admin-console) to create a unified experience for admin users. All SQL admin settings are now accessed from the admin console.\n\n.. azure-aws::\n\n  ## Model Serving is GA\n\n  **March 7, 2023**\n\n  Model Serving, formerly Serverless Real-Time Inference, is now generally available.\n\n  Model Serving provides a highly available and low-latency service for deploying models. The service automatically scales up or down to meet demand changes within the chosen concurrency range. See [_](/machine-learning/model-serving/index.md).\n\n.. azure::\n\n  ## For new workspaces that use UDRs, Azure storage IP needs an additional UDR\n\n  **March 6, 2023**\n\n  Due to [changes in DBFS root storage for new workspaces](#root-dbfs-adls), new workspaces that use [VNet injection](/administration-guide/cloud-configurations/azure/vnet-inject.md) and define [user-defined routes (UDRs)](/administration-guide/cloud-configurations/azure/udr.md) using _IP addresses_ instead of the recommended _service tags_ must add an additional Azure UDR for each VNet. To configure the two UDRs, see [_](/resources/supported-regions.md#dbfs-blob). This change does _not_ affect existing workspaces.\n\n  <a id=\"root-dbfs-adls\"></a>\n\n  ## <Databricks> DBFS root storage uses <ADLS2> for new workspaces\n\n  **March 6, 2023**\n\n  New <Databricks> workspaces use <ADLS2> storage accounts for the [DBFS root](/dbfs/dbfs-root.md). Existing <Databricks> workspaces continue to use Blob storage for the DBFS root. See [Introduction to Azure Data Lake Storage Gen2](https://learn.microsoft.com/azure/storage/blobs/data-lake-storage-introduction).\n\n## New Data Explorer availability\n\n**March 6, 2023**\n\nA new version of Data Explorer is now available in all workspaces that use a supported version of <DBR> 7.3 or later.\n\n.. azure-aws::\n\n  ## View frequent queries and users of a table using the Insights tab\n\n  **March 6, 2023**\n\n  You can now use the Insights tab in Data Explorer to view the most frequent queries and users of any table registered in <UC>. You must have the `SELECT` privilege on a table to see this data, and you only see queries that you have permission to view. See [_](/data/table-insights.md).\n\n.. azure-aws::\n\n   ## View lineage information for your <Databricks> jobs\n\n   **March 3, 2023**\n\n   If <UC> is enabled in your workspace, you can view lineage information for your jobs in the <Databricks> Jobs UI, including upstream tables the job reads from and downstream tables the job writes to. See [_](/workflows/jobs/jobs.md#view-lineage).\n\n## <DBR> 12.2 LTS and <DBR> 12.2 LTS ML are GA\n\n**March 2, 2023**\n\n<DBR> 12.2 LTS and <DBR> 12.2 LTS ML are now generally available.\n\nSee [_](/release-notes/runtime/12.2.md) and [_](/release-notes/runtime/12.2ml.md).\n\n## Workspace files are now in Public Preview\n\n**March 6, 2023**\n\nYou can now work with non-notebook files in <Databricks>. Workspaces files are enabled by default in all workspaces. See [_](/files/workspace.md).\n\n.. include:: /shared/replacements.md\n\n.. include:: /shared/partner-connect.md\n", "timestamp": 0, "next": ""}}], "question": "How can I view the most frequent queries and users of a table using the Insights tab in Data Explorer?", "gold_answer": "To view the most frequent queries and users of a table using the Insights tab in Data Explorer, you need to follow these steps:\n\n1. Navigate to the Data Explorer in your Databricks workspace.\n2. Locate the Insights tab within the Data Explorer.\n3. Select any table registered in your Databricks workspace that you wish to inspect.\n4. You must have the `SELECT` privilege on the table to see the data within the Insights tab.\n5. Once you have the necessary permissions, you can view the most frequent queries and the users of the selected table.\n\nIt's important to note that you will only see queries that you have permission to view. For more detailed instructions and information, you can refer to the documentation linked in the release notes, specifically at `See [Data Explorer table insights](/data/table-insights.md)`."}
{"hard_neg_docs": [{"page_content": ".. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I add custom metadata to Delta tables in Databricks?", "gold_answer": "You can add custom metadata to Delta tables in Databricks by using user-defined strings as metadata in commits. You can do this through the following two methods:\n\n1. Using the DataFrameWriter option `userMetadata` when performing write operations. Here's an example in Python and Scala:\n\n   Python:\n   ```python\n   df.write.format(\"delta\")\n     .mode(\"overwrite\")\n     .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n     .save(\"/tmp/delta/people10m\")\n   ```\n\n   Scala:\n   ```scala\n   df.write.format(\"delta\")\n     .mode(\"overwrite\")\n     .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n     .save(\"/tmp/delta/people10m\")\n   ```\n\n2. Using the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. Here's an SQL example:\n\n   ```sql\n   SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n   INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n   ```\n\nIf both the DataFrameWriter option and the SparkSession configuration have been specified, the DataFrameWriter option takes precedence. This custom metadata is then readable in the describe history operation"}
{"hard_neg_docs": [{"page_content": ".. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/custom-metadata.html\nContent:\n---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/custom-metadata.html", "seq_num": 1341, "description": "", "text": "---\ndescription: Add custom metadata to <Delta> tables to enrich data discovery on <Databricks>.\n---\n\n# Enrich <Delta> tables with custom metadata\n\nIn addition to table comments, you can use fields in the <Delta> transaction log to add custom tags to a table or messages for an individual commit.\n\n## Set user-defined commit metadata\n\nYou can specify user-defined strings as metadata in commits made by these operations, either using the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. If both of them have been specified, then the option takes preference. This user-defined metadata is readable in the [describe history](/delta/history.md) operation.\n\n.. code-language-tabs::\n\n  ```sql\n\n  SET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\n  INSERT OVERWRITE default.people10m SELECT * FROM morePeople\n  ```\n\n  ```python\n  df.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n  ```scala\n  df.write.format(\"delta\")\n    .mode(\"overwrite\")\n    .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n    .save(\"/tmp/delta/people10m\")\n  ```\n\n## Store custom tags in table properties\n\nYou can store your own metadata as a table property using `TBLPROPERTIES` in `CREATE` and `ALTER`. You can then `SHOW` that metadata. For example:\n\n```sql\nALTER TABLE default.people10m SET TBLPROPERTIES ('department' = 'accounting', 'delta.appendOnly' = 'true');\n\n-- Show the table's properties.\nSHOW TBLPROPERTIES default.people10m;\n\n-- Show just the 'department' table property.\nSHOW TBLPROPERTIES default.people10m ('department');\n```\n\n`TBLPROPERTIES` are stored as part of Delta table metadata. You cannot define new `TBLPROPERTIES` in a `CREATE` statement if a Delta table already exists in a given location.\n\nIn addition, to tailor behavior and performance, <Delta> supports certain Delta table properties:\n\n- Block deletes and updates in a Delta table: `delta.appendOnly=true`.\n- Configure the [time travel](/delta/history.md) retention properties: `delta.logRetentionDuration=<interval-string>` and `delta.deletedFileRetentionDuration=<interval-string>`. For details, see [_](/delta/history.md#data-retention).\n\n\n- Configure the number of columns for which statistics are collected: `delta.dataSkippingNumIndexedCols=n`. This property indicates to the writer that statistics are to be collected only for the first `n` columns in the table. Also the data skipping code ignores statistics for any column beyond this column index. This property takes affect only for new data that is written out.\n\n.. aws::\n\n  - Randomize file prefixes to avoid hot spots in S3 metadata: `delta.randomizeFilePrefixes=true`. For tables that require a lot (thousands of requests per second) of fast read/write operations, we strongly recommend dedicating an S3 bucket to a table (locating table at the root of the bucket), and enabling randomized file prefixes to get the best experience.\n\n<!--  @GCP do we need gcp disclaimer too?       -->\n\n.. note::\n  - Modifying a Delta table property is a write operation that will conflict with other [concurrent write operations](/optimizations/isolation-level.md), causing them to fail. Databricks recommends you modify a table property only when there are no concurrent write operations on the table.\n\nYou can also set `delta.`-prefixed properties during the first commit to a Delta table using Spark configurations. For example, to initialize a Delta table with the property `delta.appendOnly=true`, set the Spark configuration `spark.databricks.delta.properties.defaults.appendOnly` to `true`. For example:\n\n.. code-language-tabs::\n\n  ```sql\n  spark.sql(\"SET spark.databricks.delta.properties.defaults.appendOnly = true\")\n  ```\n\n  ```python\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\n  ```scala\n  spark.conf.set(\"spark.databricks.delta.properties.defaults.appendOnly\", \"true\")\n  ```\n\nSee also the [_](/delta/table-properties.md).\n\n## Explore table metadata\n\n<Delta> supports `SHOW [PARTITIONS | COLUMNS]` and `DESCRIBE TABLE`. See [_](/sql/language-manual/sql-ref-syntax-aux-show-partitions.md), [_](/sql/language-manual/sql-ref-syntax-aux-show-columns.md), [_](/sql/language-manual/sql-ref-syntax-aux-describe-table.md)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I specify user-defined commit metadata in Delta commits?", "gold_answer": "You can specify user-defined commit metadata in Delta commits by using either the DataFrameWriter option `userMetadata` or the SparkSession configuration `spark.databricks.delta.commitInfo.userMetadata`. When performing operations that commit changes, you can set this metadata to include custom strings.\n\nIf you are specifying the metadata via DataFrameWriter in SQL, Python, or Scala, you would use the `option` method with `\"userMetadata\"` as the key and your custom string as the value. For example:\n\nIn SQL:\n```sql\nSET spark.databricks.delta.commitInfo.userMetadata=overwritten-for-fixing-incorrect-data\nINSERT OVERWRITE default.people10m SELECT * FROM morePeople\n```\n\nIn Python:\n```python\ndf.write.format(\"delta\") \\\n  .mode(\"overwrite\") \\\n  .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\") \\\n  .save(\"/tmp/delta/people10m\")\n```\n\nIn Scala:\n```scala\ndf.write.format(\"delta\")\n  .mode(\"overwrite\")\n  .option(\"userMetadata\", \"overwritten-for-fixing-incorrect-data\")\n  .save(\"/tmp/delta/people10m\")\n```\n\nIf both the DataFrameWriter option `userMetadata` and the SparkSession configuration `spark.databricks.delta"}
{"hard_neg_docs": [{"page_content": "## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "When did support end for DBR 6.1 and 6.1 ML?", "gold_answer": "Support for DBR 6.1 and 6.1 ML ended on April 16, 2020."}
{"hard_neg_docs": [{"page_content": "## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/april.html\nContent:\n## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/april.html", "seq_num": 1937, "description": "", "text": "---\ndescription: April 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n# April 2020\n\nThese features and <Databricks> platform improvements were released in April 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n.. azure::\n\n  ## Unique URLs for each <Databricks> workspace\n\n  **April 28, 2020**\n\n  <Databricks> has added a new unique URL for each workspace. This new per-workspace URL has the format `adb-<workspaceId>.<random number>.azuredatabricks.net`. This URL is complementary to the existing regional URLs (`<region>.azuredatabricks.net`) that you have used up to now to access your workspaces. Both URLs continue to be supported. We do, however, strongly recommend that you use the new per-workspace URL, because it is region independent.\n\n  If you have existing scripts or automation tools that use the old regional URLs, we recommend that you change your scripts to use the new per-workspace URLs. We also recommend that you use the per-workspace URL in any future scripts or automation tools. However, because <Databricks> continues to support the existing regional URLs, existing code will continue to work after the new URL rollout.\n\n  For details, see [_](/workspace/per-workspace-urls.md).\n\n## MLflow tracking UI enhancement\n\n**April 23-30, 2020: Version 3.18**\n\nThe MLflow UI now offers an option to delete child runs when you delete a root run.\n\n## Notebook usability improvements\n\n**April 23-30, 2020: Version 3.18**\n\nThis release brings several usability improvements when working with notebooks:\n\n- You can now select adjacent notebook cells using **Shift** + **Up** or **Down** for the previous and next cell respectively. Multi-selected cells can be copied, cut, deleted, and pasted.\n\n- When you delete a cell, by default a delete confirmation dialog appears. You can now disable the confirmation dialog when you delete the cell by selecting the **Do not show this again** checkbox and clicking **Confirm**. You can also toggle the confirmation dialog with the **Turn on command delete confirmation** option in <User Settings Icon> **> User Settings > Editor settings**.\n\n.. azure::\n\n  ## Use <AAD> tokens to authorize to the Databricks REST API (Public Preview)\n\n  **April 23-30, 2020: Version 3.18**\n\n  You can now use <AAD> tokens to authorize to the Databricks [_](/dev-tools/api/latest/index.md). <AAD> tokens enable you to automate the creation and setup of new workspaces.\n  Service principals are application objects in <AAD>. You can also use service principals within your <Databricks> workspaces to automate workflows.\n\n  For details, see [_](/dev-tools/api/latest/aad/index.md).\n\n  ## Single user can view the <AS> logs on a passthrough cluster\n\n  **April 23-30, 2020: Version 3.18**\n\n  Previously, credential passthrough on a Standard mode cluster did not allow a single user to view the <AS> logs if they were not an admin user. Now a single user with attach permission can view Spark logs even if they are not an admin.\n\n## Databricks Connect now supports <DBR> 6.5\n\n**April 20, 2020**\n\n[Databricks Connect](/dev-tools/databricks-connect.md) now supports <DBR> 6.5.\n\n## <DBR> 6.1 and 6.1 ML support ends\n\n**April 16, 2020**\n\nSupport for <DBR> 6.1 and <DBR> 6.1 for Machine Learning ended on April 16. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n\n## <DBR> 6.5 GA\n\n**April 14, 2020**\n\n<DBR> 6.5 brings many library upgrades and new features, including:\n\n* Operation metrics for all writes, updates, and deletes on a Delta table now appear in table history\n* You can rate-limit the data processed in <Delta> streaming micro-batches\n* Snowflake connector is updated to 2.5.9\n\nFor more information, see the complete [_](/release-notes/runtime/6.5.md) release notes.\n\n## <DBR> 6.5 for Machine Learning GA\n\n**April 14, 2020**\n\n<DBR> 6.5 ML brings the following library upgrade:\n\n* MLflow upgraded from 1.5.0 to 1.7.0\n\nFor more information, see the complete [_](/release-notes/runtime/6.5ml.md) release notes.\n\n## <DBR> 6.5 for Genomics GA\n\n**April 14, 2020**\n\n<DBR> 6.5 for Genomics is built on top of <DBR> 6.5. For details about the improvements and libraries included in this release, see the complete [_](/release-notes/runtime/6.5genomics.md) release notes.\n\n.. aws::\n\n  <a id=\"iam-passthrough\"></a>\n\n  ## Authenticate to S3 buckets automatically using your IAM credentials (Public Preview)\n\n  **April 9-14, 2020: Version 3.17**\n\n  IAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters by using the identity that you use to log in to Databricks. When you enable your cluster for IAM credential passthrough, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using instance profiles:\n\n  - IAM credential passthrough allows multiple users with different data access policies to share one Databricks cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one IAM role. This requires all users on a Databricks cluster to share that role and the data access policies of that role.\n  - IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n  For details, see [_](/data-governance/credential-passthrough/iam-passthrough.md).\n\n  .. note:: This feature is not available in all workspaces. Contact your Databricks sales representative for information about whether the feature is available for your workspace.\n\n  ## IAM role renamed to instance profile\n\n  **April 9-14, 2020: Version 3.17**\n\n  Databricks has changed the IAM Role label in the web application to Instance Profile. This is in line with AWS terminology and is consistent with the Databricks [_](/dev-tools/api/latest/instance-profiles.md). You will see this change in your workspace and in our documentation.\n\n  ## Easier notebook title changes\n\n  **April 9-14, 2020: Version 3.17**\n\n  You can now change the title of an open notebook by clicking the title and editing inline instead of clicking **File > Rename**.\n\n  ## Cluster termination reporting enhancement\n\n  **April 9-14, 2020: Version 3.17**\n\n  When clusters are terminated, they now return a `type` field that indicates the reason why the cluster was terminated. See [_](/dev-tools/api/latest/clusters.md#clusterterminationreasonterminationtype).\n\n  ## DBFS REST API delete endpoint size limit\n\n  From May 5th, 2020, when you delete a large number of files recursively using the [_](/dev-tools/api/latest/dbfs.md), the delete operation will be done in increments. The call returns a response after approximately 45s with an error message asking the user to re-invoke the delete operation until the directory structure is fully deleted. For example:\n\n  ```json\n  {\n    \"error_code\":\"PARTIAL_DELETE\",\"message\":\"The requested operation has deleted 324 files. There are more files remaining. You must make another request to delete more.\"\n  }\n  ```\n\n## <DBR> 6.0 and 6.0 ML support ends\n\n**April 1, 2020**\n\nSupport for <DBR> 6.0 and <DBR> 6.0 for Machine Learning ended on April 1. See [_](/release-notes/runtime/databricks-runtime-ver.md#runtime-support).\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What version of Databricks Connect does support DBR 6.5?", "gold_answer": "Databricks Connect supports DBR 6.5 as of April 20, 2020."}
{"hard_neg_docs": [{"page_content": "== \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\n== \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}, {"page_content": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\nTitle: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\n7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}], "question": "How can I make a backup of the Parquet files that were added to the table in version 6 and 7?", "gold_answer": "To make a backup of the Parquet files that were added to the table in version 6 and 7, you can use the sample Python code provided in the document to move the Parquet files to a backup folder. While the document provides a brief introduction to the code snippet, the actual code to backup the files is not fully shown. Therefore, you would need to write a Python script using functions like `os` and `shutil` that would:\n\n1. Identify and list the Parquet files associated with the corrupted versions 6 and 7.\n2. Move or copy these Parquet files to a designated backup directory.\n\nHere's a conceptual example of what the Python code might look like:\n\n```python\nimport os\nimport shutil\n\ndef get_parquet_files_list(corrupted_versions):\n    # This function should return a list of Parquet files from the specified corrupted versions\n    # For example purposes, we will assume this function is implemented and returns the file paths.\n    pass\n\ndef backup_parquet_files(corrupted_versions, backup_folder):\n    parquet_files = get_parquet_files_list(corrupted_versions)\n    if not os.path.exists(backup_folder):\n        os.makedirs(backup_folder)\n    for file_path in parquet_files:\n        shutil.move"}
{"hard_neg_docs": [{"page_content": "== \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\n== \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}, {"page_content": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\nTitle: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''", "fmt_content": "Source: https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error\nContent:\n7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''", "metadata": {"source": "https://kb.databricks.com/delta/recover-from-a-delta_log-corruption-error", "seq_num": 2219, "description": "", "text": "Title: Recover from a DELTA_LOG corruption error\n\nDescription: Learn how to repair a Delta table that reports an IllegalStateException error when queried.\n\nAnswer: Problem You are attempting to query a Delta table when you get an IllegalStateException error saying that the metadata could not be recovered. Error in SQL statement: IllegalStateException:  The metadata of your Delta table couldn't be recovered while Reconstructing version: 691193. Did you manually delete files in the _delta_log directory? Set spark.databricks.delta.stateReconstructionValidation.enabled to \"false\" to skip validation. Cause The Delta table is not accessible due to corrupted CRC or JSON files. For example, you disabled multicluster writes (AWS | Azure | GCP) but you still have more than one cluster attempting to write to the same table, leading to corruption. Solution Warning\nThis solution is a workaround that allows you to recover the Delta table and make it readable. This solution assumes that your Delta table is append-only.\nIf your Delta table contains an update operation DO NOT use this solution. Open a ticket with Databricks Support. These steps make the table readable and allow you to safely remove the corrupted delta transaction files. Disable the validation config. This allows you to bypass the IllegalStateException error when you run a query on the Delta table. This should not be used for production but is necessary to access the corrupted version of the table. Any running cluster can be used for this process. A cluster restart is not required to make changes in this property.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=False; Identify which version of the table is corrupted. \nIdentify the latest Delta version of the table by checking the _delta_log folder.%fs ls /delta-table-root-folder/_delta_log Run select * from table@v(&lt;latest&gt;) in a SQL notebook cell. You should see empty results as the latest version is corrupted. Keep running select * from table@v(&lt;latest-x&gt;) until you find a version that is not corrupted.For the purpose of this article, assume that versions 6 and 7 are corrupted. Version 5 is intact and has not been corrupted. Make a backup of the Parquet files (data files) that were added to the table in version 6 and 7. This sample code moves the Parquet files to a backup folder. %python import os import shutil def get_parquet_files_list(corrupted_versions):   '''   Get list contains corrupted json versions, identify the parquet data files from these version and returns them in list   '''   final_list=[]   for file in corrupted_versions: delta_table_location = '&lt;delta-table-location&gt;' # prefix /dbfs not needed     json_path = delta_table_location+'/_delta_log/'+str(file)     df = spark.read.json(&lt;json_path&gt;).select('add.path')     target_list = list(df.select('path').toPandas()['path'])     final_list.extend(target_list)   return list(filter(None, final_list)) def copy_files(final_list,source_folder,backup_folder):   '''   copy parquet files from source to backup folder   '''      i=1   for file_path in final_list:     src_path = source_folder + file_path     trg_path = backup_folder + file_path     os.makedirs(os.path.dirname(trg_path), exist_ok=True)     shutil.copy(src_path, trg_path)     print(\"Done --&gt; \"+str(i))     i = i+1      def main():     source_folder = '&lt;delta-table-location&gt;' # prefix /dbfs is needed if the table location is in dbfs folder or in mount point backup_folder = '&lt;backup-folder-storage-path&gt;' # prefix /dbfs is needed   corrupted_versions = ['00000000000000000006.json','00000000000000000007.json'] # enter the corrupted versions json files here   final_list = get_parquet_files_list(corrupted_versions)   copy_files(final_list,source_folder,backup_folder)    if __name__ == \"__main__\":     main() Remove the CRC and JSON files for the corrupted versions of the table.%fs rm /delta-table-root-folder/_delta_log/00000000000000000006.json rm /delta-table-root-folder/_delta_log/00000000000000000006.crc\n%fs rm /delta-table-root-folder/_delta_log/00000000000000000007.json rm /delta-table-root-folder/_delta_log/00000000000000000007.crc Run RESTORE TABLE to restore the Delta table to the most recent version that is not corrupted. In our example, this is version 5.%sql RESTORE TABLE &lt;table-name&gt; TO VERSION AS OF 5 Now that the corrupted files have been removed, the table can be queried on any version. To avoid data loss, you must append the Parquet files that you previously backed up. This ensures that any data added to the corrupted versions of the table are inserted into the restored version and unnecessary data loss can be avoided. While appending, please verify:\nIf the target table is partitioned. If it is partitioned, include the partitionBy option in the append statement.\nConfirm all data types match the target table.%python backup_folder = '/backup-folder' target_table_path = '/delta-table-root-folder/' append_df = spark.read.format('parquet').load(&lt;backup-folder&gt;) append_df_new = append_df.withColumn('col1',col('col1').cast('string'))# casting to match the target table schema append_df_new.write.format('delta').partitionBy('col2','col3','col4').mode('append').option(\"path\",&lt;target-table-path&gt;).saveAsTable(\"db.tableName\") Re-enable validation checking.%sql set spark.databricks.delta.stateReconstructionValidation.enabled=True;", "timestamp": 1657245945.644, "next": ""}}], "question": "Which versions of the Parquet files were corrupted?", "gold_answer": "Based on the information provided in the document, versions 6 and 7 of the Parquet files were corrupted. Version 5 is mentioned as intact and not corrupted."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I add existing storage accounts to Unity Catalog (UC) using external locations?", "gold_answer": "To add existing storage accounts to Unity Catalog (UC) using external locations, you should follow these recommendations:\n\n- Only load storage accounts into UC using external locations if all other storage credentials and access patterns have been revoked to ensure maximum security.\n- Do not load a storage account that is used as a DBFS root as an external location in UC.\n\nThe process involves loading your existing storage accounts into UC, but you must ensure that the storage account is dedicated to UC and is not mixed with DBFS mounts or other access methods that could bypass UC's security model."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access", "fmt_content": "Source: https://docs.databricks.com/dbfs/unity-catalog.html\nContent:\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access", "metadata": {"source": "https://docs.databricks.com/dbfs/unity-catalog.html", "seq_num": 1351, "description": "", "text": "---\ndescription: Learn how to securely use DBFS in <UC> enabled <Databricks> workspaces.\n---\n\n# Best practices for DBFS and <UC>\n\n.. gcp::\n\n  .. admonition::  Preview\n    :class: preview\n\n    <UC> is in [Private Preview](/release-notes/release-types.md) on Databricks on <GCP>. To try it, reach out to your <Databricks> contact.\n\n<UC> introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. This article outlines several best practices around working with <UC> external locations and DBFS.\n\nDatabricks recommends against using DBFS and mounted cloud object storage for most use cases in <UC>-enabled <Databricks> workspaces. This article describes a few scenarios in which you should use mounted cloud object storage. Note that Databricks does not recommend using the DBFS root in conjunction with <UC>, unless you must migrate files or data stored there into <UC>.\n\n## How is DBFS used in <UC>-enabled workspaces?\n\nThe [DBFS root](/dbfs/dbfs-root.md) is the [default location](/dbfs/root-locations.md) for storing files associated with a number of actions performed in the <Databricks> workspace, including creating managed tables in the workspace-scoped `hive_metastore`. Actions performed against tables in the `hive_metastore` use legacy data access patterns, which may include data and storage credentials managed by DBFS.\n\n## How does DBFS work in single user access mode?\n\nClusters configured with single user access mode have full access to DBFS, including all files in the DBFS root and [mounted data](/dbfs/mounts.md). DBFS root and mounts are available in this access mode, making it the choice for ML workloads that need access to <UC> datasets.\n\nDatabricks recommends using service principals with scheduled jobs and single user access mode for production workloads that need access to data managed by both DBFS and <UC>.\n\n## How does DBFS work in shared access mode?\n\nShared access mode combines <UC> data governance with <Databricks> legacy table ACLs. Access to data in the `hive_metastore` is only available to users that have permissions explicitly granted.\n\nTo interact with files directly using DBFS, you must have `ANY FILE` permissions granted. Because `ANY FILE` allows users to bypass legacy tables ACLs in the `hive_metastore` and access all data managed by DBFS, Databricks recommends caution when granting this privilege.\n\nShared access mode does not support DBFS root or mounts.\n\n## Use DBFS while launching <UC> clusters with single user access mode\n\nDatabricks recommends using DBFS mounts for init scripts, configurations, and libraries stored in external storage. This behavior is not supported in shared access mode.\n\n## Do not use DBFS with <UC> external locations\n\n<UC> secures access to data in external locations by using full cloud URI paths to identify grants on managed object storage directories. DBFS mounts use an entirely different data access model that bypasses <UC> entirely. Databricks recommends that you do not reuse cloud object storage volumes between DBFS mounts and UC external volumes.\n\n## Secure your <UC>-managed storage\n\nEach <UC> metastore has an object storage account configured by <a Databricks> account administrator. <UC> uses this location to store all data and metadata for <UC>-managed tables.\n\nA storage account used for a <UC> metastore should:\n\n- Be created new for <UC>.\n- Have a custom identity policy defined for <UC>.\n- Only be accessible with <UC>.\n- Only be accessed using the identity access policies created for <UC>.\n\n## Add existing data to external locations\n\nIt is possible to load existing storage accounts into <UC> using external locations. For greatest security, Databricks recommends only loading storage accounts to external locations if all other storage credentials and access patterns have been revoked.\n\nYou should never load a storage account used as a DBFS root as an external location in <UC>.\n\n## Cluster configurations are ignored by <UC> filesystem access\n\n<UC> does not respect cluster configurations for filesystem settings. This means that Hadoop filesystem settings for configuring custom behavior with cloud object storage do not work when accessing data using <UC>.\n\n## Limitation around multiple path access\n\nWhile you can generally use <UC> and DBFS together, paths that are equal or share a parent/child relationship cannot be referenced in the same command or notebook cell using different access methods.\n\nFor example, if an external table `foo` is defined in the `hive_metastore` at location `a/b/c` and an external location is defined in <UC> on `a/b/`, the following code would throw an error:\n\n```python\nspark.read.table(\"foo\").filter(\"id IS NOT NULL\").write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\nThis error would not arise if this logic is broken into two cells:\n\n```python\ndf = spark.read.table(\"foo\").filter(\"id IS NOT NULL\")\n```\n\n```python\ndf.write.mode(\"overwrite\").save(\"a/b/c\")\n```\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the recommended approach for securing access to data in external locations when using DBFS mounts and Unity Catalog (UC)?", "gold_answer": "The recommended approach for securing access to data in external locations when using DBFS mounts and Unity Catalog (UC) is to not reuse cloud object storage volumes between DBFS mounts and UC external volumes. Since UC secures access by using full cloud URI paths to identify grants on managed object storage directories, and DBFS mounts use a different access model that bypasses UC, it is advised to keep them separate. Additionally, when setting up a storage account for a UC metastore, the following guidelines should be adhered to:\n\n- The storage account should be created new for UC.\n- A custom identity policy should be defined specifically for UC.\n- The storage account should only be accessible with UC.\n- Access should only occur using the identity access policies created for UC.\n\nFor existing data, Databricks recommends only loading storage accounts into UC using external locations if all other storage credentials and access patterns have been revoked. It is also advised never to load a storage account used as a DBFS root as an external location in UC."}
{"hard_neg_docs": [{"page_content": ".. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Can I write feature tables to a non-default Hive metastore in UC-enabled workspaces?", "gold_answer": "No, you cannot write feature tables to a non-default Hive metastore in Unity Catalog (UC)-enabled workspaces. In UC-enabled workspaces, you can write feature tables only to the default Hive metastore."}
{"hard_neg_docs": [{"page_content": ".. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/index.html\nContent:\n---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/index.html", "seq_num": 240, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store, centralized repository of features that enables feature sharing and discovery across your organization and ensures that the same feature computation code is used for model training and inference.\n---\n\n# Databricks Feature Store\n\nThis page explains what a feature store is and what benefits it provides, and the specific advantages of Databricks Feature Store.\n\nThe Databricks Feature Store library is available only on <DBR> for Machine Learning and is accessible through <Databricks> notebooks and workflows.\n\n.. azure-aws::\n\n  .. note:: At this time, Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables only to the default Hive metastore.\n\n## What is a feature store?\n\nA feature store is a centralized repository that enables data scientists to find and share features and also ensures that the same code used to compute the feature values is used for model training and inference.\n\nMachine learning uses existing data to build a model to predict future outcomes. In almost all cases, the raw data requires preprocessing and transformation before it can be used to build a model. This process is called feature engineering, and the outputs of this process are called features - the building blocks of the model.\n\nDeveloping features is complex and time-consuming. An additional complication is that for machine learning, feature calculations need to be done for model training, and then again when the model is used to make predictions. These implementations may not be done by the same team or using the same code environment, which can lead to delays and errors. Also, different teams in an organization will often have similar feature needs but may not be aware of work that other teams have done. A feature store is designed to address these problems.\n\n## Why use Databricks Feature Store?\n\nDatabricks Feature Store is fully integrated with other components of <Databricks>.\n\n- Discoverability. The Feature Store UI, accessible from the Databricks workspace, lets you browse and search for existing features.\n- Lineage. When you create a feature table with Feature Store, the data sources used to create the feature table are saved and accessible. For each feature in a feature table, you can also access the models, notebooks, jobs, and endpoints that use the feature.\n- Integration with model scoring and serving. When you use features from Feature Store to train a model, the model is packaged with feature metadata. When you use the model for batch scoring or online inference, it automatically retrieves features from Feature Store. The caller does not need to know about them or include logic to look up or join features to score new data. This makes model deployment and updates much easier.\n- Point-in-time lookups. Feature Store supports time series and event-based use cases that require point-in-time correctness.\n\n## Start using Feature Store\n\nSee the following articles to get started with Feature Store:\n\n.. azure-aws::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - Learn about [publishing features to online feature stores](/machine-learning/feature-store/online-feature-stores.md) for real-time serving and automatic feature lookup.\n  - [Share feature tables across workspaces](/machine-learning/feature-store/multiple-workspaces.md).\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n.. gcp::\n\n  - Try one of the [example notebooks](/machine-learning/feature-store/example-notebooks.md) that illustrate feature store capabilities.\n  - See the reference material for the [Feature Store Python API](/machine-learning/feature-store/python-api.md).\n  - Learn about [training models with Feature Store](/machine-learning/feature-store/train-models-with-feature-store.md).\n  - Learn about [working with feature tables](/machine-learning/feature-store/feature-tables.md).\n  - Use [time series feature tables and point-in-time lookups](/machine-learning/feature-store/time-series.md) to retrieve the latest feature values as of a particular time for training or scoring a model.\n  - [Control access to feature tables](/machine-learning/feature-store/access-control.md).\n\n## More information\n\nFor more information on best practices for using Feature Store, download [The Comprehensive Guide to Feature Stores](https://www.databricks.com/p/ebook/the-comprehensive-guide-to-feature-stores).\n\n.. azure-aws::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     online-feature-stores\n     time-series\n     multiple-workspaces\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. gcp::\n\n  .. toctree::\n     :maxdepth: 1\n     :hidden:\n\n     concepts\n     python-api\n     workflow-overview-and-notebook\n     example-notebooks\n     feature-tables\n     train-models-with-feature-store\n     time-series\n     ui\n     access-control\n     troubleshooting-and-limitations\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of a feature store?", "gold_answer": "The purpose of a feature store is to act as a centralized repository that enables data scientists to find and share features. It ensures that the same feature computation code is used for both model training and inference, thereby promoting consistency and efficiency in the machine learning workflow. Features are the outputs of the feature engineering process, which involves preprocessing and transforming raw data, and they serve as the building blocks of machine learning models. The feature store facilitates feature sharing and discovery across an organization."}
{"hard_neg_docs": [{"page_content": "If you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}, {"page_content": "You can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Unmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}], "question": "What should I do before unmounting a storage location in Databricks?", "gold_answer": "Before unmounting a storage location in Databricks, you should:\n\n1. Instruct users to get approval before unmounting a storage location.\n2. Verify that there are no jobs running on the cluster that are reading or writing to the mount point, as attempting a manual recovery while the mount is in use may cause the issue to reoccur."}
{"hard_neg_docs": [{"page_content": "If you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}, {"page_content": "You can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Unmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.", "fmt_content": "Source: https://kb.databricks.com/delta/filereadexception-when-reading-delta-table\nContent:\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.", "metadata": {"source": "https://kb.databricks.com/delta/filereadexception-when-reading-delta-table", "seq_num": 2142, "description": "", "text": "Title: FileReadException when reading a Delta table\n\nDescription: A FileReadException error occurs when you attempt to read from a Delta table. The underlying data has been deleted, or the storage blob was unmounted during a write.\n\nAnswer: Problem You attempt to read a Delta table from mounted storage and get a FileReadException error. FileReadException: Error while reading file abfss:REDACTED@REDACTED.dfs.core.windows.net/REDACTED/REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet. A file referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.microsoft.com/azure/databricks/delta/delta-intro#frequently-asked-questions  Caused by: FileNotFoundException: Operation failed: 'The specified path does not exist.', 404, HEAD, https:// REDACTED.dfs.core.windows.net/ REDACTED/ REDACTED/REDACTED/REDACTED/PARTITION=REDACTED/part-00042-0725ec45-5c32-412a-ab27-5bc88c058773.c000.snappy.parquet?upn=false&amp;action=getStatus&amp;timeout=90 Cause FileReadException errors occur when the underlying data does not exist. The most common cause is manual deletion. If the underlying data was not manually deleted, the mount point for the storage blob was removed and recreated while the cluster was writing to the Delta table. Delta Lake does not fail a table write if the location is removed while the data write is ongoing. Instead, a new folder is created in the default storage account of the workspace, with the same path as the removed mount. Data continues to be written in that location. If the mount is recreated before the write operation is finished, and the Delta transaction logs are made available again, Delta updates the transaction logs and the write is considered successful. When this happens, data files written to the default storage account while the mount was deleted are not accessible, as the path currently references the mounted storage account location. Info\nYou can use diagnostic logging to verify that a mount was removed. Query the DBFS table for mount and unmount events.\nFor example:\nDatabricksDBFS | where ActionName == \"unmount\" or ActionName == \"mount\" Solution You can restore the missing data in one of two ways. Repair the Delta table and add the missing data back with a custom job. Use FSCK to repair the table. %sql FSCK REPAIR TABLE &lt;table-name&gt; Rewrite the missing data with a custom job. This option is a good choice if you can re-run the last job without risking duplicate data. Manually recover the missing files. Verify that there are no active jobs reading or writing to the mounted storage account that contains the Delta table.\nUnmount the mount path. This allows you to access the /mnt/&lt;path-to-table&gt;directory in the default storage account.%python dbutils.fs.unmount(\"/mnt/&lt;mount-containing-table&gt;\") Use dbutils.fs.mvto move the files located in the table path to a temporary location.%python dbutils.fs.mv(\"/mnt/&lt;path-to-table&gt;\", \"/tmp/tempLocation/\", True)) Recreate the mount point.%python dbutils.fs.mount(source = \"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/\", mount_point = \"/mnt/&lt;mount-containing-table&gt;\", extra_configs = configs)Review the Access Azure Data Lake Storage Gen2 and blob storage documentation for more information.\nMove the files from the temporary location to the updated Delta table path.%python dbutils.fs.mv(\"/tmp/tempLocation\", \"/mnt/&lt;path-to-table&gt;\", True)) If any jobs are reading or writing to the mount point when you attempt a manual recovery you may cause the issue to reoccur. Verify that the mount is not in use before attempting a manual repair. Best practices Instruct users to get approval before unmounting a storage location.\nIf you must unmount a storage location, verify there are no jobs running on the cluster.\nUse dbutils.fs.updateMount to update information about the mount. Do not use unmount and mount to update the mount.\nUse diagnostic logging to identify any possible unmount issues.\nRun production jobs only on job clusters which are not affected by temporary unmount commands while running, unless they run the dbutils.fs.refreshMounts command.\nWhen running jobs on interactive clusters, add a verification step at the end of a job (such as a count) to check for missing data files. If any are missing an error is triggered immediately.", "timestamp": 1677165705.46, "next": ""}}], "question": "How can I move the files located in the table path to a temporary location in Databricks?", "gold_answer": "To move the files located in the table path to a temporary location in Databricks, you can use the `dbutils.fs.mv` command in Python. Here's the command provided in the document:\n\n```python\ndbutils.fs.mv(\"/mnt/<path-to-table>\", \"/tmp/tempLocation/\", True)\n```\n\nReplace `<path-to-table>` with the actual path to your table. This command moves the files to the `/tmp/tempLocation/` directory and the `True` parameter indicates that the move should be recursive (i.e., it should include subdirectories and their contents)."}
{"hard_neg_docs": [{"page_content": "# providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n# providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}, {"page_content": "## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "-- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n-- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}], "question": "How can I read changes from a Delta table using timestamps?", "gold_answer": "To read changes from a Delta table using timestamps, you can use the following code snippet in Python:\n\n```python\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingTimestamp\", 'YYYY-MM-DD HH:MM:SS') \\  # replace with your starting timestamp\n  .option(\"endingTimestamp\", 'YYYY-MM-DD HH:MM:SS') \\    # replace with your ending timestamp, if desired\n  .table(\"myDeltaTable\")\n```\n\nMake sure to replace `'YYYY-MM-DD HH:MM:SS'` with the actual start and end timestamps you want to use for reading the changes, and replace `\"myDeltaTable\"` with the name of your Delta table. If you only want to provide the starting timestamp and read all changes from that point onwards, you can omit the `.option(\"endingTimestamp\", 'YYYY-MM-DD HH:MM:SS')` line."}
{"hard_neg_docs": [{"page_content": "# providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n# providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}, {"page_content": "## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "-- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta/delta-change-data-feed.html\nContent:\n-- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```", "metadata": {"source": "https://docs.databricks.com/delta/delta-change-data-feed.html", "seq_num": 1331, "description": "", "text": "---\ndescription: Learn how to get row-level change information from Delta tables using the <Delta> change data feed.\n---\n\n# Use <Delta> change data feed on <Databricks>\n\n.. note::\n\n  - <Delta> change data feed is available in <DBR> 8.4 and above.\n  - This article describes how to record and query row-level change information for Delta tables using the change data feed feature. To learn how to update tables in a <DLT> pipeline based on changes in source data, see [_](/delta-live-tables/cdc.md).\n\nChange data feed allows <Databricks> to track row-level changes between versions of a Delta table. When enabled on a Delta table, the runtime records _change events_ for all the data written into the table. This includes the row data along with metadata indicating whether the specified row was inserted, deleted, or updated.\n\nYou can read the change events in batch queries using Spark SQL, <AS> DataFrames, and <SS>.\n\n.. important:: Change data feed works in tandem with table history to provide change information. Because cloning a Delta table creates a separate history, the change data feed on cloned tables doesn't match that of the original table.\n\n## Use cases\n\nChange data feed is not enabled by default. The following use cases should drive when you enable the change data feed.\n\n- **Silver and Gold tables**: Improve <Delta> performance by processing only row-level changes following initial `MERGE`, `UPDATE`, or `DELETE` operations to accelerate and simplify ETL and ELT operations.\n- **Materialized views**: Create up-to-date, aggregated views of information for use in BI and analytics without having to reprocess the full underlying tables, instead updating only where changes have come through.\n- **Transmit changes**: Send a change data feed to downstream systems such as Kafka or RDBMS that can use it to incrementally process in later stages of data pipelines.\n- **Audit trail table**: Capture the change data feed as a Delta table provides perpetual storage and efficient query capability to see all changes over time, including when deletes occur and what updates were made.\n\n## Enable change data feed\n\nYou must explicitly enable the change data feed option using one of the following methods:\n\n- **New table**: Set the table property `delta.enableChangeDataFeed = true` in the `CREATE TABLE` command.\n\n  ```sql\n  CREATE TABLE student (id INT, name STRING, age INT) TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **Existing table**: Set the table property `delta.enableChangeDataFeed = true` in the `ALTER TABLE` command.\n\n  ```sql\n  ALTER TABLE myDeltaTable SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n  ```\n\n- **All new tables**:\n\n  ```sql\n  set spark.databricks.delta.properties.defaults.enableChangeDataFeed = true;\n  ```\n\n.. important::\n  - Once you enable the change data feed option for a table, you can no longer write to the table using <DBR> 8.1 or below. You can always read the table.\n\n  Only changes made _after_ you enable the change data feed are recorded; past changes to a table are not captured.\n\n### Change data storage\n\n<Databricks> records change data for `UPDATE`, `DELETE`, and `MERGE` operations in the `_change_data` folder under the table directory. Some operations, such as insert-only operations and full partition deletes, do not generate data in the `_change_data` directory because <Databricks> can efficiently compute the change data feed directly from the transaction log.\n\nThe files in the `_change_data` folder follow the retention policy of the table. Therefore, if you run the [VACUUM](/delta/vacuum.md) command, change data feed data is also deleted.\n\n## Read changes in batch queries\n\nYou can provide either version or timestamp for the start and end. The start and end versions and timestamps are inclusive in the queries. To read the changes from a particular start version to the _latest_ version of the table, specify only the starting version or timestamp.\n\nYou specify a version as an integer and a timestamps as a string in the format `yyyy-MM-dd[ HH:mm:ss[.SSS]]`.\n\nIf you provide a version lower or timestamp older than one that has recorded change events---that is, when the change data feed was enabled---an error is thrown indicating that the change data feed was not enabled.\n\n.. code-language-tabs::\n  ```sql\n  -- version as ints or longs e.g. changes from version 0 to 10\n  SELECT * FROM table_changes('tableName', 0, 10)\n\n  -- timestamp as string formatted timestamps\n  SELECT * FROM table_changes('tableName', '2021-04-21 05:45:46', '2021-05-21 12:00:00')\n\n  -- providing only the startingVersion/timestamp\n  SELECT * FROM table_changes('tableName', 0)\n\n  -- database/schema names inside the string for table name, with backticks for escaping dots and special characters\n  SELECT * FROM table_changes('dbName.`dotted.tableName`', '2021-04-21 06:45:46' , '2021-05-21 12:00:00')\n\n  -- path based tables\n  SELECT * FROM table_changes_by_path('\\path', '2021-04-21 05:45:46')\n  ```\n\n  ```python\n  # version as ints or longs\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .option(\"endingVersion\", 10) \\\n    .table(\"myDeltaTable\")\n\n  # timestamps as formatted timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .option(\"endingTimestamp\", '2021-05-21 12:00:00') \\\n    .table(\"myDeltaTable\")\n\n  # providing only the startingVersion/timestamp\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # path based tables\n  spark.read.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", '2021-04-21 05:45:46') \\\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n  ```scala\n  // version as ints or longs\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .option(\"endingVersion\", 10)\n    .table(\"myDeltaTable\")\n\n  // timestamps as formatted timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .option(\"endingTimestamp\", \"2021-05-21 12:00:00\")\n    .table(\"myDeltaTable\")\n\n  // providing only the startingVersion/timestamp\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // path based tables\n  spark.read.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingTimestamp\", \"2021-04-21 05:45:46\")\n    .load(\"pathToMyDeltaTable\")\n  ```\n\n## Read changes in streaming queries\n\n.. code-language-tabs::\n\n  ```python\n  # providing a starting version\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingVersion\", 0) \\\n    .table(\"myDeltaTable\")\n\n  # providing a starting timestamp\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .option(\"startingTimestamp\", \"2021-04-21 05:35:43\") \\\n    .load(\"/pathToMyDeltaTable\")\n\n  # not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\") \\\n    .option(\"readChangeFeed\", \"true\") \\\n    .table(\"myDeltaTable\")\n  ```\n\n  ```scala\n  // providing a starting version\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", 0)\n    .table(\"myDeltaTable\")\n\n  // providing a starting timestamp\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .option(\"startingVersion\", \"2021-04-21 05:35:43\")\n    .load(\"/pathToMyDeltaTable\")\n\n  // not providing a starting version/timestamp will result in the latest snapshot being fetched first\n  spark.readStream.format(\"delta\")\n    .option(\"readChangeFeed\", \"true\")\n    .table(\"myDeltaTable\")\n  ```\n\nTo get the change data while reading the table, set the option `readChangeFeed` to `true`.\nThe `startingVersion` or `startingTimestamp` are optional and if not provided the stream returns the latest\nsnapshot of the table at the time of streaming as an `INSERT` and future changes as change data.\nOptions like rate limits (`maxFilesPerTrigger`, `maxBytesPerTrigger`) and `excludeRegex` are also supported when reading change data.\n\n.. note::\n  Rate limiting can be atomic for versions other than the starting snapshot version. That is, the entire commit version will be rate limited or the entire commit will be returned.\n\n  By default, if a user passes in a version or timestamp exceeding the last commit on a table, the error `timestampGreaterThanLatestCommit` is thrown. Change data feed can handle the out of range version case if the user sets the following configuration to `true`.\n\n  ```sql\n  set spark.databricks.delta.changeDataFeed.timestampOutOfRange.enabled = true;\n  ```\n\n  If you provide a start version greater than the last commit on a table or a start timestamp newer than the last commit on a table, then when the preceding configuration is enabled, an empty read result is returned.\n\n  If you provide an end version greater than the last commit on a table or an end timestamp newer than the last commit on a table, then when the preceding configuration is enabled in batch read mode, all changes between the start version and the last commit are be returned.\n\n## What is the schema for the change data feed?\n\nWhen you read from the change data feed for a table, the schema for the latest table version is used.\n\n.. note:: Most schema change and evolution operations are fully supported. Table with column mapping enabled do not support all use cases and demonstrate different behavior. See [_](#column-mapping-limitations).\n\nIn addition to the data columns from the schema of the Delta table, change data feed contains metadata columns that identify the type of change event:\n\n| Column name                            | Type      | Values                                                               |\n|----------------------------------------|-----------|----------------------------------------------------------------------|\n| `_change_type`                         | String    | `insert`, `update_preimage` , `update_postimage`, `delete` [(1)](#1) |\n| `_commit_version`                      | Long      | The Delta log or table version containing the change.         ", "timestamp": 0, "next": ""}}], "question": "How can I read changes from a Delta table using versions?", "gold_answer": "To read changes from a Delta table using versions, you can use the Spark read API with specific options set for the Delta format. Here is an example of how you can do that in Python:\n\n```python\nspark.read.format(\"delta\") \\\n  .option(\"readChangeFeed\", \"true\") \\\n  .option(\"startingVersion\", 0) \\\n  .option(\"endingVersion\", 10) \\\n  .table(\"myDeltaTable\")\n```\n\nIn this example, the `readChangeFeed` option is set to `\"true\"` to indicate that you want to read the change data feed. The `startingVersion` is set to `0`, which is the initial version from where you want to start reading the changes. The `endingVersion` is set to `10`, which is the last version up to which you want to read the changes. This will read the changes made to the Delta table between versions 0 and 10 inclusive."}
{"hard_neg_docs": [{"page_content": "This copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\n---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I install the `renv` package in Databricks for notebook-scoped libraries?", "gold_answer": "To install the `renv` package in Databricks for notebook-scoped libraries, you can use the following R command:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to fix the package version."}
{"hard_neg_docs": [{"page_content": "This copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].", "fmt_content": "Source: https://docs.databricks.com/sparkr/renv.html\nContent:\n---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].", "metadata": {"source": "https://docs.databricks.com/sparkr/renv.html", "seq_num": 345, "description": "", "text": "---\ndescription: Learn how to utilize the third party R package `renv` to save and restore environment state on <Databricks>.\n---\n\n# `renv` on <Databricks>\n\n[renv](https://rstudio.github.io/renv/articles/renv.html) is an R package that lets users manage R dependencies specific to the notebook.\n\nUsing `renv`, you can create and manage the R library environment for your project, save the state of these libraries to a `lockfile`, and later restore libraries as required. Together, these tools can help make projects more isolated, portable, and reproducible.\n\n## Basic `renv` workflow\n\n.. contents:: In this section:\n    :depth: 1\n    :local:\n\n<a id=\"installRenv\"></a>\n\n### Install `renv`\n\nYou can install `renv` as a [cluster-scoped library](/libraries/cluster-libraries.md) or as a [notebook-scoped library](/libraries/notebooks-r-libraries.md). To install `renv` as a notebook-scoped library, use:\n\n```R\ninstall.packages(\"renv\", repo = \"https://cran.microsoft.com/snapshot/2021-07-16/\")\n```\n\nDatabricks recommends using a CRAN snapshot as the repository to [fix the package version][pin-r-packages].\n\n.. aws::\n    [pin-r-packages]: https://kb.databricks.com/r/pin-r-packages.html\n\n.. azure::\n    [pin-r-packages]: https://learn.microsoft.com/azure/databricks/kb/r/pin-r-packages\n\n.. gcp::\n    [pin-r-packages]: https://kb.gcp.databricks.com/r/pin-r-packages.html\n\n### Initialize `renv` session with pre-installed R libraries\n\nThe first step when using `renv` is to initialize a session using `renv::init()`. Set `libPaths` to change the default download location to be your [R notebook-scoped library path](/libraries/notebooks-r-libraries.md).\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths())\n```\n\n### Use `renv` to install additional packages\n\nYou can now use `renv`’s API to install and remove R packages. For example, to install the latest version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest\")\n```\n\nTo install an old version of `digest`, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"digest@0.6.18\")\n```\n\nTo install `digest` from GitHub, run the following inside of a notebook cell.\n\n```R\nrenv::install(\"eddelbuettel/digest\")\n```\n\nTo install a package from Bioconductor, run the following inside of a notebook cell.\n\n```R\n# (note: requires the BiocManager package)\nrenv::install(\"bioc::Biobase\")\n```\n\nNote that the `renv::install` API uses the [_](#cache).\n\n### Use `renv` to save your R notebook environment to DBFS\n\nRun the following command once before saving the environment.\n\n```R\nrenv::settings$snapshot.type(\"all\")\n```\n\nThis sets `renv` to snapshot all packages that are installed into `libPaths`, not just the ones that are currently used in the notebook. See [renv documentation](https://rstudio.github.io/renv/reference/snapshot.html#snapshot-type) for more information.\n\nNow you can run the following inside of a notebook cell to save the current state of your environment.\n\n```R\nrenv::snapshot(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/WANT/TO/SAVE/renv.lock\", force=TRUE)\n```\n\nThis updates the `lockfile` by capturing all packages installed on `libPaths`. It also moves your `lockfile` from the local filesystem to [DBFS](/dbfs/index.md), where it persists even if your cluster terminates or restarts.\n\n### Reinstall a `renv` environment given a `lockfile` from DBFS\n\nFirst, make sure that your new cluster is running an identical <DBR> version as the one you first created the `renv` environment on. This ensures that the pre-installed R packages are identical. You can find a list of these in each runtime's [release notes](/release-notes/runtime/releases.md).  After you [_](#installRenv), run the following inside of a notebook cell.\n\n```R\nrenv::init(settings = list(external.libraries=.libPaths()))\n.libPaths(c(.libPaths()[2], .libPaths()))\nrenv::restore(lockfile=\"/dbfs/PATH/TO/WHERE/YOU/SAVED/renv.lock\", exclude=c(\"Rserve\", \"SparkR\"))\n```\n\nThis copies your `lockfile` from DBFS into the local file system and then restores any packages specified in the `lockfile`.\n\n.. note:: To avoid missing repository errors, exclude the `Rserve` and `SparkR` packages from package restoration. Both of these packages are pre-installed in all runtimes.\n\n<a id=\"cache\"></a>\n\n## `renv` Cache\n\nA very useful feature of `renv` is its [global package cache](https://rstudio.github.io/renv/articles/renv.html#cache-1), which is shared across all `renv` projects on the cluster. It speeds up installation times and saves disk space. The `renv` cache does not cache packages downloaded via the `devtools` API or `install.packages()` with any additional arguments other than `pkgs`.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the `renv` package in R on Databricks?", "gold_answer": "The purpose of the `renv` package in R on Databricks is to manage R dependencies specific to the notebook. It allows users to create and manage the R library environment for their project, save the state of these libraries to a lockfile, and later restore the libraries as required. This helps make projects more isolated, portable, and reproducible."}
{"hard_neg_docs": [{"page_content": "[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are the two types of inference available for models trained using Databricks Feature Store?", "gold_answer": "The two types of inference available for models trained using Databricks Feature Store are:\n- Batch inference: Feature values are retrieved from the offline store and joined with new data prior to scoring.\n- Real-time inference: Feature values are retrieved from the online store."}
{"hard_neg_docs": [{"page_content": "[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/machine-learning/feature-store/concepts.html\nContent:\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/machine-learning/feature-store/concepts.html", "seq_num": 246, "description": "", "text": "---\ndescription: Learn about Databricks Feature Store concepts.\n---\n\n# Feature Store concepts\n\nThis section describes concepts to help you use Databricks Feature Store and feature tables.\n\n.. azure-aws::\n\n  .. note:: At this time, Databricks Feature Store does not support writing to a <UC> metastore. In <UC>-enabled workspaces, you can write feature tables to the default Hive metastore.\n\n## Feature tables\n\nFeatures are organized as feature tables. Each table is backed by a [Delta table](/delta/index.md) and additional metadata.\n\nA feature table must have a primary key. Features in a feature table are typically computed and updated using a common computation function.\n\nFeature table metadata tracks the data sources from which a table was generated and the notebooks and jobs that created or wrote to the table.\n\n.. azure-aws::\n\n  You can publish a feature table to an [online store](#online-store-1) for real-time model inference.\n\nYou can [create tags](/machine-learning/feature-store/feature-tables.md#work-with-feature-table-tags) and associate them with a feature table to facilitate grouping and discovery.\n\n## Time series feature tables\n\nThe data used to train a model often has time dependencies built into it. When you build the model, you must consider only feature values up until the time of the observed target value. If you train on features based on data measured after the timestamp of the target value, the model's performance may suffer.\n\n[Time series feature tables](/machine-learning/feature-store/time-series.md) include a timestamp key column that ensures that each row in the training dataset represents the latest known feature values as of the row’s timestamp. You should use time series feature tables whenever feature values change over time, for example with time series data, event-based data, or time-aggregated data.\n\nWhen you create a time series feature table, you specify time-related keys using the `timestamp_keys` argument. This enables point-in-time lookups when you use `create_training_set` or `score_batch`. The system performs an as-of timestamp join, using the `timestamp_lookup_key` you specify.\n\nIf you do not use the `timestamp_keys` argument, and instead designate a timestamp column as [a primary key column](/machine-learning/feature-store/feature-tables.md#store-past-values-of-daily-features), feature store does not apply point-in-time logic to the timestamp column during joins. Instead, it matches only rows with an exact time match instead of matching all rows prior to the timestamp.\n\n## Offline store\n\nThe offline feature store is used for feature discovery, model training, and batch inference. It contains feature tables materialized as [Delta tables](/delta/index.md).\n\n<!-- anchor required since entire section is conditionalized, can remove when online stores are available for GCP-->\n\n<a id=\"online-store-1\"></a>\n\n.. azure-aws::\n\n  ## Online store\n\n  An online feature store is a low-latency database used for real-time model inference. For a list of online stores that <Databricks> supports, see [_](/machine-learning/feature-store/online-feature-stores.md).\n\n## Streaming\n\nIn addition to batch writes, Databricks Feature Store supports streaming. You can write feature values to a feature table from a streaming source, and feature computation code can utilize [Structured Streaming](/structured-streaming/index.md) to transform raw data streams into features.\n\n.. azure-aws::\n\n  You can also stream feature tables from the offline store to an online store.\n\n## Training set\n\nA training set consists of a list of features and a DataFrame containing raw training data, labels, and primary keys by which to look up features. You create the training set by specifying features to extract from Feature Store, and provide the training set as input during model training.\n\nSee [_](/machine-learning/feature-store/train-models-with-feature-store.md#create-a-training-dataset) for an example of how to create and use a training set.\n\n## Model packaging\n\nA machine learning model trained using features from Databricks Feature Store retains references to these features. At inference time, the model can optionally retrieve feature values from Feature Store. The caller only needs to provide the primary key of the features used in the model (for example, `user_id`), and the model retrieves all required feature values from Feature Store.\n\nIn batch inference, feature values are retrieved from the offline store and joined with new data prior to scoring.\nIn real-time inference, feature values are retrieved from the online store.\n\nTo package a model with feature metadata, use `FeatureStoreClient.log_model()`.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I package a model with feature metadata in Databricks Feature Store?", "gold_answer": "To package a model with feature metadata in Databricks Feature Store, you can use the `FeatureStoreClient.log_model()` method."}
{"hard_neg_docs": [{"page_content": ",\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}, {"page_content": "## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}], "question": "What are some methods that do not provide certain columns when writing into a Delta table?", "gold_answer": "Some methods that do not provide certain columns when writing into a Delta table include:\n\n- JDBC or ODBC\n- JAR job\n- spark-submit job\n- Running a command using the REST API"}
{"hard_neg_docs": [{"page_content": ",\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}, {"page_content": "## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"", "fmt_content": "Source: https://docs.databricks.com/delta/history.html\nContent:\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"", "metadata": {"source": "https://docs.databricks.com/delta/history.html", "seq_num": 1333, "description": "", "text": "---\ndescription: Review and navigate <Delta> table versions using history and time travel commands.\n---\n\n# Work with <Delta> table history\n\nEach operation that modifies a <Delta> table creates a new table version. You can use history information to audit operations or query a table at a specific point in time.\n\n## Retrieve Delta table history\n\nYou can retrieve information on the operations, user, timestamp, and so on for each write to a Delta table\nby running the `history` command. The operations are returned in reverse chronological order. By default table history is retained for 30 days.\n\n```sql\nDESCRIBE HISTORY '/data/events/'          -- get the full history of the table\n\nDESCRIBE HISTORY delta.`/data/events/`\n\nDESCRIBE HISTORY '/data/events/' LIMIT 1  -- get the last operation only\n\nDESCRIBE HISTORY eventsTable\n```\n\nFor Spark SQL syntax details, see [_](/sql/language-manual/delta-describe-history.md).\n\nSee the [_](/delta/index.md#delta-api) for Scala/Java/Python syntax details.\n\n[Data Explorer](/data/index.md) provides a visual view of this detailed table information and history for Delta tables. In addition to the table schema and sample data, you can click the **History** tab to see the table history that displays with `DESCRIBE HISTORY`.\n\n## History schema\n\nThe output of the `history` operation has the following columns.\n\n| Column | Type | Description |\n| --- | --- | ---|\n| version | long | Table version generated by the operation. |\n| timestamp | timestamp | When this version was committed. |\n| userId | string | ID of the user that ran the operation. |\n| userName | string | Name of the user that ran the operation. |\n| operation | string | Name of the operation. |\n| operationParameters | map | Parameters of the operation (for example, predicates.) |\n| job | struct | Details of the job that ran the operation.  |\n| notebook | struct | Details of notebook from which the operation was run. |\n| clusterId | string | ID of the cluster on which the operation ran. |\n| readVersion | long | Version of the table that was read to perform the write operation. |\n| isolationLevel | string | Isolation level used for this operation. |\n| isBlindAppend | boolean | Whether this operation appended data. |\n| operationMetrics | map | Metrics of the operation (for example, number of rows and files modified.) |\n| userMetadata | string | User-defined commit metadata if it was specified |\n\n\n```text\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|version|          timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n|      5|2019-07-29 14:07:47|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          4|WriteSerializable|        false|[numTotalRows -> ...|\n|      4|2019-07-29 14:07:41|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          3|WriteSerializable|        false|[numTotalRows -> ...|\n|      3|2019-07-29 14:07:29|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          2|WriteSerializable|        false|[numTotalRows -> ...|\n|      2|2019-07-29 14:06:56|   ###|     ###|   UPDATE|[predicate -> (id...|null|     ###|      ###|          1|WriteSerializable|        false|[numTotalRows -> ...|\n|      1|2019-07-29 14:04:31|   ###|     ###|   DELETE|[predicate -> [\"(...|null|     ###|      ###|          0|WriteSerializable|        false|[numTotalRows -> ...|\n|      0|2019-07-29 14:01:40|   ###|     ###|    WRITE|[mode -> ErrorIfE...|null|     ###|      ###|       null|WriteSerializable|         true|[numFiles -> 2, n...|\n+-------+-------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+\n```\n\n.. note::\n  - A few of the other columns are not available if you write into a Delta table using the following methods:\n\n    - [JDBC or ODBC](/integrations/jdbc-odbc-bi.md)\n    - [JAR job](/dev-tools/api/latest/examples.md#spark-jar-job)\n    - [spark-submit job](/dev-tools/api/latest/examples.md#spark-submit-api-example)\n    - [Run a command using the REST API](/dev-tools/api/1.2/index.md#command-execution)\n\n  - Columns added in the future will always be added after the last column.\n\n<a id=\"delta-history-metrics\"></a>\n\n## Operation metrics keys\n\nThe `history` operation returns a collection of operations metrics in the `operationMetrics` column map.\n\nThe following tables list the map key definitions by operation.\n\n.. csv-table::\n  :header: \"Operation\", \"Metric name\", \"Description\"\n\n  \"WRITE, CREATE TABLE AS SELECT, REPLACE TABLE AS SELECT, COPY INTO\",,\n  ,\"numFiles\", Number of files written.\n  ,\"numOutputBytes\", Size in bytes of the written contents.\n  ,\"numOutputRows\", Number of rows written.\n  STREAMING UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numOutputRows\", Number of rows written.\n  ,\"numOutputBytes\", Size of write in bytes.\n  DELETE,,\n  ,\"numAddedFiles\", Number of files added. Not provided when partitions of the table are deleted.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numDeletedRows\", Number of rows removed. Not provided when partitions of the table are deleted.\n  ,\"numCopiedRows\", Number of rows copied in the process of deleting files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  TRUNCATE,,\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  MERGE,,\n  ,\"numSourceRows\", Number of rows in the source DataFrame.\n  ,\"numTargetRowsInserted\", Number of rows inserted into the target table.\n  ,\"numTargetRowsUpdated\", Number of rows updated in the target table.\n  ,\"numTargetRowsDeleted\", Number of rows deleted in the target table.\n  ,\"numTargetRowsCopied\", Number of target rows copied.\n  ,\"numOutputRows\", Total number of rows written out.\n  ,\"numTargetFilesAdded\", Number of files added to the sink(target).\n  ,\"numTargetFilesRemoved\", Number of files removed from the sink(target).\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  UPDATE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files removed.\n  ,\"numUpdatedRows\", Number of rows updated.\n  ,\"numCopiedRows\", Number of rows just copied over in the process of updating files.\n  ,\"executionTimeMs\", Time taken to execute the entire operation.\n  ,\"scanTimeMs\", Time taken to scan the files for matches.\n  ,\"rewriteTimeMs\", Time taken to rewrite the matched files.\n  FSCK,\"numRemovedFiles\", Number of files removed.\n  CONVERT,\"numConvertedFiles\", Number of Parquet files that have been converted.\n  OPTIMIZE,,\n  ,\"numAddedFiles\", Number of files added.\n  ,\"numRemovedFiles\", Number of files optimized.\n  ,\"numAddedBytes\", Number of bytes added after the table was optimized.\n  ,\"numRemovedBytes\", Number of bytes removed.\n  ,\"minFileSize\", Size of the smallest file after the table was optimized.\n  ,\"p25FileSize\", Size of the 25th percentile file after the table was optimized.\n  ,\"p50FileSize\", Median file size after the table was optimized.\n  ,\"p75FileSize\", Size of the 75th percentile file after the table was optimized.\n  ,\"maxFileSize\", Size of the largest file after the table was optimized.\n  CLONE,,\n  ,\"sourceTableSize\", Size in bytes of the source table at the version that's cloned.\n  ,\"sourceNumOfFiles\", Number of files in the source table at the version that's cloned.\n  ,\"numRemovedFiles\", Number of files removed from the target table if a previous Delta table was replaced.\n  ,\"removedFilesSize\", Total size in bytes of the files removed from the target table if a previous Delta table was replaced.\n  ,\"numCopiedFiles\", Number of files that were copied over to the new location. 0 for shallow clones.\n  ,\"copiedFilesSize\", Total size in bytes of the files that were copied over to the new location. 0 for shallow clones.\n  RESTORE,,\n  ,\"tableSizeAfterRestore\", Table size in bytes after restore.\n  ,\"numOfFilesAfterRestore\", Number of files in the table after restore.\n  ,\"numRemovedFiles\", Number of files removed by the restore operation.\n  ,\"numRestoredFiles\", Number of files that were added as a result of the restore.\n  ,\"removedFilesSize\", Size in bytes of files removed by the restore.\n  ,\"restoredFilesSize\", Size in bytes of files added by the restore.\n  VACUUM,,\n  ,numDeletedFiles, Number of deleted files.\n  ,numVacuumedDirectories, Number of vacuumed directories.\n  ,numFilesToDelete, Number of files to delete.\n\n## Query an older snapshot of a table (time travel)\n\n<Delta> time travel allows you to query an older snapshot of a Delta table. Time travel has many use cases, including:\n\n- Re-creating analyses, reports, or outputs (for example, the output of a machine learning model). This could be useful for debugging or auditing, especially in regulated industries.\n- Writing complex temporal queries.\n- Fixing mistakes in your data.\n- Providing snapshot isolation for a set of queries for fast changing tables.\n\n## <Delta> time travel syntax\n\n<Delta> supports querying previous table versions based on timestamp or table version (as recorded in the transaction log).\n\n.. include:: /shared/delta-timetravel.md\n\nOnly date or timestamp strings are accepted. For example, `\"2019-01-01\"` and `\"2019-01-01T00:00:00.000Z\"`. See the following code for example syntax:\n\n.. code-language-tabs::\n  ```sql\n  SELECT * FROM people10m TIMESTAMP AS OF '2018-10-18T22:15:12.013Z'\n  SELECT * FROM delta.`/tmp/delta/people10m` VERSION AS OF 123\n  ```\n\n  ```python\n  df1 = spark.read.option(\"timestampAsOf\", \"2019-01-01\").table(\"people10m\")\n  df2 = spark.read.option(\"versionAsOf\", 123).load(\"/", "timestamp": 0, "next": ""}}], "question": "How can I retrieve operation metrics using the `history` operation in Delta tables?", "gold_answer": "To retrieve operation metrics using the `history` operation in Delta tables, you would use the `operationMetrics` column map that is returned with the `history` operation. The `operationMetrics` column contains a collection of operation metrics keys and their corresponding values, which provide insights into the operations performed on the Delta table.\n\nThe document does not provide specific commands or methods to access these metrics, but typically you would use a query that invokes the `history` operation on your Delta table, and then you would extract the desired metrics from the `operationMetrics` map.\n\nFor example, in Databricks you might run a SQL query like:\n\n```sql\nDESCRIBE HISTORY my_delta_table\n```\n\nThis would return the history of operations on the Delta table `my_delta_table`, including the `operationMetrics` map. You can then examine this map to understand various metrics related to each operation, such as the number of files added, number of files removed, and so on, depending on the specific operation and the keys available in the `operationMetrics` map.\n\nPlease note that some columns may not be available if the Delta table is written using methods such as JDBC/ODBC, JAR job, spark-submit job, or command execution using the REST API."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}, {"page_content": "#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}], "question": "What attributes should be added in the Okta Admin Console for passing through SAML to Databricks?", "gold_answer": "In the Okta Admin Console for passing through SAML to Databricks, the following attributes should be added:\n\n1. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n   - Name format: URI Reference\n   - Value: `user.login`\n\n2. Name: `https://aws.amazon.com/SAML/Attributes/Role`\n   - Name format: URI Reference\n   - Value: (The value is not provided in the text, it likely involves an expression to match groups and roles, but the specific value is not included in the provided content.)"}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}, {"page_content": "#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html\nContent:\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-federation.html", "seq_num": 1526, "description": "", "text": "---\ndescription: Learn how to use AWS IAM federation for IAM credential passthrough.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SAML 2.0 federation (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nAWS supports SAML 2.0 [identity federation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml.html) to allow for single-sign on to AWS Management Console and AWS APIs. Databricks workspaces that are configured with [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) can use AWS IAM federation to maintain the mapping of users to IAM roles within their identity provider (IdP) rather than within Databricks using SCIM. This allows you to centralize data access within your IdP and have those entitlements pass directly to Databricks clusters.\n\nThe following diagram illustrates the federation workflow:\n\n![Federation workflow](/_static/images/security/credential-passthrough/federation-workflow.png)\n\n#. Configure a trust relationship between your IdP and AWS accounts in order for the IdP to control which roles users can assume.\n#. Users login to Databricks via SAML SSO, the entitlement to the roles are passed by the IdP.\n#. Databricks calls the AWS Security Token Service (STS) and assumes the roles for the user by passing the SAML response and getting temporary tokens.\n#. When a user accesses S3 from a Databricks cluster, Databricks runtime uses the temporary tokens for the user to perform the access automatically and securely.\n\n.. note:: Federation for IAM credential passthrough always maps roles to users in SAML when the [Allow IAM role entitlement auto sync](#sync-role-mappings-scim) is enabled. It will overwrite any previous roles set via the [SCIM API](/dev-tools/api/latest/scim/index.md).\n\n## Requirements\n\n- <Security plan>.\n- SAML [single sign-on](/administration-guide/users-groups/single-sign-on/index.md) configured in your Databricks workspace.\n- AWS administrator access to:\n  - IAM roles and policies in the AWS account of the Databricks deployment.\n  - AWS account of the S3 bucket.\n- Identity provider (IdP) administrator to configure your IdP to pass AWS roles to Databricks.\n- A Databricks admin to include AWS roles in the SAML assertion.\n\n## Step 1: Get the Databricks SAML URL\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Click the **Single Sign-On** tab.\n#. Copy the Databricks SAML URL.\n\n   ![SAML URL](/_static/images/security/credential-passthrough/databricks-saml-url.png)\n\n## Step 2: Download identity provider metadata\n\n.. note:: The steps within the identity provider console vary slightly for each identity provider. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n#. In your identity provider admin console, find your Databricks application for single sign-on.\n#. Download the SAML metadata.\n\n   ![Add attribute](/_static/images/security/credential-passthrough/identity-provider-metadata.png)\n\n## Step 3: Configure the identity provider\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Identity Providers** tab in the sidebar.\n#. Click **Create Provider**.\n   a. In Provider Type, select **SAML**.\n   #. In Provider Name, enter a name.\n   #. In **Metadata Document**, click **Choose File** and navigate to the file containing the metadata document you downloaded above.\n   #. Click **Next Step** and then **Create**.\n\n<a id=\"configure-iam-role-for-federation\"></a>\n\n## Step 4: Configure the IAM role for federation\n\n.. note:: Only roles used for data access should be used for federation with Databricks. We do not recommend allowing roles normally used for AWS console access as they may have more privileges than necessary.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **SAML 2.0 federation**.\n   #. In SAML provider, select the name created in Step 3.\n   #. Select **Allow programmatic access only**.\n   #. In Attribute, select **SAML:aud**.\n   #. In Value, paste the Databricks SAML URL you copied in Step 1.\n   #. Click **Next: Permissions**, **Next: Tags**, and **Next: Review**.\n   #. In the Role Name field, type a role name.\n   #. Click **Create role**. The list of roles displays.\n#. Add an inline policy to the role. This policy grants access to the S3 bucket.\n   a. In the Permissions tab, click <Inline policy>.\n   #. Click the **JSON** tab.\n      Copy this policy and set `<s3-bucket-name>` to the name of your bucket.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:ListBucket\"\n            ],\n           \"Resource\": [\n              \"arn:aws:s3:::<s3-bucket-name>\"\n            ]\n          },\n          {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n              \"s3:PutObject\",\n              \"s3:GetObject\",\n              \"s3:DeleteObject\",\n              \"s3:PutObjectAcl\"\n            ],\n            \"Resource\": [\n               \"arn:aws:s3:::<s3-bucket-name>/*\"\n            ]\n          }\n        ]\n      }\n      ```\n\n   #. Click **Review policy**.\n   #. In the Name field, type a policy name.\n   #. Click **Create policy**.\n#. In the **Trusted Relationships** tab, you should be able to see something similar to:\n\n   ![Trust relationship](/_static/images/security/credential-passthrough/edit-trust-relationship.png)\n\n#. Click the **Edit trust relationship** button. The IAM resulting trust policy document should be similar to the following:\n\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [\n       {\n         \"Effect\": \"Allow\",\n         \"Principal\": {\n           \"Federated\": \"arn:aws:iam::<accountID>:saml-provider/<IdP-name>\"\n         },\n         \"Action\": \"sts:AssumeRoleWithSAML\",\n         \"Condition\": {\n           \"StringEquals\": {\n             \"SAML:aud\": \"https://xxxxxx.cloud.databricks.com/saml/consume\"\n           }\n         }\n       }\n     ]\n   }\n   ```\n\n## Step 5: Configure the identity provider to pass attributes to Databricks\n\nThe following attributes must be passed to Databricks in the SAML response via SSO in order for Databricks to pass roles to clusters:\n\n* `https://aws.amazon.com/SAML/Attributes/Role`\n* `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n\nThese attributes are the list of role ARNs and the username matching the single sign-on login. Role mappings are refreshed when a user logs in to the Databricks workspace.\n\n.. note:: If user entitlement to the IAM roles is based on AD/LDAP group membership, you must configure that group to role mapping per your IdP.\n\nEach identity provider differs in how you add attributes to pass through SAML. The following section shows one example with Okta. See [Integrating Third-Party SAML Solution Providers with AWS](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_saml_3rd-party.html) for examples with your identity provider.\n\n### Okta example\n\n#. In the Okta Admin Console under Applications, select your Single Sign-On to Databricks application.\n#. Click **Edit** under SAML Settings and click **Next** to the **Configure SAML** tab.\n#. In Attribute Statements add the following attributes:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`, Name format: URI Reference, Value: `user.login`\n\n#. To manage the roles easily using groups, create groups corresponding to your IAM roles, for example `GroupA` and `GroupB`, and add the users to those groups.\n#. You can use Okta Expressions to match groups and roles in the following way:\n   a. Name: `https://aws.amazon.com/SAML/Attributes/Role`, Name format: `URI Reference`, Value:\n\n      ```text\n      Arrays.flatten(isMemberOfGroupName(\"GroupA\") ? \"arn:aws:iam::xxx:role/role-a,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {}, isMemberOfGroupName(\"GroupB\") ? \"arn:aws:iam::xxx:role/role-b,arn:aws:iam::xxx:saml-provider/okta-databricks\" : {})\n      ```\n      It should look like:\n\n      ![Okta expression](/_static/images/security/credential-passthrough/okta-expression.png)\n\n      Only users in a certain group would have permission to use the corresponding IAM role.\n\n#. Use **Manage People** to add users to the group.\n#. Use **Manage Apps** to assign the group to the SSO application to allow users to log in to Databricks.\n\nTo add additional roles follow the steps above, mapping an Okta group to a federated role. To have roles in different AWS accounts, add the SSO application as a new IAM identity provider to each additional AWS account that will have federated roles for Databricks.\n\n<a id=\"sync-role-mappings-scim\"></a>\n\n## Step 6:", "timestamp": 0, "next": ""}}], "question": "How do I configure group to role mapping for IAM roles based on AD/LDAP group membership?", "gold_answer": "To configure group to role mapping for IAM roles based on AD/LDAP group membership, you must follow the configuration settings specific to your identity provider (IdP). The provided document does not give detailed steps for all IdPs but outlines the process for Okta as an example. Here's what you would do in the case of Okta:\n\n1. Go to the Okta Admin Console and select your Single Sign-On to Databricks application.\n2. Click **Edit** under SAML Settings and proceed to the **Configure SAML** tab.\n3. In the Attribute Statements section, you would add the necessary attributes for SAML to recognize the group to role mapping. The document provides an example attribute for the RoleSessionName:\n\n   - Name: `https://aws.amazon.com/SAML/Attributes/RoleSessionName`\n   - Name format: URI Reference\n   - Value: `user.login`\n\n4. To manage the roles easily using groups within Okta, you would create groups corresponding to your IAM roles (e.g., `GroupA` and `GroupB`) and add the users to those groups.\n5. It is suggested that you can use Okta Expressions to match groups and roles, although the exact expression to use is not provided in the"}
{"hard_neg_docs": [{"page_content": "- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}, {"page_content": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}], "question": "How can I configure roles for IAM credential passthrough in Databricks?", "gold_answer": "To configure roles for IAM credential passthrough in Databricks, you need to:\n\n1. **Create a data IAM role**: Use an existing data IAM role or create a new one that can access S3 buckets. You can optionally follow the provided tutorial to create a data IAM role.\n\n2. **Configure a meta IAM role**: Set up your meta IAM role to assume the data IAM role.\n\nUsers are granted access to these data IAM roles using the SCIM API. When using a cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you have access to. This setup allows multiple users with different data access policies to share one Databricks cluster while maintaining secure data access."}
{"hard_neg_docs": [{"page_content": "- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}, {"page_content": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.", "fmt_content": "Source: https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html\nContent:\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.", "metadata": {"source": "https://docs.databricks.com/data-governance/credential-passthrough/iam-passthrough.html", "seq_num": 1529, "description": "", "text": "﻿---\ndescription: Learn how to use IAM credential passthrough to enable secure access to S3.\n---\n\n.. azure-gcp::\n\n  ---\n  orphan: 1\n  ---\n\n# Access S3 with IAM credential passthrough with SCIM (legacy)\n\n.. include:: /shared/preview.md\n\n.. note::\n  IAM credential passthrough is a legacy data governance model. Databricks recommends that you upgrade to <UC>. <UC> simplifies security and governance of your data by providing a central place to administer and audit data access across multiple workspaces in your account. See[_](/data-governance/unity-catalog/index.md).\n\nIAM credential passthrough allows you to authenticate automatically to S3 buckets from <Databricks> clusters using the identity that you use to log in to <Databricks>. When you enable IAM credential passthrough for your cluster, commands that you run on that cluster can read and write data in S3 using your identity. IAM credential passthrough has two key benefits over securing access to S3 buckets using [instance profiles](/aws/iam/instance-profile-tutorial.md):\n\n- IAM credential passthrough allows multiple users with different data access policies to share one <Databricks> cluster to access data in S3 while always maintaining data security. An instance profile can be associated with only one [IAM role](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html). This requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\n- IAM credential passthrough associates a user with an identity. This in turn enables S3 object logging via CloudTrail. All S3 access is tied directly to the user via the ARN in CloudTrail logs.\n\n## Requirements\n\n- <Security plan>.\n- AWS administrator access to:\n\n  - IAM roles and policies in the AWS account of the <Databricks> deployment.\n  - AWS account of the S3 bucket.\n  - <Databricks> administrator access to configure instance profiles.\n\n<a id=\"meta-instance-profiles\"></a>\n\n## Set up a meta instance profile\n\nIn order to use IAM credential passthrough, you must first set up at least one  _meta instance profile_ to assume the IAM roles that you assign to your users.\n\nAn _IAM role_ is an AWS identity with policies that determine what the identity can and cannot do in AWS. An _instance\nprofile_ is a container for an IAM role that you can use to pass the role information to an EC2 instance when\nthe instance starts. Instance profiles allow you to access data from <Databricks> clusters\nwithout having to [embed your AWS keys](/aws/iam/instance-profile-tutorial.md) in notebooks.\n\nWhile instance profiles make configuring roles on clusters very simple, an instance profile can be associated with only _one_ IAM role.\nThis requires all users on <a Databricks> cluster to share that role and the data access policies of that role.\nHowever, IAM roles can be used to assume other IAM roles or to access data directly\nthemselves. Using the credentials for one role to assume a different role is called [role\nchaining](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html#iam-term-role-chaining).\n\nIAM credential passthrough allows admins to split the IAM role the instance profile is using and\nthe roles users use to access data. In <Databricks>, we call the instance role the _meta IAM\nrole_ and the data access role the _data IAM role_. Similar to the instance profile,\na _meta instance profile_ is a container for a meta IAM role.\n\n![Meta instance profile](/_static/images/security/credential-passthrough/meta-iam-role.png)\n\nUsers are granted access to data IAM roles using the [SCIM API](/dev-tools/api/latest/scim/index.md).\nIf you are mapping roles with your identity provider, then those roles will sync to the <Databricks> SCIM API. When you use\na cluster with credential passthrough and a meta instance profile, you can assume only the data IAM roles that you can\naccess. This allows multiple users with different data access policies to share one <Databricks> cluster while keeping data secure.\n\nThis section describes how to set up the meta instance profile required to enable IAM credential passthrough.\n\n### Step 1: Configure roles for IAM credential passthrough\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Create a data IAM role\n\nUse an existing data IAM role or optionally follow [_](/aws/iam/instance-profile-tutorial.md) to create a data IAM role that can access S3 buckets.\n\n<a id=\"meta-iam-role\"></a>\n\n#### Configure a meta IAM role\n\nConfigure your meta IAM role to assume the data IAM role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Click **Create role**.\n   a. Under **Select type of trusted entity**, select **AWS** service.\n   #. Click the **EC2** service.\n#. Click **Next Permissions**.\n#. Click **Create Policy**. A new window opens.\n   a. Click the **JSON** tab.\n   #. Copy the following policy and set `<account-id>` to your AWS Account ID and `<data-iam-role>` to the name of your data IAM role from  the preceding section.\n\n      ```json\n      {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n          {\n            \"Sid\": \"AssumeDataRoles\",\n            \"Effect\": \"Allow\",\n            \"Action\": \"sts:AssumeRole\",\n            \"Resource\": [\n              \"arn:aws:iam::<account-id>:role/<data-iam-role>\"\n            ]\n          }\n        ]\n      }\n      ```\n   #. Click **Review Policy**.\n   #. In the Name field, type a policy name and click **Create policy**.\n\n#. Return to the role window and refresh it.\n#. Search for the policy name and select the checkbox next to the policy name.\n#. Click **Next Tags** and **Next Review**.\n#. In the Role name file, type a name for the meta IAM role.\n#. Click **Create role**.\n#. In the role summary, copy the **Instance Profile ARN**.\n\n#### Configure the data IAM role to trust the meta IAM role\n\nTo make the meta IAM role able to assume the data IAM role, you make the meta role trusted by the data role.\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Find the data role created in the previous step and click it to go to the role detail page.\n#. Click the **Trust relationships** tab and add the following statement if not set:\n\n   ```json\n    {\n      \"Version\": \"2012-10-17\",\n      \"Statement\": [\n        {\n          \"Effect\": \"Allow\",\n          \"Principal\": {\n            \"AWS\": \"arn:aws:iam::<account-id>:role/<meta-iam-role>\"\n          },\n          \"Action\": \"sts:AssumeRole\"\n        }\n      ]\n    }\n   ```\n\n### Step 2: Configure a meta instance profile in <Databricks>\n\nThis section describes how to configure a meta instance profile in <Databricks>.\n\n.. contents:: In this section:\n  :local:\n  :depth: 1\n\n#### Determine the IAM role used for <Databricks> deployment\n\n#. Go to the [account console](/administration-guide/account-settings/index.md#account-console).\n#. Click the **Workspaces** icon.\n#. Click on the name of your workspace.\n#. Note the role name at the end of the ARN key in the credentials section, in the image below it's`testco-role`.\n\n   ![Role arn](/_static/images/security/credential-passthrough/e2-role-arn.png)\n\n#### Modify policy in the IAM role used for <Databricks> deployment\n\n#. In the AWS console, go to the **IAM** service.\n#. Click the **Roles** tab in the sidebar.\n#. Edit the role you noted in the preceding section.\n#. Click the policy attached to the role.\n#. Modify the policy to allow the EC2 instances for the Spark clusters within <Databricks> to use the meta instance profile you created in [_](#meta-iam-role). For an example, see [_](/aws/iam/add-instance-profile-workspace-role.md).\n#. Click **Review policy** and **Save Changes**.\n\n<a id=\"add-meta-instance-profile\"></a>\n\n#### Add the meta instance profile to <Databricks>\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n#. Select the **Instance Profiles** tab.\n#. Click the **Add Instance Profile** button. A dialog appears.\n#. Paste in the Instance Profile ARN for the meta IAM role from [_](#meta-iam-role).\n#. Check the **Meta Instance Profile** checkbox and click **Add**.\n\n   ![Add instance profile](/_static/images/security/credential-passthrough/add-instance-profile.png)\n\n#. Optionally identify users who can launch clusters with the meta instance profile.\n\n   ![Configure instance profile](/_static/images/security/credential-passthrough/configure-instance-profile.png)\n\n### Step 3: Attach IAM role permissions to <Databricks> users\n\nThere are two ways to maintain the mapping of users to IAM roles:\n\n- Within <Databricks> using the [SCIM Users API](/dev-tools/api/latest/scim/scim-users.md#add-role-to-user) or [SCIM Groups API](/dev-tools/api/latest/scim/scim-groups.md#add-role-to-group).\n\n- Within your [identity provider](iam-federation.md). This allows you to centralize data access and pass those entitlements directly to <Databricks> clusters via SAML 2.0 identity federation.\n\nUse the following chart to help you decide which mapping method is better for your workspace:\n\n+---------------------------------+--------+-------------------+\n| Requirement                     | SCIM   | Identity Provider |\n+=================================+========+===================+\n| Single sign-on to <Databricks>  | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure AWS identity provider | No     | Yes               |\n+---------------------------------+--------+-------------------+\n| Configure meta instance profile | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| <Databricks> admin              | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| AWS admin                       | Yes    | Yes               |\n+---------------------------------+--------+-------------------+\n| Identity provider admin     ", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the meta instance profile in IAM credential passthrough in Databricks?", "gold_answer": "The purpose of the meta instance profile in IAM credential passthrough in Databricks is to enable users to assume only the data IAM roles that they can access while using a cluster with credential passthrough. This setup allows multiple users with different data access policies to share one Databricks cluster while keeping data secure. It essentially facilitates the management of data access permissions in a secure and scalable manner within Databricks."}
{"hard_neg_docs": [{"page_content": "\"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\n\"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}, {"page_content": "Answer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\",", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\nto SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\",", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}], "question": "How can I run a set of SQL expressions using selectExpr() in Spark?", "gold_answer": "To run a set of SQL expressions using `selectExpr()` in Spark, you would use the `selectExpr()` method on a DataFrame object. This method takes a set of SQL expressions as arguments and evaluates them as if they were part of a SQL SELECT statement. Here's an example of how you might use `selectExpr()` with a DataFrame:\n\n```scala\nimport sqlContext.implicits._\n\nval df = Seq(\n  (1, \"First Value\"),\n  (2, \"Second Value\")\n).toDF(\"int_column\", \"string_column\")\n\n// Using selectExpr to run SQL expressions\nval modifiedDF = df.selectExpr(\"int_column\", \"string_column\", \"CURRENT_TIMESTAMP as current_time\")\n```\n\nIn the example above, `selectExpr()` is used to select the existing columns `int_column` and `string_column` from the DataFrame and also to add a new column called `current_time` with the current timestamp for each row. The SQL expression `CURRENT_TIMESTAMP` is evaluated by `selectExpr()`.\n\nIf you wanted to use `timestamp_millis` or `unix_millis` with a DataFrame, as mentioned in the document provided, you would include them as arguments within the `selectExpr()` method. Here's a hypothetical example assuming you"}
{"hard_neg_docs": [{"page_content": "\"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\n\"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}, {"page_content": "Answer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\",", "fmt_content": "Source: https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis\nContent:\nto SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\",", "metadata": {"source": "https://kb.databricks.com/scala/cannot-import-timestamp-millis-unix-millis", "seq_num": 2310, "description": "", "text": "Title: Cannot import timestamp_millis or unix_millis\n\nDescription: Cannot use timestamp_millis or unix_millis directly with a DataFrame. You must first use selectExpr() or use SQL commands.\n\nAnswer: Problem You are trying to import timestamp_millis or unix_millis into a Scala notebook, but get an error message. %scala import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} error: value timestamp_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{timestamp_millis, unix_millis} Cause The functions timestamp_millis and unix_millis are not available in the Apache Spark DataFrame API. These functions are specific to SQL and are included in Spark 3.1.1 and above. Solution You need to use selectExpr() with timestamp_millis or unix_millis if you want to use either one of them with a DataFrame. selectExpr() takes a set of SQL expressions and runs them. For example, this sample code returns an error message when run. %scala import sqlContext.implicits._ val df = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") import org.apache.spark.sql.functions.{unix_millis} import org.apache.spark.sql.functions.col df.select(unix_millis(col(\"int_column\"))).show() error: value unix_millis is not a member of object org.apache.spark.sql.functions import org.apache.spark.sql.functions.{unix_millis} While this sample code, using selectExpr(), successfully returns timestamp values. %scala import org.apache.spark.sql.functions._ import sqlContext.implicits._ val ndf = Seq(  (1, \"First Value\"),  (2, \"Second Value\") ).toDF(\"int_column\", \"string_column\") display(ndf.selectExpr(\"timestamp_millis(int_column)\")) Example notebook Review the Cannot import timestamp_millis or unix_millis example notebook.", "timestamp": 1653036825.731, "next": ""}}], "question": "What is the solution to resolve the error message when using timestamp_millis or unix_millis with a DataFrame?", "gold_answer": "The solution to resolve the error message when using `timestamp_millis` or `unix_millis` with a DataFrame is to use the `selectExpr()` method, which takes a set of SQL expressions and runs them on the DataFrame."}
{"hard_neg_docs": [{"page_content": "B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nB ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}, {"page_content": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nTitle: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nbased on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}], "question": "What are the solutions to mitigate the issue of different physical plans for identical Delta tables?", "gold_answer": "The provided document does not specify the exact solutions to mitigate the issue of different physical plans for identical Delta tables. It only indicates that if best practices for Delta tables are not followed, table statistics could be different, leading to different physical plans being generated by Spark. Since the specific solutions are not listed in the excerpt given, I cannot provide a direct answer from the document. However, typically, ensuring that table statistics are up-to-date and consistent between the Delta tables would be a logical step to mitigate such issues."}
{"hard_neg_docs": [{"page_content": "B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nB ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}, {"page_content": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nTitle: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution", "fmt_content": "Source: https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query\nContent:\nbased on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution", "metadata": {"source": "https://kb.databricks.com/delta/different-tables-with-same-data-generate-different-plans-when-used-in-same-query", "seq_num": 2203, "description": "", "text": "Title: Different tables with same data generate different plans when used in same query\n\nDescription: Ensure that tables with the same data generate the same physical plans with Spark SQL.\n\nAnswer: Problem Assume you have two Delta tables test_table_1 and test_table_2. Both tables have the same schema, same data volume, same partitions, and contain the same number of files. You are doing a join transformation with another Delta table, test_table_join, which has a million records. When you run the below join queries using test_table_1 and test_table_2, different physical plans are generated, even if both tables are identical. Query 1: %sql SELECT COUNT(A.*) FROM TEST_TABLE_1 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Query 2: %sql SELECT COUNT(A.*) FROM TEST_TABLE_2 A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID When different physical plans are generated for identical tables it can result in a delay as compared to other queries. Cause Apache Spark generates the physical plan for the transformation based on the table statistics. If best practices for Delta tables are not followed, table statistics could be different, even if the tables are otherwise identical. If the table statistics are different, Spark can generate a different plan than it might have done if both tables had the same statistics. Solution If you notice different physical plans are being generated for identical Delta tables there are two different solutions you can use to mitigate the issue. The solution you choose depends on the specific Databricks Runtime version you are running on your cluster. Databricks Runtime 7.3 LTS Use the Delta table path instead of the table name in the query. This directly reads the data from the path without checking the table statistics. %sql SELECT COUNT(A.*) FROM delta.`path` A INNER JOIN TEST_TABLE_JOIN B ON A.ID=B.ID Databricks Runtime 9.1 LTS and above Run the ANALYZE TABLE (AWS | Azure | GCP) and COMPUTE STATISTICS commands on the table. This calculates the table statistics and stores them in the metadata. %sql ANALYZE TABLE test_table_1 COMPUTE STATISTICS; After implementing the solution, re-run both queries and use a diff checker to compare the resulting physical plans for the two Delta tables. The plans are now identical.", "timestamp": 1660652931.338, "next": ""}}], "question": "Why might Spark generate different plans for identical Delta tables?", "gold_answer": "Spark might generate different plans for identical Delta tables if the table statistics are different. Even if the tables are otherwise identical, differences in table statistics can lead to Spark generating different physical plans than it might have done if both tables had the same statistics."}
{"hard_neg_docs": [{"page_content": "### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What Git automation can be set up to update Databricks Repos to the latest version on Azure?", "gold_answer": "On Azure, you can set up Git automation to call the Repos API endpoint after every successful merge of a PR into the main branch. This automation updates the appropriate repo in the Production folder to the latest version using the Repos API. For example, on GitHub, this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, you can refer to the [Repos API](/api-explorer/workspace/repos) documentation."}
{"hard_neg_docs": [{"page_content": "### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": ".. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::", "fmt_content": "Source: https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html\nContent:\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::", "metadata": {"source": "https://docs.databricks.com/repos/ci-cd-techniques-with-repos.html", "seq_num": 339, "description": "", "text": "---\ndescription: Learn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n---\n\n# CI/CD workflows with Git integration and Databricks Repos\n\nLearn techniques for using Databricks Repos in CI/CD workflows. Integrating Git repos with Databricks Repos provides source control for project files.\n\nThe following figure shows an overview of the techniques and workflow.\n\n![Overview of CI/CD techniques for Repos.](/_static/images/repos/repos-cicd-techniques.png)\n\n## Development flow\n\nDatabricks Repos have user-level folders and non-user top level folders. User-level folders are automatically created when users first clone a remote repository. You can think of Databricks Repos in user folders as \"local checkouts\" that are individual for each user and where users make changes to their code.\n\nIn your user folder in Databricks Repos, clone your remote repository. A best practice is to [create a new feature branch](git-operations-with-repos.md#create-a-new-branch) or select a previously created branch for your work, instead of directly committing and pushing changes to the main branch. You can make changes, commit, and push changes in that branch. When you are ready to merge your code, create a pull request and then follow the review and merge processes in your Git provider.\n\n### Requirements\n\nThis workflow requires that you have already [set up your Git integration](repos-setup.md).\n\n.. note:: Databricks recommends that each developer work on their own feature branch. Sharing feature branches among developers can cause merge conflicts, which must be resolved using your Git provider. For information about how to resolve merge conflicts, see [_](/repos/git-operations-with-repos.md#merge-conflicts).\n\n### Collaborate in Repos\n\n#. [Clone your existing Git repository to your Databricks workspace](git-operations-with-repos.md).\n#. Use the Repos UI to [create a feature branch](git-operations-with-repos.md#create-a-new-branch) from the main branch. This example uses a single feature branch _feature-b_ for simplicity. You can create and use multiple feature branches to do your work.\n#. Make your modifications to <Databricks> notebooks and other files in the Repo.\n#. [Commit and push your changes to your Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. Coworkers can now clone the Git repository into their own user folder.\n  a. Working on a new branch, a coworker makes changes to the notebooks and other files in the Repo.\n  #. The coworker [commits and pushes their changes to the Git provider](git-operations-with-repos.md#commit-and-push-changes-to-the-remote-git-repository).\n#. To merge changes from other branches or rebase the feature branch, you must use the Git command line or an IDE on your local system. Then, in the Repos UI, use the Git dialog to pull changes into the _feature-b_ branch in the Databricks Repo.\n#. When you are ready to merge your work to the main branch, use your Git provider to create a PR to merge the changes from _feature-b_.\n#. In the Repos UI, pull changes to the main branch.\n\n## Production job workflow\n\nDatabricks Repos provides two options for running your production jobs:\n- **Option 1**: Provide a remote Git ref in the job definition, for example, a specific notebook in main branch of a Github repository.\n- **Option 2**: Set up a production repo and use Repos APIs to update it programmatically. Then run jobs against this Databricks repo.\n\n## Option 1: Run jobs using notebooks in a remote repo\n\nSimplify the job definition process and keep a single source of truth by running <a Databricks> job using notebooks located in a remote Git repository. This Git reference can be a git commit, tag, or branch and is provided by you in the job definition.\n\nThis ensures that you can prevent unintentional changes to your production job, for example, when a user makes local edits in a production repo or switches branches. It also automates the CD step as you do not need to create a separate production repo in Databricks, manage permissions for it, and keep it updated.\n\nSee [_](/workflows/jobs/how-to/use-repos.md).\n\n<a id=\"automate-production\"></a>\n\n## Option 2: Set up a production repo and Git automation\n\nIn this option, you set up a production repo and Git automation to update Databricks Repos on merge.\n\n### Step 1: Set up top-level folders\n\nThe admin creates non-user top-level folders. The most common use case for these top-level folders is to create development, staging, and production folders that contain Databricks Repos for the appropriate versions or branches for development, staging, and production. For example, if your company uses the Main branch for production, the production folder would contain a Repo that is checked out to the Main branch.\n\nTypically permissions on these top-level folders are read-only for all non-admin users within the workspace. For such top-level folders we recommend you only provide service principal(s) with **Can Edit** and **Can Manage** permissions to avoid accidental edits to your production code by workspace users.\n\n![Top-level repo folders.](/_static/images/repos/top-level-repo-folders.png)\n\n### Step 2: Set up automated updates to Databricks Repos via the Repos API\n\n.. aws-gcp::\n\n  In this step, use the [Repos API](/api-explorer/workspace/repos) to set up automation to update Databricks Repos upon a merge event.\n\n.. azure::\n\n  In this step, use the [Repos API](/dev-tools/api/latest/repos.md) to set up automation to update Databricks Repos upon a merge event.\n\n\n.. aws::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to pull the changes and update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions).\n\n.. azure::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/dev-tools/api/latest/repos.md). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n.. gcp::\n\n  To ensure that Databricks Repos are always at the latest version, you can set up Git automation to call the [_](/api-explorer/workspace/repos). In your Git provider, set up automation that---after every successful merge of a PR into the main branch---calls the Repos API endpoint on the appropriate repo in the Production folder to update that repo to the latest version.\n\n  For example, on GitHub this can be achieved with [GitHub Actions](https://github.com/features/actions). For more information, see the [Repos API](/api-explorer/workspace/repos).\n\n## Run jobs using a notebook in a Databricks Repo\n\nYou can point a job directly to a notebook in a Databricks Repo. When a job kicks off a run, it uses the current version of the code in the repo.\n\nIf the automation is setup as described in [_](#automate-production), every successful merge calls the Repos API to update the repo. As a result, jobs that are configured to run code from a repo always use the latest version available when the job was run.\n\n.. aws::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Your Git provider PAT the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal> in your workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Create <a access-token-sp> for <a service-principal> with the [_](/dev-tools/api/latest/token-management.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these three APIs, you can use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n.. azure::\n\n  ## Use a service principal with Databricks Repos\n\n  To execute the above mentioned workflows with service principals:\n\n  1. Create a service principal with <Databricks>.\n  #. Add the git credentials: Use your Git provider PAT for the service principal.\n\n  To set up service principals and then add Git provider credentials:\n\n  1. Create <a service-principal>. See [Run jobs with service principals](https://learn.microsoft.com/azure/databricks/tutorials/run-jobs-with-service-principals).\n  #. Create <a access-token-sp> for <a service-principal>.\n  #. After you create <a service-principal>, you add it to your <Databricks> workspace with the [_](/dev-tools/api/latest/scim/scim-sp.md).\n  #. Add your Git provider credentials to your workspace with your <access-token-sp> and the [_](/dev-tools/api/latest/gitcredentials.md).\n\n  To call these two Databricks APIs, you can also use tools such as `curl`, Postman, or Terraform. You cannot use the <Databricks> user interface.\n\n  To learn more service principals on <Databricks>, see [_](/dev-tools/service-principals.md). For information about service principals and CI/CD, see [_](/dev-tools/ci-cd/ci-cd-sp.md).\n\n<a id=\"terraform-integration\"></a>\n\n## Terraform integration\n\nYou can also manage Databricks Repos in a fully automated setup using [_](/dev-tools/terraform/index.md) and [databricks_repo](https://registry.terraform.io/providers/databrickslabs/databricks/latest/docs/resources/repo):\n\n```text\nresource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n```\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I ensure that Databricks Repos are always at the latest version on AWS?", "gold_answer": "To ensure that Databricks Repos are always at the latest version on AWS, you can set up Git automation to call the Repos API endpoint. After every successful merge of a pull request (PR) into the main branch, the automation should call the Repos API endpoint on the appropriate repository in the Production folder to pull the changes and update that repository to the latest version. This can be achieved on GitHub by using GitHub Actions."}
{"hard_neg_docs": [{"page_content": "## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}, {"page_content": "## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "question": "What are some improvements made to Delta in the September 2020 release?", "gold_answer": "Some improvements made to Delta in the September 2020 Databricks release include:\n\n- Delta performance optimizations significantly reduce overhead\n- Clone metrics\n- Delta `MERGE INTO` improvements\n- Specify the initial position for Delta Structured Streaming"}
{"hard_neg_docs": [{"page_content": "## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}, {"page_content": "## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades", "fmt_content": "Source: https://docs.databricks.com/release-notes/product/2020/september.html\nContent:\n---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades", "metadata": {"source": "https://docs.databricks.com/release-notes/product/2020/september.html", "seq_num": 1928, "description": "", "text": "---\ndescription: September 2020 release notes for new <Databricks> features and improvements.\n---\n\n.. gcp::\n    ---\n    orphan: 1\n    ---\n\n\n# September 2020\n\nThese features and <Databricks> platform improvements were released in September 2020.\n\n.. note::\n    Releases are staged. Your <Databricks> account may not be updated until up to a week after the initial release date.\n\n## <DBR> 7.3, 7.3 ML, and 7.3 Genomics are now GA\n\n**September 24, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now generally available. They bring many features and improvements, including:\n\n.. aws::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Kinesis starting position for stream using `at_timestamp`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * GA of S3 storage connector updates\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\n.. azure::\n  * <Delta> performance optimizations significantly reduce overhead\n  * Clone metrics\n  * <Delta> `MERGE INTO` improvements\n  * Specify the initial position for <Delta> Structured Streaming\n  * Auto Loader improvements\n  * Adaptive query execution\n  * Azure Synapse Analytics connector column length control\n  * Improved behavior of `dbutils.credentials.showRoles`\n  * Simplified pandas to Spark DataFrame conversion\n  * New `maxResultSize` in `toPandas()` call\n  * Debuggability of pandas and PySpark UDFs\n  * (ML only) Conda activation on workers\n  * (Genomics only) Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n  * Library upgrades\n\nFor more information, see the [_](/release-notes/runtime/7.3.md), [_](/release-notes/runtime/7.3ml.md), and [_](/release-notes/runtime/7.3genomics.md) release notes.\n\n.. aws::\n  ## Debugging hints for SAML credential passthrough misconfigurations\n\n  **September 23-29, 2020: Version 3.29**\n\n  The response from a single-sign on request using SAML credential passthrough now includes an error hint to help debug misconfigurations. For details, see [_](/data-governance/credential-passthrough/iam-federation.md#troubleshooting).\n\n## Single Node clusters (Public Preview)\n\n**September 23-29, 2020: Version 3.29**\n\nA Single Node cluster is a cluster consisting of a Spark driver and no Spark workers. In contrast, Standard mode clusters require at least one Spark worker to run Spark jobs. Single Node mode clusters are helpful in the following situations:\n\n- Running single node machine learning workloads that need Spark to load and save data\n- Lightweight exploratory data analysis (EDA)\n\nFor details, see [_](/clusters/single-node.md).\n\n## DBFS REST API rate limiting\n\n**September 23-29, 2020: Version 3.29**\n\nTo ensure high quality of service under heavy load, <Databricks> is now enforcing API rate limits for [DBFS API](/dev-tools/api/latest/dbfs.md) calls. Limits are set per workspace to ensure fair usage and high availability. Automatic retries are available using Databricks CLI version 0.12.0 and above. We advise all customers to switch to the latest Databricks CLI version.\n\n## New sidebar icons\n\n**September 23-29, 2020**\n\nWe've updated the sidebar in the <Databricks> workspace UI. No big deal, but we think the new icons look pretty nice.\n\n![sidebar](/_static/images/release-notes/new-sidebar-icons.png)\n\n## Running jobs limit increase\n\n**September 23-29, 2020: Version 3.29**\n\nThe concurrent running job run limit has been increased from 150 to 1000 per workspace. No longer will runs over 150 be queued in the pending state. Instead of a queue for run requests above concurrent runs, a `429 Too Many Requests` response is returned when you request a run that cannot be started immediately. This limit increase was rolled out gradually and is now available on all workspaces in all regions.\n\n## Artifact access control lists (ACLs) in MLflow\n\n**September 23-29, 2020: Version 3.29**\n\nMLflow Experiment permissions are now enforced on artifacts in MLflow Tracking, enabling you to easily control access to your models, datasets, and other files. By default, when you create a new experiment, its run artifacts are now stored in an MLflow-managed location. The four MLflow Experiment permissions levels (**No Permissions**, **Can Read**, **Can Edit**, and **Can Manage**) automatically apply to run artifacts stored in MLflow-managed locations as follows:\n\n* **Can Edit** or **Can Manage** permissions are required to log run artifacts to an experiment.\n* **Can Read** permissions are required to list and download run artifacts from an experiment.\n\nFor more information, see [_](/security/auth-authz/access-control/workspace-acl.md#mlflow-artifact-permissions).\n\n## MLflow usability improvements\n\n**September 23-29, 2020: Version 3.29**\n\nThis release includes the following MLflow usability improvements:\n\n* The MLflow **Experiment** and **Registered Models** pages now have tips to help new users get started.\n* The model version table now shows the description text for a model version. A new column shows the first 32 characters or the first line (whichever is shorter) of the description.\n\n<a id=\"powerbi-connector\"></a>\n\n## New <Databricks> Power BI connector (Public Preview)\n\n**September 22, 2020**\n\nPower BI Desktop version 2.85.681.0 includes a new <Databricks> Power BI connector that makes the integration between <Databricks> and Power BI far more seamless and reliable. The new connector comes with the following improvements:\n\n.. azure::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Authentication based on Azure Active Directory credentials---no more need for administrators to configure PAT tokens.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md) and Azure storage account permissions associated with your Azure AD identity.\n\n.. aws::\n\n  - Simple connection configuration: the new Power BI <Databricks> connector is integrated into Power BI, and you configure it using a simple dialog with a couple of clicks.\n\n  - Faster imports and optimized metadata calls, thanks to the new <Databricks> ODBC driver, which comes with significant performance improvements.\n\n  - Access to <Databricks> data through Power BI respects <Databricks> [table access control](/data-governance/table-acls/index.md).\n\nFor more information, see [_](/partners/bi/power-bi.md).\n\n.. azure::\n\n  ## Use customer-managed keys for DBFS root (Public Preview)\n\n  **September 15, 2020**\n\n  You can now use your own encryption key in Azure Key Vault to encrypt the DBFS storage account. See [_](/security/keys/customer-managed-keys-dbfs/index.md).\n\n## New JDBC and ODBC drivers bring faster and lower latency BI\n\n**September 15, 2020**\n\nWe have released new versions of the Databricks JDBC and ODBC drivers [(download)](https://databricks.com/spark/odbc-driver-download) with the following improvements:\n\n* Performance: Reduced connection and short query latency, improved result transfer speed based on Apache Arrow serialization and improved metadata retrieval performance.\n* User experience: Authentication using Azure AD OAuth2 access tokens, improved error messages and auto-retry when connecting to a shutdown cluster, more robust handling of retries on intermittent network errors.\n* Support for connections using HTTP proxy.\n\nFor more information about connecting to BI tools using JDBC and ODBC, see [_](/integrations/jdbc-odbc-bi.md).\n\n## MLflow Model Serving (Public Preview)\n\n**September 9-15, 2020: Version 3.28**\n\nMLflow Model Serving is now available in Public Preview. MLflow Model Serving allows you to deploy a MLflow model registered in Model Registry as a REST API endpoint hosted and managed by <Databricks>. When you enable model serving for a registered model, <Databricks> creates a cluster and deploys all non-archived versions of that model.\n\nYou can query all model versions by REST API requests with standard <Databricks> authentication. Model access rights are inherited from the Model Registry --- anyone with read rights for a registered model can query any of the deployed model versions. While this service is in preview, we recommend its use for low throughput and non-critical applications.\n\nFor more information, see [_](/archive/legacy-model-serving/model-serving.md).\n\n## Clusters UI improvements\n\n**September 9-15, 2020: Version 3.28**\n\nThe [Clusters page](/clusters/clusters-manage.md#display-clusters) now has separate tabs for **<Interactive> Clusters** and **<Automated> Clusters**. The list on each tab is now paginated. In addition, we have fixed the delay that sometimes occurred between creating a cluster and being able to see it in the UI.\n\n## Visibility controls for jobs, clusters, notebooks, and other workspace objects\n\n**September 9-15, 2020: Version 3.28**\n\nBy default, any user can see all jobs, clusters, notebooks, and folders in their workspace displayed in the <Databricks> UI and can list them using the Databricks API, even when access control is enabled for those objects and a user has no permissions on those objects.\n\nNow any <Databricks> admin can enable visibility controls for notebooks and folders (workspace objects), clusters, and jobs to ensure that users can view only those objects that they have been given access to through workspace, cluster, or jobs access control.\n\nSee:\n- [_](/administration-guide/access-control/workspace-acl.md#workspace-object-visibility)\n- [_](/administration-guide/access-control/cluster-acl.md#cluster-visibility)\n- [_](/administration-guide/access-control/jobs-acl.md#jobs-visibility)\n\n## Ability to create tokens no longer permitted by default\n\n**September 9-15, 2020: Version 3.28**\n\nFor workspaces created after the release of <Databricks> platform version 3.28, users will no longer have the ability to generate personal access tokens by default. Admins must explicitly grant those permissions, whether to the entire `users` group or on a user-by-user or group-by-group basis. Workspaces created before 3.28 was released will maintain the permissions that were already in place.\n\nSee [_](/administration-guide/access-control/tokens.md).\n\n.. aws::\n\n  ## Support for c5.24xlarge instances\n\n  **September 9-15, 2020: Version 3.28**\n\n  Databricks now supports the c5.24xlarge EC2 instance type.\n\n## MLflow Model Registry supports sharing of models across workspaces\n\n**September 9, 2020**\n\n<Databricks> now supports access to the model registry from multiple workspaces. You can now register models, track model runs, and load models across workspaces. Multiple teams can now share access to models, and organizations can use multiple workspaces to handle the different stages of development. For details, see [_](/machine-learning/manage-model-lifecycle/multiple-workspaces.md).\n\nThis functionality requires MLflow Python client version 1.11.0 or above.\n\n## <DBR> 7.3 (Beta)\n\n**September 3, 2020**\n\n<DBR> 7.3, <DBR> 7.3 for Machine Learning, and <DBR> 7.3 for Genomics are now available as Beta releases.\n\nFor information, see", "timestamp": 0, "next": ""}}], "question": "What are some features and improvements included in DBR 7.3, DBR 7.3 for Machine Learning, and DBR 7.3 for Genomics?", "gold_answer": "Some features and improvements included in DBR 7.3, DBR 7.3 for Machine Learning, and DBR 7.3 for Genomics are:\n\n- Delta performance optimizations that significantly reduce overhead\n- Clone metrics\n- Delta `MERGE INTO` improvements\n- The ability to specify the initial position for Delta Structured Streaming\n- Auto Loader improvements\n- Adaptive query execution\n- Azure Synapse Analytics connector column length control\n- Improved behavior of `dbutils.credentials.showRoles`\n- Kinesis starting position for stream using `at_timestamp`\n- Simplified pandas to Spark DataFrame conversion\n- New `maxResultSize` in `toPandas()` call\n- Debuggability of pandas and PySpark UDFs\n- General availability of S3 storage connector updates\n- For Machine Learning (ML) only: Conda activation on workers\n- For Genomics only: Support for reading BGEN files with uncompressed or zstd-compressed genotypes\n- Library upgrades"}
{"hard_neg_docs": [{"page_content": "![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Thread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I view the thread dump of the driver in the Spark UI?", "gold_answer": "To view the thread dump of the driver in the Spark UI, follow these steps:\n\n1. Click the **Executors** tab.\n2. In the **Executors** table, find the row for the **driver**.\n3. Click the link in the **Thread Dump** column that corresponds to the driver. The driver's thread dump will be displayed for you to review."}
{"hard_neg_docs": [{"page_content": "![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Thread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:", "fmt_content": "Source: https://docs.databricks.com/clusters/debugging-spark-ui.html\nContent:\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:", "metadata": {"source": "https://docs.databricks.com/clusters/debugging-spark-ui.html", "seq_num": 1670, "description": "", "text": "---\ndescription: Learn how to troubleshoot and debug <AS> applications using the UI and cluster logs in <Databricks>.\n---\n\n# Debugging with the <AS> UI\n\n.. gcp::\n\n  .. note:: The Spark UI feature is unavailable on <Databricks> on <GCP> as of this release.\n\n  <!--  @GCP-spark-ui        -->\n\nThis guide walks you through the different debugging options available to peek at the internals of your <AS> application. The three important places to look are:\n\n* Spark UI\n* Driver logs\n* Executor logs\n\n## Spark UI\n\nOnce you start the job, the Spark UI shows information about what's happening in your application. To get to the Spark UI, click the attached cluster:\n\n![Select Spark UI](/_static/images/spark/legacy-streaming/getting2-spark-ui.png)\n\n### Streaming tab\n\nOnce you get to the Spark UI, you will see a Streaming tab if a streaming job is running in this cluster. If there is no streaming job running in this cluster, this tab will not be visible. You can skip to [_](#driver-logs) to learn how to check for exceptions that might have happened while starting the streaming job.\n\nThe first thing to look for in this page is to check if your streaming application is receiving any input events from your source. In this case, you can see the job receives 1000 events/second.\n\n.. note: For `TextFileStream`, since files are input, the # of input events is always 0. In such cases, you can look at the **Completed Batches** section in the notebook to figure out how to find more information.\n\nIf you have an application that receives multiple input streams, you can click the **Input Rate** link which will show the # of events received for each receiver.\n\n![Input rate](/_static/images/spark/legacy-streaming/streaming-tab.png)\n\n### Processing time\n\nAs you scroll down, find the graph for **Processing Time**. This is one of the key graphs to understand the performance of your streaming job. As a general rule of thumb, it is good if you can process each batch within 80% of your batch processing time.\n\nFor this application, the batch interval was 2 seconds. The average processing time is 450ms which is well under the batch interval. If the average processing time is closer or greater than your batch interval, then you will have a streaming application that will start queuing up resulting in backlog soon which can bring down your streaming job eventually.\n\n![Processing time](/_static/images/spark/legacy-streaming/processing-time.png)\n\n### Completed batches\n\nTowards the end of the page, you will see a list of all the completed batches. The page displays details about the last 1000 batches that completed. From the table, you can get the # of events processed for each batch and their processing time. If you want to know more about what happened on one of the batches, you can click the batch link to get to the Batch Details Page.\n\n![Completed batches](/_static/images/spark/legacy-streaming/completed-batches.png)\n\n### Batch details page\n\nThis page has all the details you want to know about a batch. Two key things are:\n\n* Input: Has details about the input to the batch. In this case, it has details about the Apache Kafka topic, partition and offsets read by Spark Structured Streaming for this batch. In case of TextFileStream, you see a list of file names that was read for this batch. This is the best way to start debugging a Streaming application reading from text files.\n* Processing: You can click the link to the Job ID which has all the details about the processing done during this batch.\n\n![Batch details](/_static/images/spark/legacy-streaming/batch-details-page.png)\n\n### Job details page\n\nThe job details page shows a DAG visualization. This is a very useful to understand the order of operations and dependencies for every batch. In this case, you can see that the batch read input from Kafka direct stream followed by a flat map operation and then a map operation. The resulting stream was then used to update a global state using updateStateByKey. (The grayed boxes represents skipped stages. Spark is smart enough to skip some stages if they don't need to be recomputed. If the data is checkpointed or cached, then Spark would skip recomputing those stages. In this case, those stages correspond to the dependency on previous batches because of `updateStateBykey`. Since Spark Structured Streaming internally checkpoints the stream and it reads from the checkpoint instead of depending on the previous batches, they are shown as grayed stages.)\n\nAt the bottom of the page, you will also find the list of jobs that were executed for this batch. You can click the links in the description to drill further into the task level execution.\n\n![Job details](/_static/images/spark/legacy-streaming/job-details-page1.png)\n\n![Completed stages](/_static/images/spark/legacy-streaming/job-details-page2.png)\n\n### Task details page\n\nThis is the most granular level of debugging you can get into from the Spark UI for a Spark application. This page has all the tasks that were executed for this batch. If you are investigating performance issues of your streaming application, then this page would provide information such as the number of tasks that were executed and where they were executed (on which executors) and shuffle information\n\n.. tip:: Ensure that the tasks are executed on multiple executors (nodes) in your cluster to have enough parallelism while processing. If you have a single receiver, sometimes only one executor might be doing all the work though you have more than one executor in your cluster.\n\n![Task details](/_static/images/spark/legacy-streaming/task-details-page.png)\n\n### Thread dump\n\nA thread dump shows a snapshot of a JVM's thread states.\n\nThread dumps are useful in debugging a specific hanging or slow-running task. To view a specific task's thread dump in the Spark UI:\n\n#. Click the **Jobs** tab.\n#. In the **Jobs** table, find the target job that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the job's **Stages** table, find the target stage that corresponds to the thread dump you want to see, and click the link in the **Description** column.\n#. In the stage's **Tasks** list, find the target task that corresponds to the thread dump you want to see, and note its **Task ID** and **Executor ID** values.\n#. Click the **Executors** tab.\n#. In the **Executors** table, find the row that contains the **Executor ID** value that corresponds to the **Executor ID** value that you noted earlier. In that row, click the link in the **Thread Dump** column.\n#. In the **Thread dump for executor** table, click the row where the **Thread Name** column contains **(TID** followed by the **Task ID** value that you noted earlier. (If the task has finished running, you will not find a matching thread). The task's thread dump is shown.\n\nThread dumps are also useful for debugging issues where the driver appears to be hanging (for example, no Spark progress bars are showing) or making no progress on queries (for example, Spark progress bars are stuck at 100%). To view the driver's thread dump in the Spark UI:\n\n#. Click the **Executors** tab.\n#. In the **Executors** table, in the **driver** row, click the link in the **Thread Dump** column. The driver's thread dump is shown.\n\n## Driver logs\n\nDriver logs are helpful for 2 purposes:\n\n* Exceptions: Sometimes, you may not see the Streaming tab in the Spark UI. This is because the Streaming job was not started because of some exception. You can drill into the Driver logs to look at the stack trace of the exception. In some cases, the streaming job may have started properly. But you will see all the batches never going to the Completed batches section. They might all be in processing or failed state. In such cases too, driver logs could be handy to understand on the nature of the underlying issues.\n* Prints: Any print statements as part of the DAG shows up in the logs too.\n\n<!-- Not clear what is still relevant here.\n\nThe following table shows the DStream transformations and where the corresponding log location would be if the transformation had a print statement:\n\n+-------------------------------+----------------------------+\n| Description                   | Location                   |\n+===============================+============================+\n| foreachRDD(), transform()     | Driver Stdout Logs         |\n+-------------------------------+----------------------------+\n| foreachPartition()            | Executor's Stdout Logs     |\n+-------------------------------+----------------------------+\n\nTo get to the driver logs, you can click the attached cluster.\n\n![Select driver logs](/_static/images/spark/legacy-streaming/get2-driver-logs.png)\n\n![Driver log](/_static/images/spark/legacy-streaming/driver-logs.png)\n\n.. note:: For PySpark streaming, all the prints and exceptions does not automatically show up in the logs. The current limitation is that a notebook cell needs to be active for the logs to show up. Since the streaming job runs in the background thread, the logs are lost. If you want to see the logs while running a pyspark streaming application, you can provide `ssc.awaitTerminationOrTimeout(x)` in one of the cells in the notebook. This will put the cell on hold for 'x' seconds. After the 'x' seconds, all the prints and exceptions during that time will be present in the logs. -->\n\n## Executor logs\n\nExecutor logs are sometimes helpful if you see certain tasks are misbehaving and would like to see the logs for specific tasks. From the task details page shown above, you can get the executor where the task was run. Once you have that, you can go to the clusters UI page, click the # nodes, and then the master. The master page lists all the workers. You can choose the worker where the suspicious task was run and then get to the log4j output.\n\n![Select master](/_static/images/spark/legacy-streaming/clusters.png)\n\n![Spark master](/_static/images/spark/legacy-streaming/spark-master.png)\n\n![Spark worker](/_static/images/spark/legacy-streaming/executor-page.png)\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are the purposes of driver logs in debugging Spark issues?", "gold_answer": "The purposes of driver logs in debugging Spark issues are:\n\n1. Exceptions: To investigate when the Streaming tab is not visible in the Spark UI, possibly due to a streaming job not starting because of an exception. The driver logs can provide the stack trace of the exception to understand the issue.\n\n2. Prints: To view any print statements that are part of the Directed Acyclic Graph (DAG) execution, as these will show up in the driver logs."}
{"hard_neg_docs": [{"page_content": "<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I create a Single Node cluster in Databricks on GCP?", "gold_answer": "You cannot create Single Node clusters using cluster policies in Databricks on Google Cloud Platform (GCP) because cluster policies are unavailable on Databricks on GCP. The documentation provided specifically notes that cluster policies are not an option for Single Node clusters on the GCP platform. If you are using another cloud provider that supports Databricks, such as AWS or Azure, you would follow the steps outlined in the document to create a pool and a cluster policy, then grant the policy to the relevant team members. However, for GCP, an alternative approach not described in the document would need to be used."}
{"hard_neg_docs": [{"page_content": "<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.", "fmt_content": "Source: https://docs.databricks.com/clusters/single-node.html\nContent:\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.", "metadata": {"source": "https://docs.databricks.com/clusters/single-node.html", "seq_num": 1680, "description": "", "text": "---\ndescription: Learn about <Databricks> Single Node clusters, when to use them, how to create them, and limitations.\nkeywords: single-node\n---\n\n# Single Node clusters\n\n.. include:: /shared/clusters-preview-ui-short.md\n\nA Single Node cluster is a cluster consisting of an <AS> driver and no Spark workers. A Single Node cluster supports Spark jobs and all Spark data sources, including [Delta Lake](/delta/index.md). A Standard cluster requires a minimum of one Spark worker to run Spark jobs.\n\nSingle Node clusters are helpful for:\n\n- Single-node machine learning workloads that use Spark to load and save data\n- Lightweight exploratory data analysis\n\n## Create a Single Node cluster\n\nTo create a Single Node cluster, select the **Single Node** button when you [configure a cluster](/clusters/configure.md#access-mode).\n\n## Single Node cluster properties\n\nA Single Node cluster has the following properties:\n\n- Runs Spark locally.\n- The driver acts as both master and worker, with no worker nodes.\n- Spawns one executor thread per logical core in the cluster, minus 1 core for the driver.\n- All `stderr`, `stdout`, and `log4j` log output is saved in the driver log.\n- A Single Node cluster can't be converted to a Multi Node cluster.\n\n<a id=\"single-node-limitation\"></a>\n\n## Limitations\n\n- Large-scale data processing will exhaust the resources on a Single Node cluster. For these workloads, Databricks recommends using a Multi Node cluster.\n- Single Node clusters are not designed to be shared. To avoid resource conflicts, Databricks recommends using a Multi Node cluster when the cluster must be shared.\n- A Multi Node cluster can't be scaled to 0 workers. Use a Single Node cluster instead.\n- Single Node clusters are not compatible with process isolation.\n- GPU scheduling is not enabled on Single Node clusters.\n- On Single Node clusters, Spark cannot read Parquet files with a UDT column. The following error message results:\n\n  ```console\n  The Spark driver has stopped unexpectedly and is restarting. Your notebook will be automatically reattached.\n  ```\n\n  To work around this problem, disable the native Parquet reader:\n\n  ```python\n  spark.conf.set(\"spark.databricks.io.parquet.nativeReader.enabled\", False)\n  ```\n\n<a id=\"single-node-rest-api\"></a>\n\n## REST API\n\nYou can use the Clusters API to [create a Single Node cluster](/dev-tools/api/latest/clusters.md#clusterclusterservicecreatecluster).\n\n<a id=\"single-node-policy\"></a>\n\n## Single Node cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\n[Cluster policies](/administration-guide/clusters/policies.md) simplify cluster configuration for Single Node clusters.\n\nConsider the example of a data science team whose members do not have permission to create clusters. An admin can create a cluster policy that authorizes team members to  create a maximum number of Single Node clusters, using [pools](/clusters/pools.md) and [cluster policies](/administration-guide/clusters/policies.md):\n\n#. Create [a pool](/clusters/pools.md):\n  #. Set **Max capacity** to `10`.\n  #. In **Autopilot options**, enable **autoscaling enabled for local storage**.\n  #. Set **Instance type** to **Single Node cluster**.\n  #. Select <a Databricks> version. Databricks recommends using the latest version if possible.\n  #. Click **Create**.\n\n  The pool's properties page appears. Make a note of the pool ID and instance type ID page for the newly-created pool.\n\n#. Create a [cluster policy](/administration-guide/clusters/policies.md):\n\n  - Set the <instance pool> ID and instance type ID from the pool properties from the pool.\n  - Specify [constraints](/administration-guide/clusters/policies.md) as needed.\n\n#. Grant the cluster policy to the team members. You can use [_](/administration-guide/users-groups/index.md) to simplify user management.\n\n   .. azure-aws::\n\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"11.3.x-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        },\n        \"docker_image.url\": {\n          \"type\": \"forbidden\",\n          \"hidden\": true\n        }\n      }\n      ```\n\n<!-- @GCP-single-node-cluster not supported yet\n    ..gcp::\n      ```json\n      {\n        \"spark_conf.spark.databricks.cluster.profile\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNode\",\n          \"hidden\": true\n        },\n        \"instance_pool_id\": {\n          \"type\": \"fixed\",\n          \"value\": \"singleNodePoolId1\",\n          \"hidden\": true\n        },\n        \"spark_version\": {\n          \"type\": \"fixed\",\n          \"value\": \"7.3.x-cpu-ml-scala2.12\",\n          \"hidden\": true\n        },\n        \"autotermination_minutes\": {\n          \"type\": \"fixed\",\n          \"value\": 120,\n          \"hidden\": true\n        },\n        \"num_workers\": {\n          \"type\": \"fixed\",\n          \"value\": 0,\n          \"hidden\": true\n        }\n      }\n      ```\n  @gcp-container-service add back docker_image.url\n-->\n\n<a id=\"single-node-policy-job\"></a>\n\n## Single Node job cluster policy\n\n.. gcp::\n  .. note:: Cluster policies are unavailable on <Databricks> on <GCP>.\n\nTo set up a cluster policy for jobs, you can define a similar cluster policy.\nSet the `cluster_type.type` to `fixed` and `cluster_type.value` to `job`.\nRemove all references to `auto_termination_minutes`.\n\n.. azure-aws::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"11.3.x-scala2.12\",\n      \"hidden\": true\n    },\n    \"docker_image.url\": {\n      \"type\": \"forbidden\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n<!-- @GCP-single-node-cluster not supported yet\n\n.. gcp::\n  ```json\n  {\n    \"cluster_type\": {\n      \"type\": \"fixed\",\n      \"value\": \"job\"\n    },\n    \"spark_conf.spark.databricks.cluster.profile\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNode\",\n      \"hidden\": true\n    },\n    \"instance_pool_id\": {\n      \"type\": \"fixed\",\n      \"value\": \"singleNodePoolId1\",\n      \"hidden\": true\n    },\n    \"num_workers\": {\n      \"type\": \"fixed\",\n      \"value\": 0,\n      \"hidden\": true\n    },\n    \"spark_version\": {\n      \"type\": \"fixed\",\n      \"value\": \"7.3.x-cpu-ml-scala2.12\",\n      \"hidden\": true\n    }\n  }\n  ```\n\n  @gcp-container-service add back docker_image.url\n  -->\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What are cluster policies and how can they simplify cluster configuration for Single Node clusters?", "gold_answer": "Cluster policies are a set of rules that administrators can use to control and simplify the configuration of Single Node clusters in Databricks. They help by providing standardized, pre-defined settings that enforce best practices, compliance, and governance. Cluster policies can be used to limit the ability of users to modify certain cluster attributes or to provide templates for cluster creation that ensure consistency and adherence to organizational standards.\n\nIn the context of Single Node clusters, cluster policies can simplify configuration by allowing an admin to:\n\n- Set a maximum number of Single Node clusters that team members are authorized to create.\n- Use pools to manage resources efficiently, including setting a maximum capacity, enabling autoscaling for local storage, and specifying the instance type for the Single Node cluster.\n- Set constraints on various cluster attributes to ensure that clusters are created within certain parameters.\n\nFor example, an admin can create a cluster policy that specifies the use of a particular instance pool with a pre-defined maximum capacity and instance type for Single Node clusters. This policy can then be granted to team members who do not have permission to create clusters on their own, allowing them to create clusters within the limits set by the policy. This streamlines the cluster creation process and ensures that clusters are configured correctly and comply with organizational policies."}
{"hard_neg_docs": [{"page_content": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\nTitle: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}, {"page_content": "pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\npipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Answer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}], "question": "What kind of models can the feature importance levels be extracted from?", "gold_answer": "The feature importance levels can be extracted from tree-based models, such as a decision tree, random forest, or gradient boosted tree."}
{"hard_neg_docs": [{"page_content": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\nTitle: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}, {"page_content": "pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\npipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "Answer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the", "fmt_content": "Source: https://kb.databricks.com/machine-learning/extract-feature-info\nContent:\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the", "metadata": {"source": "https://kb.databricks.com/machine-learning/extract-feature-info", "seq_num": 2408, "description": "", "text": "Title: How to extract feature information for tree-based Apache SparkML pipeline models\n\nDescription: Learn how to extract feature information for tree-based <AS>ML pipeline models in Databricks.\n\nAnswer: When you are fitting a tree-based model, such as a decision tree, random forest, or gradient boosted tree, it is helpful to be able to review the feature importance levels along with the feature names. Typically models in SparkML are fit as the last stage of the pipeline. To extract the relevant feature information from the pipeline with the tree model, you must extract the correct pipeline stage. You can extract the feature names from the VectorAssembler object: %python from pyspark.ml.feature import StringIndexer, VectorAssembler from pyspark.ml.classification import DecisionTreeClassifier from pyspark.ml import Pipeline pipeline = Pipeline(stages=[indexer, assembler, decision_tree) DTmodel = pipeline.fit(train) va = dtModel.stages[-2] tree = DTmodel.stages[-1] display(tree) #visualize the decision tree model print(tree.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), tree.featureImportances)) You can also tune a tree-based model using a cross validator in the last stage of the pipeline. To visualize the decision tree and print the feature importance levels, you extract the bestModel from the CrossValidator object: %python from pyspark.ml.tuning import ParamGridBuilder, CrossValidator cv = CrossValidator(estimator=decision_tree, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3) pipelineCV = Pipeline(stages=[indexer, assembler, cv) DTmodelCV = pipelineCV.fit(train) va = DTmodelCV.stages[-2] treeCV = DTmodelCV.stages[-1].bestModel display(treeCV) #visualize the best decision tree model print(treeCV.toDebugString) #print the nodes of the decision tree model list(zip(va.getInputCols(), treeCV.featureImportances)) The display function visualizes decision tree models only. See Machine learning visualizations (AWS | Azure | GCP).", "timestamp": 1652685945.878, "next": ""}}], "question": "How can the feature names be extracted from a pipeline with a tree model in SparkML?", "gold_answer": "To extract the feature names from a pipeline with a tree model in SparkML, you must extract the correct pipeline stage that contains the tree-based model, such as a decision tree, random forest, or gradient boosted tree. The document does not provide the complete procedure, but typically, this involves accessing the model within the pipeline and then retrieving the feature names from the model's metadata. The specific steps for extracting feature names are not detailed in the provided content excerpt."}
{"hard_neg_docs": [{"page_content": "// Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n// Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}, {"page_content": "```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}], "question": "Can I use the Azure Synapse connector with all Synapse components?", "gold_answer": "No, the Azure Synapse connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components."}
{"hard_neg_docs": [{"page_content": "// Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n// Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}, {"page_content": "```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.", "fmt_content": "Source: https://docs.databricks.com/external-data/synapse-analytics.html\nContent:\n---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.", "metadata": {"source": "https://docs.databricks.com/external-data/synapse-analytics.html", "seq_num": 165, "description": "", "text": "---\ndescription: Learn how to read and write data to Azure Synapse Analytics using <Databricks>.\n---\n\n<a id=\"synapse-analytics\"></a>\n\n# Query data in Azure Synapse Analytics\n\nYou can access Azure Synapse from <Databricks> using the Azure Synapse connector, which uses the `COPY` statement in Azure Synapse to transfer large volumes of data efficiently between <a Databricks> cluster and an Azure Synapse instance using an <ADLS2> storage account for temporary staging.\n\n[Azure Synapse Analytics](https://azure.microsoft.com/services/synapse-analytics/) is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data.\n\n.. important:: This connector is for use with Synapse Dedicated Pool instances only, and is not compatible with other Synapse components.\n\n.. note:: `COPY` is available only on <ADLS2> instances. If you're looking for details on working with Polybase, see [_](/archive/azure/synapse-polybase.md).\n\n## Example syntax for Synapse\n\nYou can query Synapse in Scala, Python, SQL, and R. The following code examples use storage account keys and forward the storage credentials from <Databricks> to Synapse.\n\n.. note:: Use the connection string provided by Azure portal, which enables Secure Sockets Layer (SSL) encryption for all data sent between the Spark driver and the Azure Synapse instance through the JDBC connection. To verify that the SSL encryption is enabled, you can search for `encrypt=true` in the connection string.\n\n.. code-language-tabs::\n\n  ```scala\n\n  // Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  // Get some data from an Azure Synapse table.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .load()\n\n  // Load data from an Azure Synapse query.\n  val df: DataFrame = spark.read\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\")\n    .load()\n\n  // Apply some transformations to the data, then use the\n  // Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write\n    .format(\"com.databricks.spark.sqldw\")\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\")\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\")\n    .option(\"dbTable\", \"<your-table-name>\")\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n    .save()\n  ```\n\n  ```python\n\n  # Set up the storage account access key in the notebook session conf.\n  spark.conf.set(\n    \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\",\n    \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .load()\n\n  # Load data from an Azure Synapse query.\n  df = spark.read \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"query\", \"select x, count(*) as cnt from table group by x\") \\\n    .load()\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  df.write \\\n    .format(\"com.databricks.spark.sqldw\") \\\n    .option(\"url\", \"jdbc:sqlserver://<the-rest-of-the-connection-string>\") \\\n    .option(\"forwardSparkAzureStorageCredentials\", \"true\") \\\n    .option(\"dbTable\", \"<your-table-name>\") \\\n    .option(\"tempDir\", \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\") \\\n    .save()\n  ```\n\n  ```sql\n\n  -- Set up the storage account access key in the notebook session conf.\n  SET fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net=<your-storage-account-access-key>;\n\n  -- Read data using SQL.\n  CREATE TABLE example_table_in_spark_read\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbtable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  );\n\n  -- Write data using SQL.\n  -- Create a new table, throwing an error if a table with the same name already exists:\n\n  CREATE TABLE example_table_in_spark_write\n  USING com.databricks.spark.sqldw\n  OPTIONS (\n    url 'jdbc:sqlserver://<the-rest-of-the-connection-string>',\n    forwardSparkAzureStorageCredentials 'true',\n    dbTable '<your-table-name>',\n    tempDir 'abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>'\n  )\n  AS SELECT * FROM table_to_save_in_spark;\n  ```\n\n  ```r\n  # Load SparkR\n  library(SparkR)\n\n  # Set up the storage account access key in the notebook session conf.\n  conf <- sparkR.callJMethod(sparkR.session(), \"conf\")\n  sparkR.callJMethod(conf, \"set\", \"fs.azure.account.key.<your-storage-account-name>.dfs.core.windows.net\", \"<your-storage-account-access-key>\")\n\n  # Get some data from an Azure Synapse table.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     dbTable = \"<your-table-name>\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Load data from an Azure Synapse query.\n  df <- read.df(\n     source = \"com.databricks.spark.sqldw\",\n     url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n     forward_spark_azure_storage_credentials = \"true\",\n     query = \"select x, count(*) as cnt from table group by x\",\n     tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n\n  # Apply some transformations to the data, then use the\n  # Data Source API to write the data back to another table in Azure Synapse.\n\n  write.df(\n    df,\n    source = \"com.databricks.spark.sqldw\",\n    url = \"jdbc:sqlserver://<the-rest-of-the-connection-string>\",\n    forward_spark_azure_storage_credentials = \"true\",\n    dbTable = \"<your-table-name>\",\n    tempDir = \"abfss://<your-container-name>@<your-storage-account-name>.dfs.core.windows.net/<your-directory-name>\")\n  ```\n\n<a id=\"authentication\"></a>\n\n## How does authentication between <Databricks> and Synapse work?\n\nThe Azure Synapse connector uses three types of network connections:\n\n- Spark driver to Azure Synapse\n- Spark cluster to Azure storage account\n- Azure Synapse to Azure storage account\n\n## Configuring access to Azure storage\n\nBoth <Databricks> and Synapse need privileged access to an Azure storage account to be used for temporary data storage.\n\nAzure Synapse does not support using SAS for storage account access. You can configure access for both services by doing one of the following:\n\n- Use the account key and secret for the storage account and set `forwardSparkAzureStorageCredentials` to `true`. See [_](/storage/azure-storage.md#auth-with-access-key).\n- Use <ADLS2> with [OAuth 2.0 authentication](#service-principal).\n- Configure your Azure Synapse instance to have a Managed Service Identity.\n\n## Required Azure Synapse permissions\n\nBecause it uses `COPY` in the background, the Azure Synapse connector requires the JDBC connection user to have permission to run the following commands in the connected Azure Synapse instance:\n\n* [COPY INTO](https://learn.microsoft.com/sql/t-sql/statements/copy-into-transact-sql)\n\nIf the destination table does not exist in Azure Synapse, permission to run the following command is required in addition to the command above:\n\n* [CREATE TABLE](https://learn.microsoft.com/sql/t-sql/statements/create-table-azure-sql-data-warehouse)\n\nThe following table summarizes the permissions required for writes with `", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the Azure Synapse connector in Databricks?", "gold_answer": "The purpose of the Azure Synapse connector in Databricks is to transfer large volumes of data efficiently between a Databricks cluster and an Azure Synapse instance using an Azure Data Lake Storage Gen2 (ADLS2) storage account for temporary staging. This connector enables users to access Azure Synapse Analytics, which is a cloud-based enterprise data warehouse, and perform complex queries rapidly across petabytes of data by leveraging massively parallel processing (MPP). The connector is specifically for use with Synapse Dedicated Pool instances and facilitates the use of the `COPY` statement in Azure Synapse for data transfer."}
{"hard_neg_docs": [{"page_content": "## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I enable a user to access Databricks SQL?", "gold_answer": "To enable a user to access Databricks SQL, follow these steps:\n\n1. After inviting the user to the workspace and they have received a confirmation email with a temporary password, make sure the user is added to the workspace.\n2. If necessary, allow the user to access Databricks SQL by selecting the **Databricks SQL access** checkbox. All users in new workspaces are entitled to Databricks SQL access by default.\n3. Click **Confirm** to finalize the changes.\n\nNote: If your workspace was enabled for Databricks SQL (DBSQL) during the Public Preview—before the week beginning September 7, 2021—users will retain the entitlement assigned before that date, unless you change it. If a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator grants it to them."}
{"hard_neg_docs": [{"page_content": "## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "fmt_content": "Source: https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html\nContent:\n![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::", "metadata": {"source": "https://docs.databricks.com/sql/user/sql-editor/admin-set-up-user-to-query-table.html", "seq_num": 574, "description": "", "text": "---\ndescription: Learn how to add users, configure tables, and explore data.\n---\n\n# Set up a user to query a table\n\nThis article shows how an administrator can add a user, create a table containing 10 million people records from a [Databricks dataset](/dbfs/databricks-datasets.md), and allow a user to access the table. Next, it shows how the user can run a query and view the query runtime details.\n\n.. important::\n  [Data access control](/security/auth-authz/access-control/data-acl.md) is _always enabled_ in <DBSQL>, which means that users must always be granted access to data.\n\n## Requirements\n\n.. azure-aws::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n.. gcp::\n\n  - You must be <a Databricks> [workspace admin](/administration-guide/users-groups/users.md).\n\n  .. warning:: SQL warehouses won’t start if you do not provision the required amount of CPU and storage resources. See [Compute Engine API](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). If needed, you can increase the resource quotas to support your use of SQL warehouses. See [Review and increase quotas](/administration-guide/account-settings-gcp/quotas.md#compute-engine-api). For information about workspace cost, see [cost per workspace](/administration-guide/workspace/create-workspace.md).\n\n<a id=\"sidebar\"></a>\n\n## Use the sidebar\n\n.. include:: /shared/sidebar-behavior.md\n\n<a id=\"add-a-user\"></a>\n\n## Step 1: Add a user\n\n.. aws::\n\n  #. Use the [sidebar persona switcher](#sidebar) to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-aws.png)\n\n  #. Click **Send invite**.\n\n     Databricks sends a confirmation email with a temporary password. If the user does not receive the confirmation email within 5 minutes, ask the user to check their spam folder.\n\n     The user is added to the workspace.\n\n  #. If necessary, allow the the user to access <DBSQL> by selecting the **Databricks SQL access** checkbox. All users in new workspaces have this entitlement by default.\n\n     .. note:: If your workspace was enabled for <DBSQL> during the Public Preview---that is, before the week beginning September 7, 2021---users retain the entitlement assigned before that date, unless you change it. In other words, if a user did not have the **Databricks SQL access** entitlement during the Public Preview, they will not have it now unless an administrator gives it to them.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n\n  #. Click **Confirm**.\n\n.. azure::\n\n  #. Use the sidebar persona switcher to select **Data Science & Engineering**.\n  #. Go to the [admin console](/administration-guide/workspace/index.md#admin-console).\n  #. On the **Users** tab, click **Add User**.\n  #. Enter the user email ID. You can add any user who belongs to the Azure Active Directory tenant of your Azure Databricks workspace. The examples in this article assume the user is `user@example.com`.\n\n     ![Add user](/_static/images/admin-settings/users-email-azure.png)\n\n  #. Click **OK**.\n\n     The user is added to the workspace.\n\n  #. Click the **Databricks SQL access** checkbox.\n\n     ![Added user](/_static/images/admin-settings/add-user.png)\n  #. Click **Confirm**.\n\n<a id=\"create-a-table\"></a>\n\n## Step 2: Create a table of 10 million people\n\n#. Use the sidebar persona switcher to select **SQL**.\n\n   Your landing page looks like this:\n\n   ![Admin landing page](/_static/images/sql/select-starter-warehouse.png)\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Paste the following in the SQL editor:\n\n   ```sql\n   CREATE TABLE default.people10m OPTIONS (PATH 'dbfs:/databricks-datasets/learning-spark-v2/people/people-10m.delta')\n   ```\n\n   This statement creates a Delta table using <Delta> files stored in Databricks datasets.\n\n#. Press **Ctrl/Cmd + Enter** or click the **Execute** button. The query will return `No data was returned.`\n\n#. To refresh the schema, click the <Refresh Schema> button at the bottom of the schema browser.\n\n   .. <Refresh Schema> ![Refresh Schema](/_static/images/sql/refresh-schema.png)\n\n#. Type `peo` in the text box to the right of the schema. The schema browser displays the new table.\n\n   ![Schema browser](/_static/images/sql/qs-table.png)\n\n<a id=\"configure-access\"></a>\n\n## Step 3: Configure access to the `default.people10m` table\n\nEnable the user you created in [Step 1](#add-a-user) to access the `default.people10m` table you created in [Step 3](#create-a-table).\n\nYou can configure access using [Data Explorer](#data-explorer) or [SQL editor](#sql-editor).\n\n### Data explorer\n\n#. Click the <Data Icon> **Data** in the sidebar.\n#. In the drop-down list at the top right, select a SQL warehouse, such as **Starter Warehouse**. The `default` schema is selected. The schema comment and owner display.\n#. Type `peo` in the text box following the `default` schema. Data Explorer displays the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![Default db grant](/_static/images/sql/defaultdb-grant.png)\n#. Enter `user@example.com`.\n#. Select the **USAGE** checkbox.\n#. Click **OK**.\n#. Click the `people10m` table.\n#. Click the **Permissions** tab.\n#. Click the **Grant** button.\n\n   ![People table grant](/_static/images/sql/peopletable-grant.png)\n#. Enter `user@example.com`.\n#. Click the **SELECT** and **READ_METADATA** checkboxes.\n#. Click **OK**.\n\n### SQL editor\n\n#. Click <New Icon> **New** in the sidebar and select **Query**.\n#. In the drop-down box at the left, select a SQL warehouse, such as **Starter Warehouse**.\n#. Enter the following queries one by one:\n\n   ```sql\n   GRANT USAGE ON SCHEMA default TO `user@example.com`;\n\n   GRANT SELECT, READ_METADATA ON TABLE default.people10m TO `user@example.com`;\n\n   SHOW GRANTS `user@example.com` ON TABLE default.people10m;\n   ```\n\n   After each query, press **Ctrl/Cmd + Enter** or click the **Execute** button. After the last query, it should display:\n\n   ```\n   +------------------+---------------+------------+-----------------------+\n   | Principal        | ActionType    | ObjectType | ObjectKey             |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | READ_METADATA | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | SELECT        | TABLE      | `default`.`people10m` |\n   +------------------+---------------+------------+-----------------------+\n   | user@example.com | USAGE         | SCHEMA     | default               |\n   +------------------+---------------+------------+-----------------------+\n   ```\n\n## Step 5: Generate a query\n\n[Complete the user quickstart](/sql/get-started/user-quickstart.md).\n\n## Step 6: View query history\n\n#. Click <History Icon> **Query History** in the sidebar.\n#. Click a string in the Query column to display query details:\n\n   ![Query history details](/_static/images/sql/query-details.png)\n\n## Next steps\n\nTo learn how to enable access to your own data and tables, see [_](/security/auth-authz/access-control/index.md).\n\nTo walk through the process to enable access, go to the **Get Started** pane on `https://<databricks-instance>/sql`, replacing <get-workspace-instance>.\n\n.. include:: /shared/replacements.md\n.. include:: /shared/api-replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What should I do if I don't receive the confirmation email after adding a user to the workspace?", "gold_answer": "If you don't receive the confirmation email after adding a user to the workspace, you should ask the user to check their spam folder. If the email is not found in the spam folder after 5 minutes, further troubleshooting may be necessary, such as ensuring the email address was entered correctly or contacting Databricks support."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\n---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "See [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How can I modify the IAM role for the Databricks workspace?", "gold_answer": "To modify the IAM role for the Databricks workspace, you need to modify the policy attached to the role that was configured during the workspace deployment to manage EC2 instances in your AWS account. This is part of the process to make an instance profile available in your Databricks workspace.\n\nThe specific steps or details on how to modify the policy are not provided in the document snippet here, but typically this process would involve accessing the AWS Management Console, navigating to the IAM service, selecting the appropriate IAM role, and then editing the policy attached to that role to include the necessary permissions for your Databricks workspace.\n\nFor more detailed instructions, you would refer to the document at the provided link (which is not clickable in this context):\nSee [_](/aws/iam/add-instance-profile-workspace-role.md)."}
{"hard_neg_docs": [{"page_content": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\n---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "See [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles", "fmt_content": "Source: https://docs.databricks.com/aws/iam/instance-profile-tutorial.html\nContent:\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles", "metadata": {"source": "https://docs.databricks.com/aws/iam/instance-profile-tutorial.html", "seq_num": 1619, "description": "", "text": "---\ndescription: Learn how to set up instance profiles and use them in <Databricks> to access S3 buckets securely from your Databricks clusters.\n---\n\n.. azure-gcp::\n    ---\n    orphan: 1\n    ---\n\n# Configure S3 access with instance profiles\n\nThis article walks you through how to create an instance profile with read, write, update, and delete permissions on a single S3 bucket. You can grant privileges for multiple buckets using a single IAM role and instance profile. It is also possible to use instance profiles to grant only read and list permissions on S3.\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    instance-profile\n    s3-bucket-policy\n    add-instance-profile-workspace-role\n    add-instance-profile-databricks\n    manage-instance-profiles\n\n## Before you begin\n\nThis tutorial is designed for workspace administrators. You must have sufficient privileges in the AWS account containing your Databricks workspace, and be a Databricks workspace administrator.\n\nThis tutorial assumes the following existing permissions and assets:\n\n- Privileges to edit the IAM role used to deploy the Databricks workspace.\n- Privileges to create new IAM roles in AWS.\n- Privileges to edit permissions on an S3 bucket.\n\n## Step 1: Create an instance profile\n\nIn this step, you create a new IAM role and define an inline policy. Together, these settings define the instance profile deployed to EC2 instances. Here you can also add a trust relationship so the instance profile can work with serverless compute resources.\n\nSee [_](/aws/iam/instance-profile.md).\n\n## Step 2: Create an S3 bucket policy\n\nIn this step, you add a trust relationship from the S3 bucket to the IAM role you created in Step 1.\n\n.. note:: S3 buckets have universally unique names and do not require an account ID for universal identification. If you choose to link an S3 bucket to an IAM role and Databricks workspace in a different AWS account, you must specify the account ID when configuring your S3 bucket policy.\n\nMake sure you copied the role ARN from Step 1. And ensure you're creating the policy on the S3 bucket you specified in your IAM role.\n\nSee [_](/aws/iam/s3-bucket-policy.md).\n\n## Step 3: Modify the IAM role for the Databricks workspace\n\nDatabricks uses a role configured during workspace deployment to manage EC2 instances in your AWS account. To make an instance profile available in your Databricks workspace, you need to modify the policy attached to this role.\n\nSee [_](/aws/iam/add-instance-profile-workspace-role.md).\n\n## Step 4: Add the instance profile to the Databricks workspace\n\nAs a final step, add the role ARN from Step 1 into your workspace by using Databricks admin console.\n\nSee [_](/aws/iam/add-instance-profile-databricks.md).\n\n## Manage instance profiles\n\nYou can manage instance profiles similar to other workspace assets using workspace ACLs.\n\nSee [_](/aws/iam/manage-instance-profiles.md)\n\n<a id=\"cluster-instance-profile\"></a>\n\n## Deploy compute resources with an instance profile\n\nUsers with permissions to deploy clusters can deploy clusters with any of their assigned instance profiles. All users with access to the cluster gain the permissions as defined by the instance profile. \n\nSee [_](/compute/instance-profiles.md).\n\nSQL warehouses use a single instance profile for each workspace and then use table ACLs for fine-grained permissions. \n\nSee [_](/data-governance/table-acls/object-privileges.md).\n\n<a id=\"edit-instance-profile\"></a>\n\n## Edit instance profile role ARN\n\nFor instance profiles that you've already created, you can later edit them but only to specify a different IAM role ARN. This step is required for <DBSQL> Serverless to work with an instance profile whose **role name** (the text after the last slash in the role ARN) and the **instance profile name** (the text after the last slash in the instance profile ARN) do not match. For related information, see [_](/sql/admin/serverless.md).\n\n#. Go to the [Admin Console](/administration-guide/workspace/index.md#admin-console).\n\n#. Click the **Instance Profiles** tab.\n\n#. Click the name of your instance profile that you want to edit.\n\n#. Click **Edit**. A dialog appears.\n\n   ![Edit instance profile Role ARN](/_static/images/clusters/instance-profiles/edit-instanceprofile-role-arn-only.png)\n\n   Edit the **IAM role ARN** field and paste in the role ARN associated with your instance profile. As an admin, you can get this value from the AWS console.\n\n#. Click **Save**.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "What is the purpose of Step 2 in the IAM instance profile tutorial?", "gold_answer": "Step 2 in the IAM instance profile tutorial is for adding a trust relationship from the S3 bucket to the IAM role that was created in Step 1. This step involves configuring an S3 bucket policy and ensuring that it is linked to the IAM role and, if necessary, to a Databricks workspace in a different AWS account. In such cases, the account ID must be specified in the S3 bucket policy. The user is reminded to have copied the role ARN from Step 1 and to create the policy on the correct S3 bucket that was specified in their IAM role."}
{"hard_neg_docs": [{"page_content": "Replace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\n---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "How can I load JSON data to create a DLT table in Databricks?", "gold_answer": "To load JSON data to create a Delta Live Table (DLT) in Databricks, you can use the following Python code snippet:\n\n```python\nimport dlt\n\n@dlt.table\ndef clickstream_raw():\n  return (\n    spark.read\n      .format(\"json\")\n      .load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\")\n  )\n```\n\nThis code defines a DLT table named `clickstream_raw` using the `@dlt.table` decorator. The function reads the JSON data from the specified path using `spark.read.format(\"json\").load(path)`, where `path` is the location of the JSON file you want to load."}
{"hard_neg_docs": [{"page_content": "Replace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\n---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```", "fmt_content": "Source: https://docs.databricks.com/delta-live-tables/load.html\nContent:\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```", "metadata": {"source": "https://docs.databricks.com/delta-live-tables/load.html", "seq_num": 17, "description": "", "text": "---\ndescription: This article describes how to load data to <Databricks> with <DLT>.\n---\n\n# Load data with <DLT>\n\n\n.. include:: /shared/dlt-gated-preview.md\n\nYou can load data from any data source supported by <AS> on <Databricks> using <DLT>. You can define datasets (tables and views) in <DLT> against any query that returns a Spark DataFrame, including streaming DataFrames and Pandas for Spark DataFrames. For data ingestion tasks, Databricks recommends using streaming tables for most use cases. Streaming tables are good for ingesting data from cloud object storage using <AL> or from message buses like Kafka. The examples below demonstrate some common patterns.\n\n.. important:: Not all data sources have SQL support. You can mix SQL and Python notebooks in a <DLT> pipeline to use SQL for all operations beyond ingestion.\n\nFor details on working with libraries not packaged in <DLT> by default, see [_](/delta-live-tables/external-dependencies.md).\n\n.. toctree::\n    :maxdepth: 1\n    :hidden:\n\n    event-hubs\n\n## Load files from cloud object storage\n\nDatabricks recommends using <AL> with <DLT> for most data ingestion tasks from cloud object storage. <AL> and <DLT> are designed to incrementally and idempotently load ever-growing data as it arrives in cloud storage. The following examples use <AL> to create datasets from CSV and JSON files:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def customers():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"csv\")\n        .load(\"/databricks-datasets/retail-org/customers/\")\n    )\n\n  @dlt.table\n  def sales_orders_raw():\n    return (\n      spark.readStream.format(\"cloudFiles\")\n        .option(\"cloudFiles.format\", \"json\")\n        .load(\"/databricks-datasets/retail-org/sales_orders/\")\n    )\n  ```\n\n  ```sql\n  CREATE OR REFRESH STREAMING TABLE customers\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/customers/\", \"csv\")\n\n  CREATE OR REFRESH STREAMING TABLE sales_orders_raw\n  AS SELECT * FROM cloud_files(\"/databricks-datasets/retail-org/sales_orders/\", \"json\")\n  ```\n\nSee [_](/ingestion/auto-loader/index.md) and [_](/delta-live-tables/sql-ref.md#auto-loader-sql).\n\n## Load data from a message bus\n\nYou can configure <DLT> pipelines to ingest data from message buses with streaming tables. Databricks recommends combining streaming tables with continuous execution and enhanced autoscaling to provide the most efficient ingestion for low latency loading from message buses. See [_](/delta-live-tables/auto-scaling.md).\n\nFor example, the following code configures a streaming table to ingest data from Kafka:\n\n```python\nimport dlt\n\n@dlt.table\ndef kafka_raw():\n  return (\n    spark.readStream\n      .format(\"kafka\")\n      .option(\"kafka.bootstrap.servers\", \"<server:ip>\")\n      .option(\"subscribe\", \"topic1\")\n      .option(\"startingOffsets\", \"latest\")\n      .load()\n  )\n```\n\nYou can write downstream operations in pure SQL to perform streaming transformations on this data, as in the following example:\n\n```sql\nCREATE OR REFRESH STREAMING TABLE streaming_silver_table\nAS SELECT\n  *\nFROM\n  STREAM(LIVE.kafka_raw)\nWHERE ...\n```\n\nFor an example of working with Event Hubs, see [_](/delta-live-tables/event-hubs.md).\n\nSee [_](/external-data/index.md#streaming-sources).\n\n## Load data from external systems\n\n<DLT> supports loading data from any data source supported by <Databricks>. Some data sources do not have full parity for support in SQL, but you can write a standalone Python notebook to define data ingestion from these sources and then schedule this library alongside other SQL notebooks to build a <DLT> pipeline. The following example declares a materialized view to access the current state of data in a remote Postgresql table:\n\n```python\nimport dlt\n\n@dlt.table\ndef postgres_raw():\n  return (\n    spark.read\n      .format(\"postgresql\")\n      .option(\"dbtable\", table_name)\n      .option(\"host\", database_host_url)\n      .option(\"port\", 5432)\n      .option(\"database\", database_name)\n      .option(\"user\", username)\n      .option(\"password\", password)\n      .load()\n  )\n```\n\nSee [_](/external-data/index.md).\n\n## Load small or static datasets from cloud object storage\n\nYou can load small or static datasets using <AS> load syntax. <DLT> supports all of the file formats supported by <AS> on <Databricks>. For a full list, see [_](/external-data/index.md#formats).\n\nThe following examples demonstrate loading JSON to create <DLT> tables:\n\n.. code-language-tabs::\n\n  ```python\n  @dlt.table\n  def clickstream_raw():\n    return (spark.read.format(\"json\").load(\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"))\n  ```\n\n  ```sql\n  CREATE OR REFRESH LIVE TABLE clickstream_raw\n  AS SELECT * FROM json.`/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json`;\n  ```\n\n.. note:: The ``SELECT * FROM format.`path`;`` SQL construct is common to all SQL environments on <Databricks>. It is the recommended pattern for direct file access using SQL with <DLT>.\n\nSee [_](/external-data/index.md).\n\n<a id=\"configure-secrets\"></a>\n\n## Securely access storage credentials with secrets in a pipeline\n\nYou can use <Databricks> [secrets](/security/secrets/index.md) to store credentials such as access keys or passwords. To configure the secret in your pipeline, use a Spark property in the pipeline settings cluster configuration. See [_](/delta-live-tables/settings.md#cluster-config).\n\nThe following example uses a secret to store an access key required to read input data from an <ADLS2> (ADLS Gen2) storage account using [Auto Loader](/ingestion/auto-loader/index.md). You can use this same method to configure any secret required by your pipeline, for example, AWS keys to access S3, or the password to an Apache Hive metastore.\n\nTo learn more about working with <ADLS2>, see [_](/storage/azure-storage.md).\n\n.. note:: You must add the `spark.hadoop.` prefix to the `spark_conf` configuration key that sets the secret value.\n\n```json\n{\n    \"id\": \"43246596-a63f-11ec-b909-0242ac120002\",\n    \"clusters\": [\n      {\n        \"label\": \"default\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        },\n        \"autoscale\": {\n          \"min_workers\": 1,\n          \"max_workers\": 5,\n          \"mode\": \"ENHANCED\"\n        }\n      },\n      {\n        \"label\": \"maintenance\",\n        \"spark_conf\": {\n          \"spark.hadoop.fs.azure.account.key.<storage-account-name>.dfs.core.windows.net\": \"{{secrets/<scope-name>/<secret-name>}}\"\n        }\n      }\n    ],\n    \"development\": true,\n    \"continuous\": false,\n    \"libraries\": [\n      {\n        \"notebook\": {\n          \"path\": \"/Users/user@databricks.com/DLT Notebooks/Delta Live Tables quickstart\"\n        }\n      }\n    ],\n    \"name\": \"DLT quickstart using ADLS2\"\n}\n```\n\nReplace\n\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<scope-name>` with the <Databricks> secret scope name.\n- `<secret-name>` with the name of the key containing the Azure storage account access key.\n\n```python\nimport dlt\n\njson_path = \"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/<path-to-input-dataset>\"\n@dlt.create_table(\n  comment=\"Data ingested from an ADLS2 storage account.\"\n)\ndef read_from_ADLS2():\n  return (\n    spark.readStream.format(\"cloudFiles\")\n      .option(\"cloudFiles.format\", \"json\")\n      .load(json_path)\n  )\n```\n\nReplace\n\n- `<container-name>` with the name of the Azure storage account container that stores the input data.\n- `<storage-account-name>` with the ADLS Gen2 storage account name.\n- `<path-to-input-dataset>` with the path to the input dataset.\n\n.. include:: /shared/replacements.md", "timestamp": 0, "next": ""}}], "question": "What is the purpose of the `dlt.table` decorator in the Python code provided?", "gold_answer": "The `dlt.table` decorator in the Python code provided is used to define a Delta Live Table (DLT). It indicates that the function beneath it is used to define a DLT table within a Delta Live Tables pipeline in Databricks. The function itself contains the logic for reading data from a specified source and transforming it into the desired format for the table. When the DLT pipeline runs, it will execute the code within the decorated function to create or update the table with the loaded data."}
{"hard_neg_docs": [{"page_content": "![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Can I clone only a subset of directories from a remote repository in Databricks?", "gold_answer": "Yes, you can clone only a subset of directories from a remote repository in Databricks. At the stage of adding a repo connected to a remote repo, you have the option to use the Sparse Checkout mode, which allows you to clone only a subset of your repository's directories. This feature is particularly useful if your repository's size is beyond the Databricks supported limits."}
{"hard_neg_docs": [{"page_content": "![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)", "fmt_content": "Source: https://docs.databricks.com/repos/git-operations-with-repos.html\nContent:\n![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)", "metadata": {"source": "https://docs.databricks.com/repos/git-operations-with-repos.html", "seq_num": 338, "description": "", "text": "---\ndescription: Learn how to use common Git operations with Databricks Repos, including clone, commit, push, and more.\n---\n\n# Clone a Git repo & other common Git operations\n\nThe article describes how to clone a Git repo and perform other common Git operations with Databricks Repos.\n\n.. important::\n\n  If you clone a repo using the CLI through a cluster's web terminal, the files won't display in the <Databricks> UI.\n\n## Add a repo and connect remotely later\n\nYou can also create a new repo in <Databricks> and add the remote Git repository URL later.\n\n1. To create a new repo not linked to a remote Git repository, click the **Add Repo** button. Deselect **Create repo by cloning a Git repository**, enter a name for the repo, and then click **Create Repo**.\n\n  ![Add repo without connecting remotely.](/_static/images/repos/connect-repo-later.png)\n\n#. When you are ready to add the Git repository URL, click the down arrow next to the repo name in the workspace to open the Repo menu, and select **Git...** to open the Git dialog.\n\n  ![Repos menu: Add a Git repo URL.](/_static/images/repos/repo-menu.png)\n\n#. In the **Git repo URL** field, enter the URL for the remote repository and select your Git provider from the drop-down menu. Click **Save**.\n\n  ![Git dialog settings tab.](/_static/images/repos/git-dialog-settings.png)\n\n## Add a repo connected to a remote repo\n\n#. Click <Repos Icon> **Repos** in the sidebar.\n\n#. Click **Add Repo**.\n\n  ![Add repo.](/_static/images/repos/add-repo.png)\n\n#. In the **Add Repo** dialog, select **Create repo by cloning a Git repository** and enter the repository URL. Select your Git provider from the drop-down menu, optionally change the name to use for the Databricks repo, and click **Create Repo**.\n  The contents of the remote repository are cloned to the Databricks repo.\n\n    ![Clone from repo.](/_static/images/repos/clone-from-repo.png)\n\nAt this stage you have the option to clone only a subset of your repositories’s directories via the Sparse Checkout mode see [sparse checkout](#sparse). This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md)\n\n<a id=\"open-the-git-dialog\"></a>\n\n## Access the Git dialog\n\nYou can access the Git dialog from a notebook or from the Databricks Repos browser.\n\n- From a notebook, click the button next to the name of the notebook that identifies the current Git branch.\n\n  ![Git dialog button on notebook.](/_static/images/notebooks/toolbar.png)\n\n- From the Databricks Repos browser, click the button to the right of the repo name:\n\n  ![Git dialog button in repo browser.](/_static/images/repos/git-button-repos.png)\n\n  You can also click the down arrow next to the repo name, and select **Git...** from the menu.\n\n  ![Repos menu: Access the Git dialog.](/_static/images/repos/repo-menu.png)\n\n## Pull changes from the remote Git repository\n\nTo pull changes from the remote Git repository, click ![Pull](/_static/images/repos/pull-button.png)in the Git dialog. Notebooks and other files are updated automatically to the latest version in your remote repository.\n\n.. important::\n  Git operations that pull in upstream changes clear the notebook state. For more information, see [_](limits.md#incoming-changes-clear-notebook-state).\n\n<a id=\"merge-conflicts\"></a>\n\n## Resolve merge conflicts\n\nTo resolve a merge conflict, you must either discard conflicting changes or commit your changes to a new branch and then merge them into the original feature branch using a pull request.\n\n#. If there is a merge conflict, the Repos UI shows a notice allowing you to cancel the pull or resolve the conflict. If you select **Resolve conflict using PR**, a dialog appears that lets you create a new branch and commit your changes to it.\n\n  ![Resolve a merge conflict dialog.](/_static/images/repos/resolve-conflict-dialog.png)\n\n#. When you click **Commit to new branch**, a notice appears with a link: **Create a pull request to resolve merge conflicts**. Click the link to open your Git provider.\n\n  ![Merge conflict create PR message.](/_static/images/repos/merge-conflict-create-pr-msg.png)\n\n#. In your Git provider, create the PR, resolve the conflicts, and merge the new branch into the original branch.\n#. Return to the Repos UI. Use the Git dialog to pull changes from the Git repository to the original branch.\n\n## Commit and push changes to the remote Git repository\n\nWhen you have added new notebooks or files, or made changes to existing notebooks or files, the Git dialog highlights the changes.\n\n![Git dialog.](/_static/images/repos/git-commit-push.png)\n\nAdd a required Summary of the changes, and click **Commit & Push** to push these changes to the remote Git repository.\n\nIf you don't have permission to commit to the default branch, such as `main`, [create a new branch](#create-a-new-branch) and use your Git provider interface to create a pull request (PR) to merge it into the default branch.\n\n.. note::\n\n  - Results are not included with a notebook commit. All results are cleared before the commit is made.\n  - For instructions on resolving merge conflicts, see [_](#merge-conflicts).\n\n## Switch to a different branch\nYou can switch to (checkout) a different branch via the branch dropdown in the Git dialog\n\n<a id=\"create-a-new-branch\"></a>\n\n## Create a new branch\n\nYou can create a new branch based on an existing branch from the Git dialog:\n\n![Git dialog new branch.](/_static/images/repos/git-dialog-new-branch.png)\n\n## Git reset\n\n.. include:: /shared/preview.md\n\nIn Databricks Repos, you can perform a Git `reset` within the <Databricks> UI. Git reset in Databricks Repos is equivalent to a `git reset --hard` operation. Changes made to your local branch are also pushed to remote.\n\nWith Git reset you can reset a branch to a known good state. You can use this when the local edits are in conflict with the upstream branch and you don't mind losing those edits. [Read more about git `reset --hard`](https://git-scm.com/docs/git-reset#Documentation/git-reset.txt---hard).\n\n### Reset to the upstream (remote) branch\n\nTo reset your local branch to the remote branch, follow these steps.\n\n.. note:: When you reset, you lose all uncommitted changes, staged and unstaged.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the branch to reset.\n\n    ![Git reset --hard dialog](/_static/images/repos/reset-to-branch.png)\n\n### Reset to a different remote branch\n\nIn this scenario, you reset your selected branch (for example, `feature_a`) to a different branch (for example, `main`).\nThis process also resets the upstream (remote) branch `feature_a` to main.\n\n.. important::\n    - If you have uncommitted changes, an alert warns \"Your uncommitted changes will be lost.\"\n\n    - If you reset to another branch, <Databricks> runs a force push operation that resets the history of your current branch on remote.\n\n1. Select **Reset** from the kebab menu.\n\n#. Select the remote branch you want to reset to. In this example, you reset to the `main` branch.\n\n    ![Git reset --hard to other branch dialog](/_static/images/repos/reset-to-main-with-notice.png)\n\n\n<a id=\"sparse\"></a>\n\n## Configure sparse checkout mode\n\nSparse checkout is a client side setting which allows you to clone and work with only a subset of the remote repositories’s directories in Databricks. This is especially useful if your repository's size is beyond the Databricks supported [limits](/repos/limits.md).\n\nYou can use the Sparse Checkout mode when adding (cloning) a new repo.\n\n1. In the **Add Repo** dialog, open **Advanced**.\n#. Select **Sparse checkout mode**.\n\n    ![Sparse checkout option in the Add Repo dialog.](/_static/images/repos/sparse-checkout-option.png)\n3. In the **Cone patterns** box, specify the cone checkout patterns you want. Separate multiple patterns by line breaks.\n\nAt this time, you can't disable sparse checkout for a repo in <Databricks>.\n\n### How cone patterns work\n\nTo understand how cone pattern works in the sparse checkout mode, see the following diagram representing the remote repository structure.\n\n![Remote repository structure without sparse checkout.](/_static/images/repos/repo-structure-without-sparse-checkout.png)\n\nIf you select **Sparse checkout mode**, but do not specify a cone pattern, the default cone pattern is applied. This includes only the files in root and no subdirectories, resulting in a repo structure as following:\n\n![Sparse checkout: default cone pattern.](/_static/images/repos/sparse-checkout-default.png)\n\nSetting the sparse checkout cone pattern as `parent/child/grandchild` results in all contents of the `grandchild` directory being recursively included. The files immediately in the `/parent`, `/parent/child` and root directory are also included. See the directory structure in the following diagram:\n\n![Sparse checkout: Specify parent-grandchild-child folder cone pattern.](/_static/images/repos/set-sparse-parent-grand-child.png)\n\nYou can add multiple patterns separated by line breaks.\n\n### Modify sparse checkout settings\n\nOnce a repo is created, the sparse checkout cone pattern can be edited from **Settings > Advanced > Cone patterns**.\n\nNote the following behavior:\n\n* Removing a folder from the cone pattern removes it from Databricks if there are no uncommitted changes .\n* Adding a folder via editing the sparse checkout cone pattern adds it to Databricks without requiring an additional pull.\n* Sparse checkout patterns cannot be changed to remove a folder when there are uncommitted changes in that folder.\n\n  For example, a user edits a file in a folder and does not commit changes. She then tries to change the sparse checkout pattern to not include this folder. In this case, the pattern is accepted, but the actual folder is not deleted. She needs to revert the pattern to include that folder, commit changes, and then reapply the new pattern.\n\n.. note::\n    You can't disable sparse checkout for a repo that was created with Sparse Checkout mode enabled.\n\n### Make and push changes with sparse checkout enabled\n\nYou can edit existing files and commit and push them from the Repos interface. When creating new folders of files you should make sure they are included in the cone pattern you had specified for that repo.\n\nIncluding a new folder outside of the cone pattern results in an error during the commit and push operation. To rectify it, edit the cone pattern to include the new folder you are trying to commit and push.\n\n## Use the Repos API\n\n[Repos API](https://docs.databricks.com/dev-tools/api/latest/repos.html) allows you to create, list, update and delete Databricks Repos programatically. In addition, you can use the [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html) to manage your Git provider PATs.\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "How do I add a Git repository to Databricks using the web interface?", "gold_answer": "To add a Git repository to Databricks using the web interface, you should follow these steps:\n\n1. Click on the **Repos** icon in the sidebar within the Databricks workspace.\n\n2. Click on **Add Repo**.\n\n3. In the **Add Repo** dialog, select **Create repo by cloning a Git repository**.\n\n4. Enter the URL of the remote Git repository you want to clone.\n\n5. Select your Git provider from the drop-down menu. If necessary, you can also change the name that will be used for the Databricks repo.\n\n6. Click **Create Repo**.\n\nAfter completing these steps, the contents of the remote Git repository will be cloned to the Databricks repo. If needed, you have the option to use Sparse Checkout mode to clone only a subset of your repository’s directories, which can be helpful if the repository's size exceeds Databricks' supported limits."}
{"hard_neg_docs": [{"page_content": "## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\n---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "See [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Can I modify a Delta table from different workspaces concurrently?", "gold_answer": "Yes, you can concurrently modify the same Delta table from different workspaces. Furthermore, if one process is writing from a workspace, readers in other workspaces will see a consistent view."}
{"hard_neg_docs": [{"page_content": "## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}, {"page_content": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\n---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "chat_history": [], "gold_docs": [{"page_content": "See [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md", "fmt_content": "Source: https://docs.databricks.com/lakehouse/acid.html\nContent:\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md", "metadata": {"source": "https://docs.databricks.com/lakehouse/acid.html", "seq_num": 2116, "description": "", "text": "---\ndescription: Learn about the ACID transaction guarantees between reads and writes on <Databricks>.\n---\n\n# What are ACID guarantees on <Databricks>?\n\n<Databricks> uses <Delta> by default for all reads and writes and builds upon the ACID guarantees provided by the [open source Delta Lake protocol](https://delta.io). ACID stands for atomicity, consistency, isolation, and durability.\n\n- [Atomicity](#atomicity) means that all transactions either succeed or fail completely.\n- [Consistency](#consistency) guarantees relate to how a given state of the data is observed by simultaneous operations.\n- [Isolation](#isolation) refers to how simultaneous operations potentially conflict with one another.\n- [Durability](#durability) means that committed changes are permanent.\n\nWhile many data processing and warehousing technologies describe having ACID transactions, specific guarantees vary by system, and transactions on <Databricks> might differ from other systems you've worked with.\n\n.. note::\n\n  This page describes guarantees for tables backed by <Delta>. Other data formats and integrated systems might not provide transactional guarantees for reads and writes.\n\n  All <Databricks> writes to cloud object storage use transactional commits, which create metadata files starting with `_started_<id>` and `_committed_<id>` alongside data files. You do not need to interact with these files, as <Databricks> routinely cleans up stale commit metadata files.\n\n<a id=\"transactions\"></a>\n\n## How are transactions scoped on <Databricks>?\n\n<Databricks> manages transactions at the table level. Transactions always apply to one table at a time. For managing concurrent transactions, <Databricks> uses optimistic concurrency control. This means that there are no locks on reading or writing against a table, and deadlock is not a possibility.\n\nBy default, <Databricks> provides snapshot isolation on reads and [write-serializable isolation](/optimizations/isolation-level.md#isolation-levels) on writes. Write-serializable isolation provides stronger guarantees than snapshot isolation, but it applies that stronger isolation only for writes.\n\nRead operations referencing multiple tables return the current version of each table at the time of access, but do not interrupt concurrent transactions that might modify referenced tables.\n\n<Databricks> does not have `BEGIN/END` constructs that allow multiple operations to be grouped together as a single transaction. Applications that modify multiple tables commit transactions to each table in a serial fashion. You can combine inserts, updates, and deletes against a table into a single write transaction using `MERGE INTO`.\n\n<a id=\"atomicity\"></a>\n\n## How does <Databricks> implement atomicity?\n\nThe transaction log controls commit atomicity. During a transaction, data files are written to the file directory backing the table. When the transaction completes, a new entry is committed to the transaction log that includes the paths to all files written during the transaction. Each commit increments the table version and makes new data files visible to read operations. The current state of the table comprises all data files marked valid in the transaction logs.\n\nData files are not tracked unless the transaction log records a new version. If a transaction fails after writing data files to a table, these data files will not corrupt the table state, but the files will not become part of the table. The `VACUUM` operation deletes all untracked data files in a table directory, including remaining uncommitted files from failed transactions.\n\n<a id=\"durability\"></a>\n\n## How does <Databricks> implement durability?\n\n<Databricks> uses cloud object storage to store all data files and transaction logs. Cloud object storage has high availability and durability. Because transactions either succeed or fail completely and the transaction log lives alongside data files in cloud object storage, tables on <Databricks> inherit the durability gurantees of the cloud object storage on which they're stored.\n\n<a id=\"consistency\"></a>\n\n## How does <Databricks> implement consistency?\n\n<Delta> uses optimistic concurrency control to provide transactional guarantees between writes. Under this mechanism, writes operate in three stages:\n\n1. **Read**: Reads (if needed) the latest available version of the table to identify which files need to be modified (that is, rewritten).\n  - Writes that are append-only do not read the current table state before writing. Schema validation leverages metadata from the transaction log.\n#. **Write**: Writes data files to the directory used to define the table.\n#. **Validate and commit**:\n  - Checks whether the proposed changes conflict with any other changes that may have been concurrently committed since the snapshot that was read.\n  - If there are no conflicts, all the staged changes are committed as a new versioned snapshot, and the write operation succeeds.\n  - If there are conflicts, the write operation fails with a concurrent modification exception. This failure prevents corruption of data.\n\nOptimistic conccurency assumes that most concurrent transactions on your data could not conflict with one another, but conflicts can occur. See [_](/optimizations/isolation-level.md).\n\n<a id=\"isolation\"></a>\n\n## How does <Databricks> implement isolation?\n\n<Databricks> uses write serializable isolation by default for all table writes and updates. Snapshot isolation is used for all table reads.\n\nWrite serializability and optimistic concurrency control work together to provide high throughput for writes. The current valid state of a table is always available, and a write can be started against a table at any time. Concurrent reads are only limited by throughput of the metastore and cloud resources.\n\nSee [_](/optimizations/isolation-level.md).\n\n## Does <Delta> support multi-table transactions?\n\n<Delta> does not support multi-table transactions. <Delta> supports transactions at the _table_ level.\n\n.. azure-aws::\n\n  Primary key and foreign key relationships on <Databricks> are informational and not enforced. See [_](/tables/constraints.md#pk-fk).\n\n<a id=\"multi-cluster\"></a>\n\n## What does it mean that <Delta> supports multi-cluster writes?\n\n<Delta> prevents data corruption when multiple clusters write to the same table concurrently. Some write operations can conflict during simultaneous execution, but don't corrupt the table. See [_](/optimizations/isolation-level.md).\n\n.. aws-gcp::\n\n  .. note:: <Delta> on S3 has several limitations not found on other storage systems. See [_](/delta/s3-limitations.md).\n\n.. azure-gcp::\n\n  ## Can I modify a Delta table from different workspaces?\n\n  Yes, you can concurrently modify the same Delta table from different workspaces. Moreover, if one process is writing from a workspace, readers in other workspaces will see a consistent view.\n\n\n.. include:: /shared/replacements.md\n", "timestamp": 0, "next": ""}}], "question": "Does Delta support multi-table transactions?", "gold_answer": "No, Delta does not support multi-table transactions. Delta supports transactions at the table level only."}
